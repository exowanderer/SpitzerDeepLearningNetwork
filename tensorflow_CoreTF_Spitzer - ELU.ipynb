{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fa1a9421-50b6-7ae4-0b03-212f533f8fcb"
   },
   "source": [
    "## TF-Core - eLU - Spitzer Calibration Data\n",
    "\n",
    "This script show a simple example of using [tf.contrib.learn][1] library to create our model.\n",
    "\n",
    "The code is divided in following steps:\n",
    "\n",
    " - Load CSVs data\n",
    " - Continuous features\n",
    " - Converting Data into Tensors\n",
    " - Selecting and Engineering Features for the Model\n",
    " - Defining The Regression Model\n",
    " - Training and Evaluating Our Model\n",
    " - Predicting output for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5259819c-f765-e649-5e67-7bcde8665463"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, minmax_scale\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from time import time\n",
    "start0 = time()\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "# %matplotlib inline\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.rcParams['axes.labelsize'] = 14\n",
    "# plt.rcParams['xtick.labelsize'] = 12\n",
    "# plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "# PROJECT_ROOT_DIR = \".\"\n",
    "# CHAPTER_ID = \"ann\"\n",
    "\n",
    "# def save_fig(fig_id, tight_layout=True):\n",
    "#     path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "#     print(\"Saving figure\", fig_id)\n",
    "#     if tight_layout:\n",
    "#         plt.tight_layout()\n",
    "#     plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0e7f9a40-2b80-bdea-d5aa-d52a87577e8c"
   },
   "source": [
    "## Load CSVs data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "cb7edeab-c13e-8a29-34dc-2e9d0bd7059b"
   },
   "source": [
    "df_train_ori = pd.read_csv('train.csv')\n",
    "df_test_ori = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nSkip = 20\n",
    "spitzerDataRaw  = pd.read_csv('pmap_ch2_0p1s_x4_rmulti_s3_7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PLDpixels = pd.DataFrame({key:spitzerDataRaw[key] for key in spitzerDataRaw.columns.values if 'pix' in key})\n",
    "PLDpixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PLDnorm = np.sum(np.array(PLDpixels),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PLDpixels = (PLDpixels.T / PLDnorm).T\n",
    "PLDpixels"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[plt.plot(PLDpixels[key]) for key in PLDpixels.columns.values];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spitzerData = spitzerDataRaw.copy()\n",
    "for key in spitzerDataRaw.columns: \n",
    "    if key in PLDpixels.columns:\n",
    "        spitzerData[key] = PLDpixels[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPLD = np.array(pd.DataFrame({key:spitzerData[key] for key in spitzerData.columns.values if 'pix' in key}))\n",
    "assert(not sum(abs(testPLD - np.array(PLDpixels))).all())\n",
    "print('Confirmed that PLD Pixels have been Normalized to Spec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notFeatures     = ['flux', 'fluxerr', 'xerr', 'yerr', 'xycov', 't_cernox']\n",
    "\n",
    "periodMax           = spitzerData['bmjd'].values.max() - spitzerData['bmjd'].values.min()\n",
    "periodMin           = np.min(np.diff(spitzerData['bmjd'].values))\n",
    "spitzerData['freq'] = np.linspace(np.pi/periodMax, 4*np.pi/periodMin, spitzerData['bmjd'].values.size)\n",
    "\n",
    "feature_columns = spitzerData.drop(notFeatures,axis=1).columns.values\n",
    "features        = spitzerData.drop(notFeatures,axis=1).values\n",
    "labels          = spitzerData['flux'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stdScaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_scaled = stdScaler.fit_transform(features)\n",
    "labels_scaled   = labels#stdScaler.fit_transform(labels[:,None]).ravel()\n",
    "\n",
    "idx_valtest, idx_train = train_test_split(np.arange(labels_scaled.size), test_size=0.6, random_state=42)\n",
    "idx_val, idx_test      = train_test_split(idx_valtest                  , test_size=0.5, random_state=42)\n",
    "\n",
    "x_val   = features_scaled[idx_val]\n",
    "x_test  = features_scaled[idx_test]\n",
    "x_train = features_scaled[idx_train]\n",
    "\n",
    "y_val   = labels_scaled[idx_val]\n",
    "y_test  = labels_scaled[idx_test]\n",
    "y_train = labels_scaled[idx_train]\n",
    "\n",
    "y_val_err   = spitzerData['fluxerr'].values[idx_val]\n",
    "y_test_err  = spitzerData['fluxerr'].values[idx_test]\n",
    "y_train_err = spitzerData['fluxerr'].values[idx_train]\n",
    "\n",
    "print(x_val.shape  , 'validation samples')\n",
    "print(x_train.shape, 'train samples')\n",
    "print(x_test.shape , 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df    = pd.DataFrame(np.c_[x_train, y_train], columns=list(feature_columns) + ['flux'])\n",
    "test_df     = pd.DataFrame(np.c_[x_test , y_test ], columns=list(feature_columns) + ['flux'])\n",
    "evaluate_df = pd.DataFrame(np.c_[x_val  , y_val  ], columns=list(feature_columns) + ['flux'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8745286a-f9f6-67db-4c2a-e38bc6673045"
   },
   "source": [
    "We only take first 1000 rows for training/testing and last 500 row for evaluation.\n",
    "\n",
    "\n",
    "This done so that this script does not consume a lot of kaggle system resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "95a34f94-b934-2e8f-fbd7-6e5c05425557"
   },
   "outputs": [],
   "source": [
    "# train_df = df_train_ori.head(1000)\n",
    "# evaluate_df = df_train_ori.tail(500)\n",
    "\n",
    "# test_df = df_test_ori.head(1000)\n",
    "\n",
    "# MODEL_DIR = \"tf_model_spitzer/withNormalization_drop50/relu\"\n",
    "# MODEL_DIR = \"tf_model_spitzer/adamOptimizer_with_drop50/relu\"\n",
    "MODEL_DIR = \"tf_model_spitzer/adamOptimizer/drop50/elu/\"\n",
    "\n",
    "print(\"train_df.shape = \"   , train_df.shape)\n",
    "print(\"test_df.shape = \"    , test_df.shape)\n",
    "print(\"evaluate_df.shape = \", evaluate_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3c9feb25-7e41-33b9-eb92-543e925b0214"
   },
   "source": [
    "## Filtering Categorical and Continuous features\n",
    "\n",
    "We store Categorical, Continuous and Target features names in different variables. This will be helpful in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3bfe4dd5-5b24-b897-b1b1-952e6944b308",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# categorical_features = [feature for feature in features if 'cat' in feature]\n",
    "categorical_features  = []\n",
    "continuous_features   = [feature for feature in train_df.columns]# if 'cat' in feature]\n",
    "LABEL_COLUMN          = 'flux'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5f408c4f-60b3-1e75-281b-ace7ae729e43"
   },
   "source": [
    "## Converting Data into Tensors\n",
    "\n",
    "> When building a TF.Learn model, the input data is specified by means of an Input Builder function. This builder function will not be called until it is later passed to TF.Learn methods such as fit and evaluate. The purpose of this function is to construct the input data, which is represented in the form of Tensors or SparseTensors.\n",
    "\n",
    "> Note that input_fn will be called while constructing the TensorFlow graph, not while running the graph. What it is returning is a representation of the input data as the fundamental unit of TensorFlow computations, a Tensor (or SparseTensor).\n",
    "\n",
    "[More detail][2] on input_fn.\n",
    "\n",
    "[2]: https://www.tensorflow.org/versions/r0.11/tutorials/input_fn/index.html#building-input-functions-with-tf-contrib-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "04601f78-9447-20e9-20ce-3adcfd9a433c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting Data into Tensors\n",
    "def input_fn(df, training = True):\n",
    "    # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "    # the values of that column stored in a constant Tensor.\n",
    "    continuous_cols = {k: tf.constant(df[k].values)\n",
    "                       for k in continuous_features}\n",
    "\n",
    "    # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "    # to the values of that column stored in a tf.SparseTensor.\n",
    "    # categorical_cols = {k: tf.SparseTensor(\n",
    "    #     indices=[[i, 0] for i in range(df[k].size)],\n",
    "    #     values=df[k].values,\n",
    "    #     shape=[df[k].size, 1])\n",
    "    #     for k in categorical_features}\n",
    "\n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = continuous_cols\n",
    "    # feature_cols = dict(list(continuous_cols.items()) + list(categorical_cols.items()))\n",
    "    \n",
    "    if training:\n",
    "        # Converts the label column into a constant Tensor.\n",
    "        label = tf.constant(df[LABEL_COLUMN].values)\n",
    "\n",
    "        # Returns the feature columns and the label.\n",
    "        return feature_cols, label\n",
    "    \n",
    "    # Returns the feature columns    \n",
    "    return feature_cols\n",
    "\n",
    "def train_input_fn():\n",
    "    return input_fn(train_df, training=True)\n",
    "\n",
    "def eval_input_fn():\n",
    "    return input_fn(evaluate_df, training=False)\n",
    "\n",
    "def test_input_fn():\n",
    "    return input_fn(test_df, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f53238a-876b-bf8b-a5f9-77cdfafe67d9"
   },
   "source": [
    "## Selecting and Engineering Features for the Model\n",
    "\n",
    "We use tf.learn's concept of [FeatureColumn][FeatureColumn] which help in transforming raw data into suitable input features. \n",
    "\n",
    "These engineered features will be used when we construct our model.\n",
    "\n",
    "[FeatureColumn]: https://www.tensorflow.org/versions/r0.11/tutorials/linear/overview.html#feature-columns-and-transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "87b90027-c671-d7aa-88b2-4e2856a83f52",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engineered_features = []\n",
    "\n",
    "for continuous_feature in continuous_features:\n",
    "    engineered_features.append(\n",
    "        tf.contrib.layers.real_valued_column(continuous_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ca73de97-e020-4cf3-934f-44dde67523ae"
   },
   "source": [
    "## Defining The Regression Model\n",
    "\n",
    "Following is the simple DNNRegressor model. More detail about hidden_units, etc can be found [here][123].\n",
    "\n",
    "**model_dir** is used to save and restore our model. This is because once we have trained the model we don't want to train it again, if we only want to predict on new data-set.\n",
    "\n",
    "[123]: https://www.tensorflow.org/versions/r0.9/api_docs/python/contrib.learn.html#DNNRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.contrib.learn.RunConfig(tf_random_seed=42) # not shown in the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs  = features_scaled.shape[0]\n",
    "n_features= features_scaled.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs  = x_train.shape[0]\n",
    "n_features= x_train.shape[1]\n",
    "n_inputs, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6c5b7254-0e26-88d2-a65b-17f81c223ab4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden1 = n_features\n",
    "n_hidden2 = n_features\n",
    "n_hidden3 = n_features\n",
    "n_outputs = n_inputs   # because: regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('data'):\n",
    "    X   = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "    y   = tf.placeholder(tf.float32, shape=(None), name=\"y\")\n",
    "    unc = tf.placeholder(tf.float32, shape=(None), name=\"unc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "l1_reg       = 0.001\n",
    "l2_reg       = 0.0001\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "\n",
    "X_bnorm= tf.layers.batch_normalization(X)\n",
    "X_drop = tf.layers.dropout(X_bnorm, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init      = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    hidden1      = tf.layers.dense(X_drop , n_hidden1, name=\"hidden1\", \n",
    "                        activation=tf.nn.elu, kernel_initializer=he_init,\n",
    "                        kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(scale_l1=l1_reg, scale_l2=l2_reg))\n",
    "    \n",
    "    hidden1_bnorm= tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "    hidden1_drop = tf.layers.dropout(hidden1_bnorm, dropout_rate, training=training)\n",
    "    \n",
    "    hidden2      = tf.layers.dense(hidden1_drop, n_hidden2, name=\"hidden2\", \n",
    "                        activation=tf.nn.elu, kernel_initializer=he_init,\n",
    "                        kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(scale_l1=l1_reg, scale_l2=l2_reg))\n",
    "    \n",
    "    hidden2_bnorm= tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "    hidden2_drop = tf.layers.dropout(hidden2_bnorm, dropout_rate, training=training)\n",
    "    \n",
    "    hidden3      = tf.layers.dense(hidden2_drop, n_hidden3, name=\"hidden3\", \n",
    "                        activation=tf.nn.elu, kernel_initializer=he_init,\n",
    "                        kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(scale_l1=l1_reg, scale_l2=l2_reg))\n",
    "    \n",
    "    hidden3_bnorm= tf.layers.batch_normalization(hidden3, training=training, momentum=0.9)\n",
    "    hidden3_drop = tf.layers.dropout(hidden3_bnorm, dropout_rate, training=training)\n",
    "    \n",
    "    output  = tf.layers.dense(hidden3_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    def tf_nll(labels, output, uncs, coeff=1):\n",
    "        error = output - labels\n",
    "        return tf.reduce_sum(1 * (coeff * np.log(2*np.pi) + coeff * tf.log(uncs) + (0.5/uncs) * tf.pow(error, 2)))\n",
    "    \n",
    "    likelihood  = tf.reduce_mean(tf_nll(labels=y, output=output, uncs=unc))\n",
    "    \n",
    "    reg_losses  = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss        = tf.add_n([likelihood] + reg_losses, name=\"loss\")\n",
    "    \n",
    "    mse         = tf.reduce_mean(tf.squared_difference(output, y, name=\"mse\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    r2_acc      = 1 - tf.reduce_mean(y-output) / tf.reduce_mean(y - tf.reduce_mean(y))\n",
    "    rho2_acc    = 1 - tf.reduce_mean((y-output) / unc) / tf.reduce_mean((y-tf.reduce_mean(y)) / unc)\n",
    "    accuracy    = tf.reduce_mean(tf.squared_difference(output, y, name=\"accuracy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "# with tf.name_scope(\"train\"):\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "print(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_summary  = tf.summary.scalar('MSE'       , mse )\n",
    "loss_summary = tf.summary.scalar('loss'      , loss)\n",
    "nll_summary  = tf.summary.scalar('likelihood', likelihood)\n",
    "r2s_summary  = tf.summary.scalar('r2_acc'    , r2_acc)\n",
    "p2s_summary  = tf.summary.scalar('rho2_acc'  , rho2_acc)\n",
    "# hid1_hist    = tf.summary.histogram('hidden1', hidden1)\n",
    "# hid2_hist    = tf.summary.histogram('hidden1', hidden1)\n",
    "# hid3_hist    = tf.summary.histogram('hidden1', hidden1)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "n_epochs   = 10\n",
    "n_batches  = n_outputs // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size, trainingNow=True):\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    indices = np.random.randint(y_train.size, size=batch_size)\n",
    "    X_batch = x_train[indices]\n",
    "    y_batch = y_train.reshape(-1, 1)[indices]\n",
    "    u_batch = y_train_err.reshape(-1, 1)[indices]\n",
    "    \n",
    "    return {training: trainingNow, X: X_batch, y: y_batch, unc:unc_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_acc_batch(epoch, batch_index, batch_size, batch_size_mod=10, trainingNow=False):\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    indices = np.random.randint(y_val.size, size=batch_size_mod*batch_size)\n",
    "    X_batch = x_val[indices]\n",
    "    y_batch = y_val.reshape(-1, 1)[indices]\n",
    "    u_batch = y_val_err.reshape(-1, 1)[indices]\n",
    "    return {training: trainingNow, X: X_batch, y: y_batch, unc:unc_batch}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/spitzer\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_tensorboard(epoch, batch_index, feed_dict_now):\n",
    "    step = epoch * n_batches + batch_index\n",
    "    \n",
    "    list_of_summaries = [nll_summary, mse_summary, loss_summary, r2s_summary, p2s_summary]\n",
    "    \n",
    "    for summary in list_of_summaries:\n",
    "        file_writer.add_summary(summary=summary.eval(feed_dict=feed_dict_now), global_step=step)\n",
    "    \n",
    "    # summary_str_nll = nll_summary.eval(feed_dict=feed_dict_now)\n",
    "    # summary_str_mse = mse_summary.eval(feed_dict=feed_dict_now)\n",
    "    # summary_str_los = loss_summary.eval(feed_dict=feed_dict_now)\n",
    "    # summary_str_r2s = r2s_summary.eval(feed_dict=feed_dict_now)\n",
    "    # summary_str_p2s = p2s_summary.eval(feed_dict=feed_dict_now)\n",
    "    # \n",
    "    # file_writer.add_summary(summary=summary_str_mse, global_step=step)\n",
    "    # file_writer.add_summary(summary=summary_str_los, global_step=step)\n",
    "    # file_writer.add_summary(summary=summary_str_r2s, global_step=step)\n",
    "    # file_writer.add_summary(summary=summary_str_p2s, global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_update(feed_dict_now, epoch, batch_index, batch_size, n_batches, batch_size_mod=10):\n",
    "    \n",
    "    acc_test  = accuracy.eval(feed_dict=fetch_acc_batch(epoch, batch_index, batch_size))\n",
    "    acc_train = accuracy.eval(feed_dict=feed_dict_now)\n",
    "    \n",
    "    step = epoch * n_batches + batch_index\n",
    "    print(epoch, step, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in tqdm_notebook(range(n_epochs), total=n_epochs, desc='epoch'):\n",
    "        for batch_index in tqdm_notebook(range(n_batches), total=n_batches, desc='batch'):\n",
    "            feed_dict_now = fetch_batch(epoch, batch_index, batch_size)\n",
    "            \n",
    "            if batch_index % 10 == 0 and batch_index > 0: \n",
    "                write_to_tensorboard(epoch, batch_index, feed_dict_now)\n",
    "                print_update(feed_dict_now, epoch, batch_index, batch_size, n_batches)\n",
    "            \n",
    "            sess.run(training_op, feed_dict=feed_dict_now)\n",
    "        \n",
    "        print_update(feed_dict_now, epoch, batch_index, batch_size, n_batches, batch_size_mod=100)\n",
    "        # acc_test  = accuracy.eval(feed_dict=fetch_acc_batch(batch_index, batch_size_mod=100))\n",
    "        # acc_train = accuracy.eval(feed_dict=feed_dict_now)\n",
    "        # print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "    \n",
    "    save_path = saver.save(sess, MODEL_DIR + \"/spitzer_calibration_20_20_20_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "09e6b3b1-0c96-df80-85e7-31e958018309"
   },
   "source": [
    "## Training and Evaluating Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add progress bar through python `logging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "109a3d46-d5b6-93f6-d792-14765b531b8d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Our Model\n",
    "nFitSteps = 100000\n",
    "start = time()\n",
    "wrap  = regressor.fit(input_fn=train_input_fn, steps=nFitSteps)\n",
    "print('TF Regressor took {} seconds'.format(time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7655bb62-0183-1778-4c86-a69ec5d3a3cf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluating Our Model\n",
    "print('Evaluating ...')\n",
    "results = regressor.evaluate(input_fn=test_input_fn, steps=1)\n",
    "\n",
    "for key in sorted(results):\n",
    "    print(\"{}: {}\".format(key, results[key]))\n",
    "\n",
    "print(\"Val Acc: {:.3f}\".format((1-results['loss'])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Track Scalable Growth**\n",
    "\n",
    "Shrunk data set to 23559 Training samples and 7853 Val/Test samples\n",
    "\n",
    "| n_iters | time (s) | val acc | multicore | gpu |\n",
    "|------------------------------------------------|\n",
    "|  100    |   5.869  |  6.332 | yes | no |\n",
    "|  200    |   6.380  | 13.178 | yes | no |\n",
    "|  500    |   8.656  | 54.220 | yes | no |\n",
    "|  1000   |  12.170  | 66.596 | yes | no |\n",
    "|  2000   |  19.891  | 62.996 | yes | no |\n",
    "|  5000   |  43.589  | 76.586 | yes | no |\n",
    "|  10000  |  80.581  | 66.872 | yes | no |\n",
    "|  20000  | 162.435  | 78.927 | yes | no |\n",
    "|  50000  | 535.584  | 75.493 | yes | no |\n",
    "|  100000 | 1062.656 | 73.162 | yes | no |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nItersList = [100,200,500,1000,2000,5000,10000,20000,50000,100000]\n",
    "rtimesList = [5.869, 6.380, 8.656, 12.170, 19.891, 43.589, 80.581, 162.435, 535.584, 1062.656]\n",
    "valAccList = [6.332, 13.178, 54.220, 66.596, 62.996, 76.586, 66.872, 78.927, 75.493, 73.162]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.loglog(nItersList, rtimesList,'o-');\n",
    "plt.twinx()\n",
    "plt.semilogx(nItersList, valAccList,'o-', color='orange');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c418c90-cbda-b772-f3be-bc9107bca2f8"
   },
   "source": [
    "## Predicting output for test data\n",
    "\n",
    "Most of the time prediction script would be separate from training script (we need not to train on same data again) but I am providing both in same script here; as I am not sure if we can create multiple notebook and somehow share data between them in Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def de_median(x):\n",
    "    return x - np.median(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_output = list(regressor.predict(input_fn=test_input_fn))\n",
    "# x = list(predicted_output)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "c4e45f26-f03e-9079-65a6-b13871b24a46",
    "scrolled": false
   },
   "source": [
    "# print([predicted_output() for _ in range(10)])\n",
    "plt.plot((predicted_output - np.median(predicted_output)) / np.std(predicted_output),'.',alpha=0.1);\n",
    "plt.plot((test_df['flux'].values - np.median(test_df['flux'].values)) / np.std(test_df['flux'].values),'.',alpha=0.1);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(de_median(x - test_df['flux'].values)/x,'.',alpha=0.1);\n",
    "plt.ylim(-1.,1.);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r2_score(test_df['flux'].values,predicted_output)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Full notebook took {} seconds'.format(time()-start0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "saveDir = 'tfSaveModels'\n",
    "regressor.export_savedmodel(saveDir, regressor)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "saveDir = 'tfSaveModels'\n",
    "reg_args = {'feature_columns': fc, 'hidden_units': hu_array, ...}\n",
    "regressor = tf.contrib.learn.DNNRegressor(**reg_args)\n",
    "pickle.dump(reg_args, open('reg_args.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "reg_args = pickle.load(open('reg_args.pkl', 'rb'))\n",
    "# On another machine and so my model dir path changed:\n",
    "reg_args['model_dir'] = NEW_MODEL_DIR\n",
    "regressor = tf.contrib.learn.DNNRegressor(**reg_args)"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 2,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
