{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fa1a9421-50b6-7ae4-0b03-212f533f8fcb"
   },
   "source": [
    "## TF-Core - eLU - Spitzer Calibration Data\n",
    "\n",
    "This script show a simple example of using [tf.contrib.learn][1] library to create our model.\n",
    "\n",
    "The code is divided in following steps:\n",
    "\n",
    " - Load CSVs data\n",
    " - Continuous features\n",
    " - Converting Data into Tensors\n",
    " - Selecting and Engineering Features for the Model\n",
    " - Defining The Regression Model\n",
    " - Training and Evaluating Our Model\n",
    " - Predicting output for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "5259819c-f765-e649-5e67-7bcde8665463",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, minmax_scale\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from time import time\n",
    "start0 = time()\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "# %matplotlib inline\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.rcParams['axes.labelsize'] = 14\n",
    "# plt.rcParams['xtick.labelsize'] = 12\n",
    "# plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "# PROJECT_ROOT_DIR = \".\"\n",
    "# CHAPTER_ID = \"ann\"\n",
    "\n",
    "# def save_fig(fig_id, tight_layout=True):\n",
    "#     path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "#     print(\"Saving figure\", fig_id)\n",
    "#     if tight_layout:\n",
    "#         plt.tight_layout()\n",
    "#     plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0e7f9a40-2b80-bdea-d5aa-d52a87577e8c"
   },
   "source": [
    "## Load CSVs data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "cb7edeab-c13e-8a29-34dc-2e9d0bd7059b"
   },
   "source": [
    "df_train_ori = pd.read_csv('train.csv')\n",
    "df_test_ori = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nSkip = 20\n",
    "spitzerDataRaw  = pd.read_csv('pmap_ch2_0p1s_x4_rmulti_s3_7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pix1</th>\n",
       "      <th>pix2</th>\n",
       "      <th>pix3</th>\n",
       "      <th>pix4</th>\n",
       "      <th>pix5</th>\n",
       "      <th>pix6</th>\n",
       "      <th>pix7</th>\n",
       "      <th>pix8</th>\n",
       "      <th>pix9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>577.447021</td>\n",
       "      <td>3465.876709</td>\n",
       "      <td>1118.598145</td>\n",
       "      <td>550.165466</td>\n",
       "      <td>2460.376953</td>\n",
       "      <td>994.374207</td>\n",
       "      <td>141.741592</td>\n",
       "      <td>521.385254</td>\n",
       "      <td>694.330688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>569.863098</td>\n",
       "      <td>3387.739258</td>\n",
       "      <td>1087.530762</td>\n",
       "      <td>556.717407</td>\n",
       "      <td>2552.070557</td>\n",
       "      <td>1021.892700</td>\n",
       "      <td>134.061081</td>\n",
       "      <td>511.347778</td>\n",
       "      <td>666.346069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552.641235</td>\n",
       "      <td>3405.411377</td>\n",
       "      <td>1082.131104</td>\n",
       "      <td>558.981445</td>\n",
       "      <td>2560.040771</td>\n",
       "      <td>1058.485352</td>\n",
       "      <td>146.488220</td>\n",
       "      <td>513.809570</td>\n",
       "      <td>691.019653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>571.821167</td>\n",
       "      <td>3340.533691</td>\n",
       "      <td>1073.962036</td>\n",
       "      <td>568.324768</td>\n",
       "      <td>2643.155273</td>\n",
       "      <td>1024.431641</td>\n",
       "      <td>147.687546</td>\n",
       "      <td>525.451538</td>\n",
       "      <td>727.683472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>538.292114</td>\n",
       "      <td>3248.569336</td>\n",
       "      <td>1021.301208</td>\n",
       "      <td>548.598145</td>\n",
       "      <td>2691.563965</td>\n",
       "      <td>1066.199707</td>\n",
       "      <td>154.170990</td>\n",
       "      <td>541.407532</td>\n",
       "      <td>718.537537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>553.332214</td>\n",
       "      <td>3183.050293</td>\n",
       "      <td>1026.863281</td>\n",
       "      <td>578.003784</td>\n",
       "      <td>2679.043457</td>\n",
       "      <td>1085.607422</td>\n",
       "      <td>154.789886</td>\n",
       "      <td>517.848328</td>\n",
       "      <td>722.456909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>541.202332</td>\n",
       "      <td>3137.938232</td>\n",
       "      <td>1035.948364</td>\n",
       "      <td>589.566528</td>\n",
       "      <td>2743.923584</td>\n",
       "      <td>1072.393555</td>\n",
       "      <td>158.401291</td>\n",
       "      <td>539.485718</td>\n",
       "      <td>724.184937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>547.699829</td>\n",
       "      <td>3057.428467</td>\n",
       "      <td>1034.417603</td>\n",
       "      <td>590.498108</td>\n",
       "      <td>2818.238770</td>\n",
       "      <td>1118.098633</td>\n",
       "      <td>167.132004</td>\n",
       "      <td>540.160950</td>\n",
       "      <td>760.767334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>534.724976</td>\n",
       "      <td>3015.335205</td>\n",
       "      <td>1007.406494</td>\n",
       "      <td>574.792358</td>\n",
       "      <td>2876.417236</td>\n",
       "      <td>1141.010498</td>\n",
       "      <td>158.403992</td>\n",
       "      <td>527.611633</td>\n",
       "      <td>765.304077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>529.423950</td>\n",
       "      <td>3010.478027</td>\n",
       "      <td>973.444946</td>\n",
       "      <td>578.736755</td>\n",
       "      <td>2875.608887</td>\n",
       "      <td>1106.352905</td>\n",
       "      <td>163.936325</td>\n",
       "      <td>540.751587</td>\n",
       "      <td>746.721191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>543.917847</td>\n",
       "      <td>2959.855225</td>\n",
       "      <td>1002.942017</td>\n",
       "      <td>600.163574</td>\n",
       "      <td>2921.819580</td>\n",
       "      <td>1124.441772</td>\n",
       "      <td>153.599976</td>\n",
       "      <td>548.917603</td>\n",
       "      <td>735.936890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>415.543091</td>\n",
       "      <td>1085.521729</td>\n",
       "      <td>446.313171</td>\n",
       "      <td>1641.077881</td>\n",
       "      <td>4778.175781</td>\n",
       "      <td>319.484497</td>\n",
       "      <td>418.018860</td>\n",
       "      <td>810.050232</td>\n",
       "      <td>567.318176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>386.626038</td>\n",
       "      <td>990.775635</td>\n",
       "      <td>455.084015</td>\n",
       "      <td>1653.742676</td>\n",
       "      <td>4831.918457</td>\n",
       "      <td>318.239410</td>\n",
       "      <td>428.828278</td>\n",
       "      <td>836.078308</td>\n",
       "      <td>576.660156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>352.204071</td>\n",
       "      <td>909.555847</td>\n",
       "      <td>448.546478</td>\n",
       "      <td>1697.422363</td>\n",
       "      <td>4942.354004</td>\n",
       "      <td>315.904175</td>\n",
       "      <td>434.170258</td>\n",
       "      <td>822.891968</td>\n",
       "      <td>576.275024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>356.336823</td>\n",
       "      <td>830.570557</td>\n",
       "      <td>446.085022</td>\n",
       "      <td>1725.667725</td>\n",
       "      <td>4915.351074</td>\n",
       "      <td>312.517944</td>\n",
       "      <td>455.358154</td>\n",
       "      <td>826.022034</td>\n",
       "      <td>565.244507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>346.803467</td>\n",
       "      <td>827.159546</td>\n",
       "      <td>445.986084</td>\n",
       "      <td>1783.878418</td>\n",
       "      <td>4926.551270</td>\n",
       "      <td>299.384399</td>\n",
       "      <td>454.638336</td>\n",
       "      <td>856.649109</td>\n",
       "      <td>572.549500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>319.600525</td>\n",
       "      <td>778.882324</td>\n",
       "      <td>452.693604</td>\n",
       "      <td>1766.618774</td>\n",
       "      <td>4900.985352</td>\n",
       "      <td>295.125763</td>\n",
       "      <td>455.190002</td>\n",
       "      <td>873.377808</td>\n",
       "      <td>581.590820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>318.834503</td>\n",
       "      <td>772.429138</td>\n",
       "      <td>450.526428</td>\n",
       "      <td>1804.441284</td>\n",
       "      <td>4888.824219</td>\n",
       "      <td>292.672455</td>\n",
       "      <td>460.712219</td>\n",
       "      <td>873.199463</td>\n",
       "      <td>557.806458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>304.534882</td>\n",
       "      <td>747.088501</td>\n",
       "      <td>433.972229</td>\n",
       "      <td>1812.826172</td>\n",
       "      <td>4949.355469</td>\n",
       "      <td>292.384491</td>\n",
       "      <td>459.964996</td>\n",
       "      <td>883.855896</td>\n",
       "      <td>556.133789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>297.334167</td>\n",
       "      <td>711.377441</td>\n",
       "      <td>427.756256</td>\n",
       "      <td>1824.887939</td>\n",
       "      <td>4977.011230</td>\n",
       "      <td>288.222961</td>\n",
       "      <td>442.602753</td>\n",
       "      <td>883.110352</td>\n",
       "      <td>578.062012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>291.785248</td>\n",
       "      <td>696.303345</td>\n",
       "      <td>434.420410</td>\n",
       "      <td>1808.774414</td>\n",
       "      <td>4967.067383</td>\n",
       "      <td>288.906006</td>\n",
       "      <td>465.871857</td>\n",
       "      <td>848.896790</td>\n",
       "      <td>584.660889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>275.729004</td>\n",
       "      <td>684.407776</td>\n",
       "      <td>436.270630</td>\n",
       "      <td>1839.743286</td>\n",
       "      <td>4958.617188</td>\n",
       "      <td>292.503906</td>\n",
       "      <td>473.751465</td>\n",
       "      <td>874.837891</td>\n",
       "      <td>576.924805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>267.109802</td>\n",
       "      <td>668.616150</td>\n",
       "      <td>428.174561</td>\n",
       "      <td>1800.920166</td>\n",
       "      <td>5008.872559</td>\n",
       "      <td>289.184784</td>\n",
       "      <td>474.868591</td>\n",
       "      <td>872.165161</td>\n",
       "      <td>579.913147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>272.932220</td>\n",
       "      <td>664.508301</td>\n",
       "      <td>437.244934</td>\n",
       "      <td>1776.687744</td>\n",
       "      <td>5002.353027</td>\n",
       "      <td>288.002045</td>\n",
       "      <td>470.427643</td>\n",
       "      <td>882.427673</td>\n",
       "      <td>600.059509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>269.932495</td>\n",
       "      <td>641.567139</td>\n",
       "      <td>432.846771</td>\n",
       "      <td>1789.313354</td>\n",
       "      <td>5064.815918</td>\n",
       "      <td>303.351807</td>\n",
       "      <td>484.210266</td>\n",
       "      <td>894.807068</td>\n",
       "      <td>602.440247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>265.979431</td>\n",
       "      <td>648.039856</td>\n",
       "      <td>432.684937</td>\n",
       "      <td>1755.147705</td>\n",
       "      <td>5088.369141</td>\n",
       "      <td>288.072937</td>\n",
       "      <td>482.552338</td>\n",
       "      <td>882.221191</td>\n",
       "      <td>592.758179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>564.362610</td>\n",
       "      <td>1541.839600</td>\n",
       "      <td>431.914764</td>\n",
       "      <td>1446.160400</td>\n",
       "      <td>4362.256348</td>\n",
       "      <td>345.700531</td>\n",
       "      <td>401.668152</td>\n",
       "      <td>821.724915</td>\n",
       "      <td>520.839478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>506.248077</td>\n",
       "      <td>1415.009277</td>\n",
       "      <td>443.126648</td>\n",
       "      <td>1487.735840</td>\n",
       "      <td>4414.598633</td>\n",
       "      <td>347.017731</td>\n",
       "      <td>393.479889</td>\n",
       "      <td>795.969360</td>\n",
       "      <td>549.537842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>515.429810</td>\n",
       "      <td>1315.711914</td>\n",
       "      <td>448.500977</td>\n",
       "      <td>1547.112671</td>\n",
       "      <td>4509.289062</td>\n",
       "      <td>323.252441</td>\n",
       "      <td>394.831543</td>\n",
       "      <td>821.549072</td>\n",
       "      <td>529.604004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>476.979919</td>\n",
       "      <td>1244.519043</td>\n",
       "      <td>429.070221</td>\n",
       "      <td>1599.996094</td>\n",
       "      <td>4529.244629</td>\n",
       "      <td>330.517914</td>\n",
       "      <td>398.165985</td>\n",
       "      <td>820.598389</td>\n",
       "      <td>528.216064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785255</th>\n",
       "      <td>475.147705</td>\n",
       "      <td>2842.766357</td>\n",
       "      <td>591.782959</td>\n",
       "      <td>633.689026</td>\n",
       "      <td>3837.167969</td>\n",
       "      <td>771.729736</td>\n",
       "      <td>238.382645</td>\n",
       "      <td>637.508911</td>\n",
       "      <td>712.187988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785256</th>\n",
       "      <td>475.464050</td>\n",
       "      <td>2835.344727</td>\n",
       "      <td>583.647827</td>\n",
       "      <td>640.641846</td>\n",
       "      <td>3873.379639</td>\n",
       "      <td>750.115234</td>\n",
       "      <td>248.780396</td>\n",
       "      <td>651.230103</td>\n",
       "      <td>716.854736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785257</th>\n",
       "      <td>481.847534</td>\n",
       "      <td>2817.340820</td>\n",
       "      <td>578.740479</td>\n",
       "      <td>637.754578</td>\n",
       "      <td>3877.218262</td>\n",
       "      <td>781.961548</td>\n",
       "      <td>241.087128</td>\n",
       "      <td>650.161316</td>\n",
       "      <td>733.283325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785258</th>\n",
       "      <td>465.787201</td>\n",
       "      <td>2804.904053</td>\n",
       "      <td>569.343262</td>\n",
       "      <td>647.821289</td>\n",
       "      <td>3857.104492</td>\n",
       "      <td>767.500122</td>\n",
       "      <td>240.323898</td>\n",
       "      <td>628.991760</td>\n",
       "      <td>726.848083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785259</th>\n",
       "      <td>473.557251</td>\n",
       "      <td>2804.657471</td>\n",
       "      <td>561.592529</td>\n",
       "      <td>633.638123</td>\n",
       "      <td>3901.351562</td>\n",
       "      <td>773.559692</td>\n",
       "      <td>231.294861</td>\n",
       "      <td>641.328003</td>\n",
       "      <td>733.939087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785260</th>\n",
       "      <td>476.991699</td>\n",
       "      <td>2791.731445</td>\n",
       "      <td>579.058228</td>\n",
       "      <td>643.239990</td>\n",
       "      <td>3923.991455</td>\n",
       "      <td>765.078247</td>\n",
       "      <td>235.408035</td>\n",
       "      <td>661.097107</td>\n",
       "      <td>718.653198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785261</th>\n",
       "      <td>476.635651</td>\n",
       "      <td>2750.070068</td>\n",
       "      <td>563.718689</td>\n",
       "      <td>635.713318</td>\n",
       "      <td>3949.495361</td>\n",
       "      <td>772.300659</td>\n",
       "      <td>246.790222</td>\n",
       "      <td>632.139221</td>\n",
       "      <td>730.945618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785262</th>\n",
       "      <td>481.483948</td>\n",
       "      <td>2736.324219</td>\n",
       "      <td>567.592407</td>\n",
       "      <td>660.586548</td>\n",
       "      <td>3944.277100</td>\n",
       "      <td>766.693787</td>\n",
       "      <td>241.954529</td>\n",
       "      <td>633.959961</td>\n",
       "      <td>734.623413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785263</th>\n",
       "      <td>472.956024</td>\n",
       "      <td>2736.230225</td>\n",
       "      <td>562.863098</td>\n",
       "      <td>659.623413</td>\n",
       "      <td>4004.276367</td>\n",
       "      <td>764.286011</td>\n",
       "      <td>239.283524</td>\n",
       "      <td>660.717346</td>\n",
       "      <td>713.998047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785264</th>\n",
       "      <td>466.867065</td>\n",
       "      <td>2732.458008</td>\n",
       "      <td>569.005798</td>\n",
       "      <td>666.330994</td>\n",
       "      <td>4024.462891</td>\n",
       "      <td>769.562134</td>\n",
       "      <td>245.346207</td>\n",
       "      <td>663.933350</td>\n",
       "      <td>725.252869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785265</th>\n",
       "      <td>480.046295</td>\n",
       "      <td>2715.175537</td>\n",
       "      <td>557.256165</td>\n",
       "      <td>645.355225</td>\n",
       "      <td>4037.946533</td>\n",
       "      <td>742.447998</td>\n",
       "      <td>258.840576</td>\n",
       "      <td>654.178589</td>\n",
       "      <td>706.549988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785266</th>\n",
       "      <td>475.449463</td>\n",
       "      <td>2769.531494</td>\n",
       "      <td>536.281189</td>\n",
       "      <td>655.740417</td>\n",
       "      <td>4031.584229</td>\n",
       "      <td>730.941650</td>\n",
       "      <td>255.209244</td>\n",
       "      <td>658.127136</td>\n",
       "      <td>719.966553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785267</th>\n",
       "      <td>502.837372</td>\n",
       "      <td>2716.243408</td>\n",
       "      <td>554.105591</td>\n",
       "      <td>677.264648</td>\n",
       "      <td>3980.764404</td>\n",
       "      <td>730.059265</td>\n",
       "      <td>259.389648</td>\n",
       "      <td>667.066284</td>\n",
       "      <td>708.509277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785268</th>\n",
       "      <td>480.399200</td>\n",
       "      <td>2769.769531</td>\n",
       "      <td>562.770813</td>\n",
       "      <td>679.593140</td>\n",
       "      <td>3983.617920</td>\n",
       "      <td>712.878052</td>\n",
       "      <td>243.168762</td>\n",
       "      <td>654.615112</td>\n",
       "      <td>689.989075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785269</th>\n",
       "      <td>480.807373</td>\n",
       "      <td>2759.118408</td>\n",
       "      <td>561.283997</td>\n",
       "      <td>699.134888</td>\n",
       "      <td>4009.483398</td>\n",
       "      <td>721.416626</td>\n",
       "      <td>262.486328</td>\n",
       "      <td>654.512329</td>\n",
       "      <td>713.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785270</th>\n",
       "      <td>523.245300</td>\n",
       "      <td>2958.067139</td>\n",
       "      <td>543.905029</td>\n",
       "      <td>697.710938</td>\n",
       "      <td>3776.409424</td>\n",
       "      <td>666.272888</td>\n",
       "      <td>263.427338</td>\n",
       "      <td>651.643799</td>\n",
       "      <td>670.494263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785271</th>\n",
       "      <td>495.875732</td>\n",
       "      <td>2955.598145</td>\n",
       "      <td>552.345215</td>\n",
       "      <td>657.297363</td>\n",
       "      <td>3747.275879</td>\n",
       "      <td>703.948242</td>\n",
       "      <td>250.976898</td>\n",
       "      <td>683.805420</td>\n",
       "      <td>657.604126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785272</th>\n",
       "      <td>516.237366</td>\n",
       "      <td>2964.318604</td>\n",
       "      <td>577.262634</td>\n",
       "      <td>668.211304</td>\n",
       "      <td>3793.730225</td>\n",
       "      <td>709.129517</td>\n",
       "      <td>256.492645</td>\n",
       "      <td>667.471802</td>\n",
       "      <td>683.404236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785273</th>\n",
       "      <td>501.681335</td>\n",
       "      <td>2919.767578</td>\n",
       "      <td>550.606995</td>\n",
       "      <td>638.708374</td>\n",
       "      <td>3802.873535</td>\n",
       "      <td>717.027649</td>\n",
       "      <td>250.493073</td>\n",
       "      <td>644.383301</td>\n",
       "      <td>702.473267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785274</th>\n",
       "      <td>510.000214</td>\n",
       "      <td>2995.055420</td>\n",
       "      <td>551.373047</td>\n",
       "      <td>660.349304</td>\n",
       "      <td>3745.079590</td>\n",
       "      <td>722.202332</td>\n",
       "      <td>247.733948</td>\n",
       "      <td>642.584961</td>\n",
       "      <td>695.485779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785275</th>\n",
       "      <td>521.931641</td>\n",
       "      <td>3013.854004</td>\n",
       "      <td>554.299988</td>\n",
       "      <td>631.953613</td>\n",
       "      <td>3710.329346</td>\n",
       "      <td>728.134094</td>\n",
       "      <td>243.888962</td>\n",
       "      <td>643.002563</td>\n",
       "      <td>691.181763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785276</th>\n",
       "      <td>495.807709</td>\n",
       "      <td>2989.654053</td>\n",
       "      <td>585.109985</td>\n",
       "      <td>639.402100</td>\n",
       "      <td>3714.461670</td>\n",
       "      <td>749.341003</td>\n",
       "      <td>235.572479</td>\n",
       "      <td>646.545593</td>\n",
       "      <td>676.286865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785277</th>\n",
       "      <td>512.751160</td>\n",
       "      <td>2958.394531</td>\n",
       "      <td>582.242310</td>\n",
       "      <td>652.946533</td>\n",
       "      <td>3707.906250</td>\n",
       "      <td>729.102295</td>\n",
       "      <td>234.076538</td>\n",
       "      <td>652.182556</td>\n",
       "      <td>688.491699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785278</th>\n",
       "      <td>504.160034</td>\n",
       "      <td>2989.499268</td>\n",
       "      <td>587.763611</td>\n",
       "      <td>658.194824</td>\n",
       "      <td>3695.002930</td>\n",
       "      <td>736.696594</td>\n",
       "      <td>231.465927</td>\n",
       "      <td>651.476196</td>\n",
       "      <td>681.635620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785279</th>\n",
       "      <td>516.517090</td>\n",
       "      <td>2976.608643</td>\n",
       "      <td>586.910278</td>\n",
       "      <td>637.642395</td>\n",
       "      <td>3714.598877</td>\n",
       "      <td>752.355835</td>\n",
       "      <td>235.402084</td>\n",
       "      <td>637.375977</td>\n",
       "      <td>700.220703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785280</th>\n",
       "      <td>506.839264</td>\n",
       "      <td>3055.761230</td>\n",
       "      <td>596.532349</td>\n",
       "      <td>647.959351</td>\n",
       "      <td>3610.522217</td>\n",
       "      <td>731.254211</td>\n",
       "      <td>234.202530</td>\n",
       "      <td>658.012695</td>\n",
       "      <td>675.393433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785281</th>\n",
       "      <td>495.875641</td>\n",
       "      <td>3055.138428</td>\n",
       "      <td>590.330444</td>\n",
       "      <td>623.768982</td>\n",
       "      <td>3686.792236</td>\n",
       "      <td>732.337524</td>\n",
       "      <td>234.055603</td>\n",
       "      <td>654.529114</td>\n",
       "      <td>690.857361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785282</th>\n",
       "      <td>510.378479</td>\n",
       "      <td>3120.925781</td>\n",
       "      <td>585.978516</td>\n",
       "      <td>623.555176</td>\n",
       "      <td>3621.902100</td>\n",
       "      <td>728.794128</td>\n",
       "      <td>230.795486</td>\n",
       "      <td>644.155518</td>\n",
       "      <td>694.674377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785283</th>\n",
       "      <td>520.521362</td>\n",
       "      <td>3118.752197</td>\n",
       "      <td>582.462891</td>\n",
       "      <td>632.206604</td>\n",
       "      <td>3600.458984</td>\n",
       "      <td>756.452515</td>\n",
       "      <td>224.326355</td>\n",
       "      <td>641.459534</td>\n",
       "      <td>696.739502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785284</th>\n",
       "      <td>520.870972</td>\n",
       "      <td>3144.105225</td>\n",
       "      <td>609.523743</td>\n",
       "      <td>617.985229</td>\n",
       "      <td>3561.716309</td>\n",
       "      <td>752.712402</td>\n",
       "      <td>225.764816</td>\n",
       "      <td>620.627991</td>\n",
       "      <td>680.224121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>785285 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              pix1         pix2         pix3         pix4         pix5  \\\n",
       "0       577.447021  3465.876709  1118.598145   550.165466  2460.376953   \n",
       "1       569.863098  3387.739258  1087.530762   556.717407  2552.070557   \n",
       "2       552.641235  3405.411377  1082.131104   558.981445  2560.040771   \n",
       "3       571.821167  3340.533691  1073.962036   568.324768  2643.155273   \n",
       "4       538.292114  3248.569336  1021.301208   548.598145  2691.563965   \n",
       "5       553.332214  3183.050293  1026.863281   578.003784  2679.043457   \n",
       "6       541.202332  3137.938232  1035.948364   589.566528  2743.923584   \n",
       "7       547.699829  3057.428467  1034.417603   590.498108  2818.238770   \n",
       "8       534.724976  3015.335205  1007.406494   574.792358  2876.417236   \n",
       "9       529.423950  3010.478027   973.444946   578.736755  2875.608887   \n",
       "10      543.917847  2959.855225  1002.942017   600.163574  2921.819580   \n",
       "11      415.543091  1085.521729   446.313171  1641.077881  4778.175781   \n",
       "12      386.626038   990.775635   455.084015  1653.742676  4831.918457   \n",
       "13      352.204071   909.555847   448.546478  1697.422363  4942.354004   \n",
       "14      356.336823   830.570557   446.085022  1725.667725  4915.351074   \n",
       "15      346.803467   827.159546   445.986084  1783.878418  4926.551270   \n",
       "16      319.600525   778.882324   452.693604  1766.618774  4900.985352   \n",
       "17      318.834503   772.429138   450.526428  1804.441284  4888.824219   \n",
       "18      304.534882   747.088501   433.972229  1812.826172  4949.355469   \n",
       "19      297.334167   711.377441   427.756256  1824.887939  4977.011230   \n",
       "20      291.785248   696.303345   434.420410  1808.774414  4967.067383   \n",
       "21      275.729004   684.407776   436.270630  1839.743286  4958.617188   \n",
       "22      267.109802   668.616150   428.174561  1800.920166  5008.872559   \n",
       "23      272.932220   664.508301   437.244934  1776.687744  5002.353027   \n",
       "24      269.932495   641.567139   432.846771  1789.313354  5064.815918   \n",
       "25      265.979431   648.039856   432.684937  1755.147705  5088.369141   \n",
       "26      564.362610  1541.839600   431.914764  1446.160400  4362.256348   \n",
       "27      506.248077  1415.009277   443.126648  1487.735840  4414.598633   \n",
       "28      515.429810  1315.711914   448.500977  1547.112671  4509.289062   \n",
       "29      476.979919  1244.519043   429.070221  1599.996094  4529.244629   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "785255  475.147705  2842.766357   591.782959   633.689026  3837.167969   \n",
       "785256  475.464050  2835.344727   583.647827   640.641846  3873.379639   \n",
       "785257  481.847534  2817.340820   578.740479   637.754578  3877.218262   \n",
       "785258  465.787201  2804.904053   569.343262   647.821289  3857.104492   \n",
       "785259  473.557251  2804.657471   561.592529   633.638123  3901.351562   \n",
       "785260  476.991699  2791.731445   579.058228   643.239990  3923.991455   \n",
       "785261  476.635651  2750.070068   563.718689   635.713318  3949.495361   \n",
       "785262  481.483948  2736.324219   567.592407   660.586548  3944.277100   \n",
       "785263  472.956024  2736.230225   562.863098   659.623413  4004.276367   \n",
       "785264  466.867065  2732.458008   569.005798   666.330994  4024.462891   \n",
       "785265  480.046295  2715.175537   557.256165   645.355225  4037.946533   \n",
       "785266  475.449463  2769.531494   536.281189   655.740417  4031.584229   \n",
       "785267  502.837372  2716.243408   554.105591   677.264648  3980.764404   \n",
       "785268  480.399200  2769.769531   562.770813   679.593140  3983.617920   \n",
       "785269  480.807373  2759.118408   561.283997   699.134888  4009.483398   \n",
       "785270  523.245300  2958.067139   543.905029   697.710938  3776.409424   \n",
       "785271  495.875732  2955.598145   552.345215   657.297363  3747.275879   \n",
       "785272  516.237366  2964.318604   577.262634   668.211304  3793.730225   \n",
       "785273  501.681335  2919.767578   550.606995   638.708374  3802.873535   \n",
       "785274  510.000214  2995.055420   551.373047   660.349304  3745.079590   \n",
       "785275  521.931641  3013.854004   554.299988   631.953613  3710.329346   \n",
       "785276  495.807709  2989.654053   585.109985   639.402100  3714.461670   \n",
       "785277  512.751160  2958.394531   582.242310   652.946533  3707.906250   \n",
       "785278  504.160034  2989.499268   587.763611   658.194824  3695.002930   \n",
       "785279  516.517090  2976.608643   586.910278   637.642395  3714.598877   \n",
       "785280  506.839264  3055.761230   596.532349   647.959351  3610.522217   \n",
       "785281  495.875641  3055.138428   590.330444   623.768982  3686.792236   \n",
       "785282  510.378479  3120.925781   585.978516   623.555176  3621.902100   \n",
       "785283  520.521362  3118.752197   582.462891   632.206604  3600.458984   \n",
       "785284  520.870972  3144.105225   609.523743   617.985229  3561.716309   \n",
       "\n",
       "               pix6        pix7        pix8        pix9  \n",
       "0        994.374207  141.741592  521.385254  694.330688  \n",
       "1       1021.892700  134.061081  511.347778  666.346069  \n",
       "2       1058.485352  146.488220  513.809570  691.019653  \n",
       "3       1024.431641  147.687546  525.451538  727.683472  \n",
       "4       1066.199707  154.170990  541.407532  718.537537  \n",
       "5       1085.607422  154.789886  517.848328  722.456909  \n",
       "6       1072.393555  158.401291  539.485718  724.184937  \n",
       "7       1118.098633  167.132004  540.160950  760.767334  \n",
       "8       1141.010498  158.403992  527.611633  765.304077  \n",
       "9       1106.352905  163.936325  540.751587  746.721191  \n",
       "10      1124.441772  153.599976  548.917603  735.936890  \n",
       "11       319.484497  418.018860  810.050232  567.318176  \n",
       "12       318.239410  428.828278  836.078308  576.660156  \n",
       "13       315.904175  434.170258  822.891968  576.275024  \n",
       "14       312.517944  455.358154  826.022034  565.244507  \n",
       "15       299.384399  454.638336  856.649109  572.549500  \n",
       "16       295.125763  455.190002  873.377808  581.590820  \n",
       "17       292.672455  460.712219  873.199463  557.806458  \n",
       "18       292.384491  459.964996  883.855896  556.133789  \n",
       "19       288.222961  442.602753  883.110352  578.062012  \n",
       "20       288.906006  465.871857  848.896790  584.660889  \n",
       "21       292.503906  473.751465  874.837891  576.924805  \n",
       "22       289.184784  474.868591  872.165161  579.913147  \n",
       "23       288.002045  470.427643  882.427673  600.059509  \n",
       "24       303.351807  484.210266  894.807068  602.440247  \n",
       "25       288.072937  482.552338  882.221191  592.758179  \n",
       "26       345.700531  401.668152  821.724915  520.839478  \n",
       "27       347.017731  393.479889  795.969360  549.537842  \n",
       "28       323.252441  394.831543  821.549072  529.604004  \n",
       "29       330.517914  398.165985  820.598389  528.216064  \n",
       "...             ...         ...         ...         ...  \n",
       "785255   771.729736  238.382645  637.508911  712.187988  \n",
       "785256   750.115234  248.780396  651.230103  716.854736  \n",
       "785257   781.961548  241.087128  650.161316  733.283325  \n",
       "785258   767.500122  240.323898  628.991760  726.848083  \n",
       "785259   773.559692  231.294861  641.328003  733.939087  \n",
       "785260   765.078247  235.408035  661.097107  718.653198  \n",
       "785261   772.300659  246.790222  632.139221  730.945618  \n",
       "785262   766.693787  241.954529  633.959961  734.623413  \n",
       "785263   764.286011  239.283524  660.717346  713.998047  \n",
       "785264   769.562134  245.346207  663.933350  725.252869  \n",
       "785265   742.447998  258.840576  654.178589  706.549988  \n",
       "785266   730.941650  255.209244  658.127136  719.966553  \n",
       "785267   730.059265  259.389648  667.066284  708.509277  \n",
       "785268   712.878052  243.168762  654.615112  689.989075  \n",
       "785269   721.416626  262.486328  654.512329  713.022461  \n",
       "785270   666.272888  263.427338  651.643799  670.494263  \n",
       "785271   703.948242  250.976898  683.805420  657.604126  \n",
       "785272   709.129517  256.492645  667.471802  683.404236  \n",
       "785273   717.027649  250.493073  644.383301  702.473267  \n",
       "785274   722.202332  247.733948  642.584961  695.485779  \n",
       "785275   728.134094  243.888962  643.002563  691.181763  \n",
       "785276   749.341003  235.572479  646.545593  676.286865  \n",
       "785277   729.102295  234.076538  652.182556  688.491699  \n",
       "785278   736.696594  231.465927  651.476196  681.635620  \n",
       "785279   752.355835  235.402084  637.375977  700.220703  \n",
       "785280   731.254211  234.202530  658.012695  675.393433  \n",
       "785281   732.337524  234.055603  654.529114  690.857361  \n",
       "785282   728.794128  230.795486  644.155518  694.674377  \n",
       "785283   756.452515  224.326355  641.459534  696.739502  \n",
       "785284   752.712402  225.764816  620.627991  680.224121  \n",
       "\n",
       "[785285 rows x 9 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PLDpixels = pd.DataFrame({key:spitzerDataRaw[key] for key in spitzerDataRaw.columns.values if 'pix' in key})\n",
    "PLDpixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PLDnorm = np.sum(np.array(PLDpixels),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pix1</th>\n",
       "      <th>pix2</th>\n",
       "      <th>pix3</th>\n",
       "      <th>pix4</th>\n",
       "      <th>pix5</th>\n",
       "      <th>pix6</th>\n",
       "      <th>pix7</th>\n",
       "      <th>pix8</th>\n",
       "      <th>pix9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.054868</td>\n",
       "      <td>0.329321</td>\n",
       "      <td>0.106287</td>\n",
       "      <td>0.052276</td>\n",
       "      <td>0.233781</td>\n",
       "      <td>0.094484</td>\n",
       "      <td>0.013468</td>\n",
       "      <td>0.049541</td>\n",
       "      <td>0.065974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.054337</td>\n",
       "      <td>0.323024</td>\n",
       "      <td>0.103697</td>\n",
       "      <td>0.053084</td>\n",
       "      <td>0.243342</td>\n",
       "      <td>0.097438</td>\n",
       "      <td>0.012783</td>\n",
       "      <td>0.048758</td>\n",
       "      <td>0.063537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.052289</td>\n",
       "      <td>0.322207</td>\n",
       "      <td>0.102387</td>\n",
       "      <td>0.052889</td>\n",
       "      <td>0.242221</td>\n",
       "      <td>0.100150</td>\n",
       "      <td>0.013860</td>\n",
       "      <td>0.048615</td>\n",
       "      <td>0.065382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.053828</td>\n",
       "      <td>0.314461</td>\n",
       "      <td>0.101097</td>\n",
       "      <td>0.053499</td>\n",
       "      <td>0.248813</td>\n",
       "      <td>0.096435</td>\n",
       "      <td>0.013903</td>\n",
       "      <td>0.049463</td>\n",
       "      <td>0.068500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051126</td>\n",
       "      <td>0.308546</td>\n",
       "      <td>0.097002</td>\n",
       "      <td>0.052105</td>\n",
       "      <td>0.255642</td>\n",
       "      <td>0.101267</td>\n",
       "      <td>0.014643</td>\n",
       "      <td>0.051422</td>\n",
       "      <td>0.068246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.052693</td>\n",
       "      <td>0.303119</td>\n",
       "      <td>0.097787</td>\n",
       "      <td>0.055043</td>\n",
       "      <td>0.255123</td>\n",
       "      <td>0.103381</td>\n",
       "      <td>0.014740</td>\n",
       "      <td>0.049314</td>\n",
       "      <td>0.068799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.051333</td>\n",
       "      <td>0.297631</td>\n",
       "      <td>0.098259</td>\n",
       "      <td>0.055920</td>\n",
       "      <td>0.260259</td>\n",
       "      <td>0.101716</td>\n",
       "      <td>0.015024</td>\n",
       "      <td>0.051170</td>\n",
       "      <td>0.068688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.051502</td>\n",
       "      <td>0.287502</td>\n",
       "      <td>0.097271</td>\n",
       "      <td>0.055527</td>\n",
       "      <td>0.265011</td>\n",
       "      <td>0.105139</td>\n",
       "      <td>0.015716</td>\n",
       "      <td>0.050794</td>\n",
       "      <td>0.071538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.050441</td>\n",
       "      <td>0.284439</td>\n",
       "      <td>0.095029</td>\n",
       "      <td>0.054221</td>\n",
       "      <td>0.271334</td>\n",
       "      <td>0.107632</td>\n",
       "      <td>0.014942</td>\n",
       "      <td>0.049770</td>\n",
       "      <td>0.072192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.050299</td>\n",
       "      <td>0.286019</td>\n",
       "      <td>0.092485</td>\n",
       "      <td>0.054984</td>\n",
       "      <td>0.273205</td>\n",
       "      <td>0.105112</td>\n",
       "      <td>0.015575</td>\n",
       "      <td>0.051376</td>\n",
       "      <td>0.070944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.051354</td>\n",
       "      <td>0.279453</td>\n",
       "      <td>0.094692</td>\n",
       "      <td>0.056664</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.106164</td>\n",
       "      <td>0.014502</td>\n",
       "      <td>0.051826</td>\n",
       "      <td>0.069483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.039645</td>\n",
       "      <td>0.103565</td>\n",
       "      <td>0.042581</td>\n",
       "      <td>0.156569</td>\n",
       "      <td>0.455867</td>\n",
       "      <td>0.030481</td>\n",
       "      <td>0.039882</td>\n",
       "      <td>0.077284</td>\n",
       "      <td>0.054126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.036899</td>\n",
       "      <td>0.094558</td>\n",
       "      <td>0.043433</td>\n",
       "      <td>0.157831</td>\n",
       "      <td>0.461151</td>\n",
       "      <td>0.030372</td>\n",
       "      <td>0.040927</td>\n",
       "      <td>0.079794</td>\n",
       "      <td>0.055036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.033545</td>\n",
       "      <td>0.086630</td>\n",
       "      <td>0.042721</td>\n",
       "      <td>0.161670</td>\n",
       "      <td>0.470731</td>\n",
       "      <td>0.030088</td>\n",
       "      <td>0.041352</td>\n",
       "      <td>0.078376</td>\n",
       "      <td>0.054887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.034154</td>\n",
       "      <td>0.079609</td>\n",
       "      <td>0.042756</td>\n",
       "      <td>0.165402</td>\n",
       "      <td>0.471128</td>\n",
       "      <td>0.029954</td>\n",
       "      <td>0.043645</td>\n",
       "      <td>0.079173</td>\n",
       "      <td>0.054178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.032986</td>\n",
       "      <td>0.078675</td>\n",
       "      <td>0.042420</td>\n",
       "      <td>0.169673</td>\n",
       "      <td>0.468588</td>\n",
       "      <td>0.028476</td>\n",
       "      <td>0.043243</td>\n",
       "      <td>0.081480</td>\n",
       "      <td>0.054458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.030660</td>\n",
       "      <td>0.074720</td>\n",
       "      <td>0.043428</td>\n",
       "      <td>0.169475</td>\n",
       "      <td>0.470161</td>\n",
       "      <td>0.028312</td>\n",
       "      <td>0.043667</td>\n",
       "      <td>0.083785</td>\n",
       "      <td>0.055793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.030600</td>\n",
       "      <td>0.074133</td>\n",
       "      <td>0.043239</td>\n",
       "      <td>0.173180</td>\n",
       "      <td>0.469202</td>\n",
       "      <td>0.028089</td>\n",
       "      <td>0.044217</td>\n",
       "      <td>0.083805</td>\n",
       "      <td>0.053535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.029170</td>\n",
       "      <td>0.071559</td>\n",
       "      <td>0.041568</td>\n",
       "      <td>0.173640</td>\n",
       "      <td>0.474071</td>\n",
       "      <td>0.028006</td>\n",
       "      <td>0.044057</td>\n",
       "      <td>0.084660</td>\n",
       "      <td>0.053269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.028507</td>\n",
       "      <td>0.068203</td>\n",
       "      <td>0.041011</td>\n",
       "      <td>0.174959</td>\n",
       "      <td>0.477166</td>\n",
       "      <td>0.027633</td>\n",
       "      <td>0.042434</td>\n",
       "      <td>0.084667</td>\n",
       "      <td>0.055421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.028092</td>\n",
       "      <td>0.067038</td>\n",
       "      <td>0.041825</td>\n",
       "      <td>0.174144</td>\n",
       "      <td>0.478215</td>\n",
       "      <td>0.027815</td>\n",
       "      <td>0.044853</td>\n",
       "      <td>0.081729</td>\n",
       "      <td>0.056289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.026480</td>\n",
       "      <td>0.065728</td>\n",
       "      <td>0.041898</td>\n",
       "      <td>0.176681</td>\n",
       "      <td>0.476205</td>\n",
       "      <td>0.028091</td>\n",
       "      <td>0.045497</td>\n",
       "      <td>0.084016</td>\n",
       "      <td>0.055405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.025709</td>\n",
       "      <td>0.064353</td>\n",
       "      <td>0.041211</td>\n",
       "      <td>0.173335</td>\n",
       "      <td>0.482094</td>\n",
       "      <td>0.027833</td>\n",
       "      <td>0.045705</td>\n",
       "      <td>0.083944</td>\n",
       "      <td>0.055815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.026257</td>\n",
       "      <td>0.063928</td>\n",
       "      <td>0.042064</td>\n",
       "      <td>0.170923</td>\n",
       "      <td>0.481243</td>\n",
       "      <td>0.027707</td>\n",
       "      <td>0.045257</td>\n",
       "      <td>0.084893</td>\n",
       "      <td>0.057728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.025749</td>\n",
       "      <td>0.061199</td>\n",
       "      <td>0.041289</td>\n",
       "      <td>0.170683</td>\n",
       "      <td>0.483133</td>\n",
       "      <td>0.028937</td>\n",
       "      <td>0.046189</td>\n",
       "      <td>0.085356</td>\n",
       "      <td>0.057467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.025487</td>\n",
       "      <td>0.062098</td>\n",
       "      <td>0.041461</td>\n",
       "      <td>0.168185</td>\n",
       "      <td>0.487587</td>\n",
       "      <td>0.027604</td>\n",
       "      <td>0.046240</td>\n",
       "      <td>0.084538</td>\n",
       "      <td>0.056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.054076</td>\n",
       "      <td>0.147736</td>\n",
       "      <td>0.041385</td>\n",
       "      <td>0.138568</td>\n",
       "      <td>0.417982</td>\n",
       "      <td>0.033124</td>\n",
       "      <td>0.038487</td>\n",
       "      <td>0.078736</td>\n",
       "      <td>0.049906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.136680</td>\n",
       "      <td>0.042803</td>\n",
       "      <td>0.143705</td>\n",
       "      <td>0.426419</td>\n",
       "      <td>0.033519</td>\n",
       "      <td>0.038007</td>\n",
       "      <td>0.076885</td>\n",
       "      <td>0.053081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.049535</td>\n",
       "      <td>0.126447</td>\n",
       "      <td>0.043103</td>\n",
       "      <td>0.148685</td>\n",
       "      <td>0.433365</td>\n",
       "      <td>0.031066</td>\n",
       "      <td>0.037945</td>\n",
       "      <td>0.078955</td>\n",
       "      <td>0.050898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.046052</td>\n",
       "      <td>0.120159</td>\n",
       "      <td>0.041427</td>\n",
       "      <td>0.154480</td>\n",
       "      <td>0.437299</td>\n",
       "      <td>0.031912</td>\n",
       "      <td>0.038443</td>\n",
       "      <td>0.079229</td>\n",
       "      <td>0.050999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785255</th>\n",
       "      <td>0.044239</td>\n",
       "      <td>0.264681</td>\n",
       "      <td>0.055099</td>\n",
       "      <td>0.059001</td>\n",
       "      <td>0.357266</td>\n",
       "      <td>0.071853</td>\n",
       "      <td>0.022195</td>\n",
       "      <td>0.059356</td>\n",
       "      <td>0.066309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785256</th>\n",
       "      <td>0.044125</td>\n",
       "      <td>0.263130</td>\n",
       "      <td>0.054165</td>\n",
       "      <td>0.059454</td>\n",
       "      <td>0.359463</td>\n",
       "      <td>0.069613</td>\n",
       "      <td>0.023088</td>\n",
       "      <td>0.060436</td>\n",
       "      <td>0.066527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785257</th>\n",
       "      <td>0.044618</td>\n",
       "      <td>0.260880</td>\n",
       "      <td>0.053590</td>\n",
       "      <td>0.059055</td>\n",
       "      <td>0.359022</td>\n",
       "      <td>0.072408</td>\n",
       "      <td>0.022324</td>\n",
       "      <td>0.060203</td>\n",
       "      <td>0.067900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785258</th>\n",
       "      <td>0.043496</td>\n",
       "      <td>0.261929</td>\n",
       "      <td>0.053167</td>\n",
       "      <td>0.060495</td>\n",
       "      <td>0.360187</td>\n",
       "      <td>0.071671</td>\n",
       "      <td>0.022442</td>\n",
       "      <td>0.058737</td>\n",
       "      <td>0.067875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785259</th>\n",
       "      <td>0.044032</td>\n",
       "      <td>0.260779</td>\n",
       "      <td>0.052217</td>\n",
       "      <td>0.058916</td>\n",
       "      <td>0.362750</td>\n",
       "      <td>0.071926</td>\n",
       "      <td>0.021506</td>\n",
       "      <td>0.059631</td>\n",
       "      <td>0.068242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785260</th>\n",
       "      <td>0.044185</td>\n",
       "      <td>0.258607</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.059585</td>\n",
       "      <td>0.363492</td>\n",
       "      <td>0.070872</td>\n",
       "      <td>0.021807</td>\n",
       "      <td>0.061240</td>\n",
       "      <td>0.066571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785261</th>\n",
       "      <td>0.044306</td>\n",
       "      <td>0.255635</td>\n",
       "      <td>0.052401</td>\n",
       "      <td>0.059093</td>\n",
       "      <td>0.367128</td>\n",
       "      <td>0.071790</td>\n",
       "      <td>0.022941</td>\n",
       "      <td>0.058761</td>\n",
       "      <td>0.067946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785262</th>\n",
       "      <td>0.044716</td>\n",
       "      <td>0.254128</td>\n",
       "      <td>0.052714</td>\n",
       "      <td>0.061350</td>\n",
       "      <td>0.366313</td>\n",
       "      <td>0.071204</td>\n",
       "      <td>0.022471</td>\n",
       "      <td>0.058877</td>\n",
       "      <td>0.068226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785263</th>\n",
       "      <td>0.043735</td>\n",
       "      <td>0.253021</td>\n",
       "      <td>0.052048</td>\n",
       "      <td>0.060996</td>\n",
       "      <td>0.370278</td>\n",
       "      <td>0.070674</td>\n",
       "      <td>0.022127</td>\n",
       "      <td>0.061097</td>\n",
       "      <td>0.066024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785264</th>\n",
       "      <td>0.042977</td>\n",
       "      <td>0.251533</td>\n",
       "      <td>0.052379</td>\n",
       "      <td>0.061338</td>\n",
       "      <td>0.370467</td>\n",
       "      <td>0.070841</td>\n",
       "      <td>0.022585</td>\n",
       "      <td>0.061118</td>\n",
       "      <td>0.066762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785265</th>\n",
       "      <td>0.044458</td>\n",
       "      <td>0.251456</td>\n",
       "      <td>0.051608</td>\n",
       "      <td>0.059767</td>\n",
       "      <td>0.373960</td>\n",
       "      <td>0.068759</td>\n",
       "      <td>0.023972</td>\n",
       "      <td>0.060584</td>\n",
       "      <td>0.065435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785266</th>\n",
       "      <td>0.043890</td>\n",
       "      <td>0.255661</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>0.060533</td>\n",
       "      <td>0.372163</td>\n",
       "      <td>0.067475</td>\n",
       "      <td>0.023559</td>\n",
       "      <td>0.060753</td>\n",
       "      <td>0.066462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785267</th>\n",
       "      <td>0.046575</td>\n",
       "      <td>0.251592</td>\n",
       "      <td>0.051324</td>\n",
       "      <td>0.062732</td>\n",
       "      <td>0.368718</td>\n",
       "      <td>0.067622</td>\n",
       "      <td>0.024026</td>\n",
       "      <td>0.061787</td>\n",
       "      <td>0.065626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785268</th>\n",
       "      <td>0.044577</td>\n",
       "      <td>0.257012</td>\n",
       "      <td>0.052221</td>\n",
       "      <td>0.063061</td>\n",
       "      <td>0.369648</td>\n",
       "      <td>0.066149</td>\n",
       "      <td>0.022564</td>\n",
       "      <td>0.060743</td>\n",
       "      <td>0.064025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785269</th>\n",
       "      <td>0.044268</td>\n",
       "      <td>0.254033</td>\n",
       "      <td>0.051678</td>\n",
       "      <td>0.064370</td>\n",
       "      <td>0.369154</td>\n",
       "      <td>0.066421</td>\n",
       "      <td>0.024167</td>\n",
       "      <td>0.060261</td>\n",
       "      <td>0.065648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785270</th>\n",
       "      <td>0.048669</td>\n",
       "      <td>0.275139</td>\n",
       "      <td>0.050590</td>\n",
       "      <td>0.064896</td>\n",
       "      <td>0.351255</td>\n",
       "      <td>0.061972</td>\n",
       "      <td>0.024502</td>\n",
       "      <td>0.060611</td>\n",
       "      <td>0.062365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785271</th>\n",
       "      <td>0.046323</td>\n",
       "      <td>0.276102</td>\n",
       "      <td>0.051598</td>\n",
       "      <td>0.061403</td>\n",
       "      <td>0.350058</td>\n",
       "      <td>0.065761</td>\n",
       "      <td>0.023445</td>\n",
       "      <td>0.063879</td>\n",
       "      <td>0.061431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785272</th>\n",
       "      <td>0.047640</td>\n",
       "      <td>0.273556</td>\n",
       "      <td>0.053271</td>\n",
       "      <td>0.061664</td>\n",
       "      <td>0.350096</td>\n",
       "      <td>0.065440</td>\n",
       "      <td>0.023670</td>\n",
       "      <td>0.061596</td>\n",
       "      <td>0.063066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785273</th>\n",
       "      <td>0.046764</td>\n",
       "      <td>0.272163</td>\n",
       "      <td>0.051324</td>\n",
       "      <td>0.059536</td>\n",
       "      <td>0.354481</td>\n",
       "      <td>0.066837</td>\n",
       "      <td>0.023349</td>\n",
       "      <td>0.060065</td>\n",
       "      <td>0.065480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785274</th>\n",
       "      <td>0.047354</td>\n",
       "      <td>0.278096</td>\n",
       "      <td>0.051196</td>\n",
       "      <td>0.061315</td>\n",
       "      <td>0.347737</td>\n",
       "      <td>0.067058</td>\n",
       "      <td>0.023003</td>\n",
       "      <td>0.059665</td>\n",
       "      <td>0.064577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785275</th>\n",
       "      <td>0.048603</td>\n",
       "      <td>0.280657</td>\n",
       "      <td>0.051618</td>\n",
       "      <td>0.058849</td>\n",
       "      <td>0.345514</td>\n",
       "      <td>0.067805</td>\n",
       "      <td>0.022711</td>\n",
       "      <td>0.059878</td>\n",
       "      <td>0.064364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785276</th>\n",
       "      <td>0.046198</td>\n",
       "      <td>0.278569</td>\n",
       "      <td>0.054519</td>\n",
       "      <td>0.059578</td>\n",
       "      <td>0.346105</td>\n",
       "      <td>0.069822</td>\n",
       "      <td>0.021950</td>\n",
       "      <td>0.060244</td>\n",
       "      <td>0.063015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785277</th>\n",
       "      <td>0.047840</td>\n",
       "      <td>0.276019</td>\n",
       "      <td>0.054323</td>\n",
       "      <td>0.060920</td>\n",
       "      <td>0.345948</td>\n",
       "      <td>0.068025</td>\n",
       "      <td>0.021839</td>\n",
       "      <td>0.060849</td>\n",
       "      <td>0.064236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785278</th>\n",
       "      <td>0.046960</td>\n",
       "      <td>0.278458</td>\n",
       "      <td>0.054748</td>\n",
       "      <td>0.061308</td>\n",
       "      <td>0.344173</td>\n",
       "      <td>0.068620</td>\n",
       "      <td>0.021560</td>\n",
       "      <td>0.060682</td>\n",
       "      <td>0.063491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785279</th>\n",
       "      <td>0.048014</td>\n",
       "      <td>0.276697</td>\n",
       "      <td>0.054558</td>\n",
       "      <td>0.059273</td>\n",
       "      <td>0.345299</td>\n",
       "      <td>0.069937</td>\n",
       "      <td>0.021882</td>\n",
       "      <td>0.059249</td>\n",
       "      <td>0.065091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785280</th>\n",
       "      <td>0.047295</td>\n",
       "      <td>0.285146</td>\n",
       "      <td>0.055665</td>\n",
       "      <td>0.060464</td>\n",
       "      <td>0.336913</td>\n",
       "      <td>0.068236</td>\n",
       "      <td>0.021854</td>\n",
       "      <td>0.061402</td>\n",
       "      <td>0.063024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785281</th>\n",
       "      <td>0.046069</td>\n",
       "      <td>0.283838</td>\n",
       "      <td>0.054845</td>\n",
       "      <td>0.057951</td>\n",
       "      <td>0.342521</td>\n",
       "      <td>0.068038</td>\n",
       "      <td>0.021745</td>\n",
       "      <td>0.060809</td>\n",
       "      <td>0.064184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785282</th>\n",
       "      <td>0.047428</td>\n",
       "      <td>0.290018</td>\n",
       "      <td>0.054453</td>\n",
       "      <td>0.057945</td>\n",
       "      <td>0.336572</td>\n",
       "      <td>0.067724</td>\n",
       "      <td>0.021447</td>\n",
       "      <td>0.059859</td>\n",
       "      <td>0.064554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785283</th>\n",
       "      <td>0.048316</td>\n",
       "      <td>0.289487</td>\n",
       "      <td>0.054065</td>\n",
       "      <td>0.058682</td>\n",
       "      <td>0.334200</td>\n",
       "      <td>0.070215</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.059541</td>\n",
       "      <td>0.064672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785284</th>\n",
       "      <td>0.048527</td>\n",
       "      <td>0.292924</td>\n",
       "      <td>0.056787</td>\n",
       "      <td>0.057575</td>\n",
       "      <td>0.331831</td>\n",
       "      <td>0.070127</td>\n",
       "      <td>0.021034</td>\n",
       "      <td>0.057821</td>\n",
       "      <td>0.063374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>785285 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pix1      pix2      pix3      pix4      pix5      pix6      pix7  \\\n",
       "0       0.054868  0.329321  0.106287  0.052276  0.233781  0.094484  0.013468   \n",
       "1       0.054337  0.323024  0.103697  0.053084  0.243342  0.097438  0.012783   \n",
       "2       0.052289  0.322207  0.102387  0.052889  0.242221  0.100150  0.013860   \n",
       "3       0.053828  0.314461  0.101097  0.053499  0.248813  0.096435  0.013903   \n",
       "4       0.051126  0.308546  0.097002  0.052105  0.255642  0.101267  0.014643   \n",
       "5       0.052693  0.303119  0.097787  0.055043  0.255123  0.103381  0.014740   \n",
       "6       0.051333  0.297631  0.098259  0.055920  0.260259  0.101716  0.015024   \n",
       "7       0.051502  0.287502  0.097271  0.055527  0.265011  0.105139  0.015716   \n",
       "8       0.050441  0.284439  0.095029  0.054221  0.271334  0.107632  0.014942   \n",
       "9       0.050299  0.286019  0.092485  0.054984  0.273205  0.105112  0.015575   \n",
       "10      0.051354  0.279453  0.094692  0.056664  0.275862  0.106164  0.014502   \n",
       "11      0.039645  0.103565  0.042581  0.156569  0.455867  0.030481  0.039882   \n",
       "12      0.036899  0.094558  0.043433  0.157831  0.461151  0.030372  0.040927   \n",
       "13      0.033545  0.086630  0.042721  0.161670  0.470731  0.030088  0.041352   \n",
       "14      0.034154  0.079609  0.042756  0.165402  0.471128  0.029954  0.043645   \n",
       "15      0.032986  0.078675  0.042420  0.169673  0.468588  0.028476  0.043243   \n",
       "16      0.030660  0.074720  0.043428  0.169475  0.470161  0.028312  0.043667   \n",
       "17      0.030600  0.074133  0.043239  0.173180  0.469202  0.028089  0.044217   \n",
       "18      0.029170  0.071559  0.041568  0.173640  0.474071  0.028006  0.044057   \n",
       "19      0.028507  0.068203  0.041011  0.174959  0.477166  0.027633  0.042434   \n",
       "20      0.028092  0.067038  0.041825  0.174144  0.478215  0.027815  0.044853   \n",
       "21      0.026480  0.065728  0.041898  0.176681  0.476205  0.028091  0.045497   \n",
       "22      0.025709  0.064353  0.041211  0.173335  0.482094  0.027833  0.045705   \n",
       "23      0.026257  0.063928  0.042064  0.170923  0.481243  0.027707  0.045257   \n",
       "24      0.025749  0.061199  0.041289  0.170683  0.483133  0.028937  0.046189   \n",
       "25      0.025487  0.062098  0.041461  0.168185  0.487587  0.027604  0.046240   \n",
       "26      0.054076  0.147736  0.041385  0.138568  0.417982  0.033124  0.038487   \n",
       "27      0.048900  0.136680  0.042803  0.143705  0.426419  0.033519  0.038007   \n",
       "28      0.049535  0.126447  0.043103  0.148685  0.433365  0.031066  0.037945   \n",
       "29      0.046052  0.120159  0.041427  0.154480  0.437299  0.031912  0.038443   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "785255  0.044239  0.264681  0.055099  0.059001  0.357266  0.071853  0.022195   \n",
       "785256  0.044125  0.263130  0.054165  0.059454  0.359463  0.069613  0.023088   \n",
       "785257  0.044618  0.260880  0.053590  0.059055  0.359022  0.072408  0.022324   \n",
       "785258  0.043496  0.261929  0.053167  0.060495  0.360187  0.071671  0.022442   \n",
       "785259  0.044032  0.260779  0.052217  0.058916  0.362750  0.071926  0.021506   \n",
       "785260  0.044185  0.258607  0.053640  0.059585  0.363492  0.070872  0.021807   \n",
       "785261  0.044306  0.255635  0.052401  0.059093  0.367128  0.071790  0.022941   \n",
       "785262  0.044716  0.254128  0.052714  0.061350  0.366313  0.071204  0.022471   \n",
       "785263  0.043735  0.253021  0.052048  0.060996  0.370278  0.070674  0.022127   \n",
       "785264  0.042977  0.251533  0.052379  0.061338  0.370467  0.070841  0.022585   \n",
       "785265  0.044458  0.251456  0.051608  0.059767  0.373960  0.068759  0.023972   \n",
       "785266  0.043890  0.255661  0.049505  0.060533  0.372163  0.067475  0.023559   \n",
       "785267  0.046575  0.251592  0.051324  0.062732  0.368718  0.067622  0.024026   \n",
       "785268  0.044577  0.257012  0.052221  0.063061  0.369648  0.066149  0.022564   \n",
       "785269  0.044268  0.254033  0.051678  0.064370  0.369154  0.066421  0.024167   \n",
       "785270  0.048669  0.275139  0.050590  0.064896  0.351255  0.061972  0.024502   \n",
       "785271  0.046323  0.276102  0.051598  0.061403  0.350058  0.065761  0.023445   \n",
       "785272  0.047640  0.273556  0.053271  0.061664  0.350096  0.065440  0.023670   \n",
       "785273  0.046764  0.272163  0.051324  0.059536  0.354481  0.066837  0.023349   \n",
       "785274  0.047354  0.278096  0.051196  0.061315  0.347737  0.067058  0.023003   \n",
       "785275  0.048603  0.280657  0.051618  0.058849  0.345514  0.067805  0.022711   \n",
       "785276  0.046198  0.278569  0.054519  0.059578  0.346105  0.069822  0.021950   \n",
       "785277  0.047840  0.276019  0.054323  0.060920  0.345948  0.068025  0.021839   \n",
       "785278  0.046960  0.278458  0.054748  0.061308  0.344173  0.068620  0.021560   \n",
       "785279  0.048014  0.276697  0.054558  0.059273  0.345299  0.069937  0.021882   \n",
       "785280  0.047295  0.285146  0.055665  0.060464  0.336913  0.068236  0.021854   \n",
       "785281  0.046069  0.283838  0.054845  0.057951  0.342521  0.068038  0.021745   \n",
       "785282  0.047428  0.290018  0.054453  0.057945  0.336572  0.067724  0.021447   \n",
       "785283  0.048316  0.289487  0.054065  0.058682  0.334200  0.070215  0.020822   \n",
       "785284  0.048527  0.292924  0.056787  0.057575  0.331831  0.070127  0.021034   \n",
       "\n",
       "            pix8      pix9  \n",
       "0       0.049541  0.065974  \n",
       "1       0.048758  0.063537  \n",
       "2       0.048615  0.065382  \n",
       "3       0.049463  0.068500  \n",
       "4       0.051422  0.068246  \n",
       "5       0.049314  0.068799  \n",
       "6       0.051170  0.068688  \n",
       "7       0.050794  0.071538  \n",
       "8       0.049770  0.072192  \n",
       "9       0.051376  0.070944  \n",
       "10      0.051826  0.069483  \n",
       "11      0.077284  0.054126  \n",
       "12      0.079794  0.055036  \n",
       "13      0.078376  0.054887  \n",
       "14      0.079173  0.054178  \n",
       "15      0.081480  0.054458  \n",
       "16      0.083785  0.055793  \n",
       "17      0.083805  0.053535  \n",
       "18      0.084660  0.053269  \n",
       "19      0.084667  0.055421  \n",
       "20      0.081729  0.056289  \n",
       "21      0.084016  0.055405  \n",
       "22      0.083944  0.055815  \n",
       "23      0.084893  0.057728  \n",
       "24      0.085356  0.057467  \n",
       "25      0.084538  0.056800  \n",
       "26      0.078736  0.049906  \n",
       "27      0.076885  0.053081  \n",
       "28      0.078955  0.050898  \n",
       "29      0.079229  0.050999  \n",
       "...          ...       ...  \n",
       "785255  0.059356  0.066309  \n",
       "785256  0.060436  0.066527  \n",
       "785257  0.060203  0.067900  \n",
       "785258  0.058737  0.067875  \n",
       "785259  0.059631  0.068242  \n",
       "785260  0.061240  0.066571  \n",
       "785261  0.058761  0.067946  \n",
       "785262  0.058877  0.068226  \n",
       "785263  0.061097  0.066024  \n",
       "785264  0.061118  0.066762  \n",
       "785265  0.060584  0.065435  \n",
       "785266  0.060753  0.066462  \n",
       "785267  0.061787  0.065626  \n",
       "785268  0.060743  0.064025  \n",
       "785269  0.060261  0.065648  \n",
       "785270  0.060611  0.062365  \n",
       "785271  0.063879  0.061431  \n",
       "785272  0.061596  0.063066  \n",
       "785273  0.060065  0.065480  \n",
       "785274  0.059665  0.064577  \n",
       "785275  0.059878  0.064364  \n",
       "785276  0.060244  0.063015  \n",
       "785277  0.060849  0.064236  \n",
       "785278  0.060682  0.063491  \n",
       "785279  0.059249  0.065091  \n",
       "785280  0.061402  0.063024  \n",
       "785281  0.060809  0.064184  \n",
       "785282  0.059859  0.064554  \n",
       "785283  0.059541  0.064672  \n",
       "785284  0.057821  0.063374  \n",
       "\n",
       "[785285 rows x 9 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PLDpixels = (PLDpixels.T / PLDnorm).T\n",
    "PLDpixels"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[plt.plot(PLDpixels[key]) for key in PLDpixels.columns.values];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spitzerData = spitzerDataRaw.copy()\n",
    "for key in spitzerDataRaw.columns: \n",
    "    if key in PLDpixels.columns:\n",
    "        spitzerData[key] = PLDpixels[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed that PLD Pixels have been Normalized to Spec\n"
     ]
    }
   ],
   "source": [
    "testPLD = np.array(pd.DataFrame({key:spitzerData[key] for key in spitzerData.columns.values if 'pix' in key}))\n",
    "assert(not sum(abs(testPLD - np.array(PLDpixels))).all())\n",
    "print('Confirmed that PLD Pixels have been Normalized to Spec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notFeatures     = ['flux', 'fluxerr', 'xerr', 'yerr', 'xycov', 't_cernox']\n",
    "\n",
    "periodMax           = spitzerData['bmjd'].values.max() - spitzerData['bmjd'].values.min()\n",
    "periodMin           = np.min(np.diff(spitzerData['bmjd'].values))\n",
    "spitzerData['freq'] = np.linspace(np.pi/periodMax, 4*np.pi/periodMin, spitzerData['bmjd'].values.size)\n",
    "\n",
    "feature_columns = spitzerData.drop(notFeatures,axis=1).columns.values\n",
    "features        = spitzerData.drop(notFeatures,axis=1).values\n",
    "labels          = spitzerData['flux'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stdScaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157057, 19) validation samples\n",
      "(471171, 19) train samples\n",
      "(157057, 19) test samples\n"
     ]
    }
   ],
   "source": [
    "features_scaled = stdScaler.fit_transform(features)\n",
    "labels_scaled   = labels#stdScaler.fit_transform(labels[:,None]).ravel()\n",
    "\n",
    "idx_valtest, idx_train = train_test_split(np.arange(labels_scaled.size), test_size=0.6, random_state=42)\n",
    "idx_val, idx_test      = train_test_split(idx_valtest                  , test_size=0.5, random_state=42)\n",
    "\n",
    "x_val   = features_scaled[idx_val]\n",
    "x_test  = features_scaled[idx_test]\n",
    "x_train = features_scaled[idx_train]\n",
    "\n",
    "y_val   = labels_scaled[idx_val]\n",
    "y_test  = labels_scaled[idx_test]\n",
    "y_train = labels_scaled[idx_train]\n",
    "\n",
    "y_val_err   = spitzerData['fluxerr'].values[idx_val]\n",
    "y_test_err  = spitzerData['fluxerr'].values[idx_test]\n",
    "y_train_err = spitzerData['fluxerr'].values[idx_train]\n",
    "\n",
    "print(x_val.shape  , 'validation samples')\n",
    "print(x_train.shape, 'train samples')\n",
    "print(x_test.shape , 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df    = pd.DataFrame(np.c_[x_train, y_train], columns=list(feature_columns) + ['flux'])\n",
    "test_df     = pd.DataFrame(np.c_[x_test , y_test ], columns=list(feature_columns) + ['flux'])\n",
    "evaluate_df = pd.DataFrame(np.c_[x_val  , y_val  ], columns=list(feature_columns) + ['flux'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8745286a-f9f6-67db-4c2a-e38bc6673045"
   },
   "source": [
    "We only take first 1000 rows for training/testing and last 500 row for evaluation.\n",
    "\n",
    "\n",
    "This done so that this script does not consume a lot of kaggle system resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "95a34f94-b934-2e8f-fbd7-6e5c05425557",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape =  (471171, 20)\n",
      "test_df.shape =  (157057, 20)\n",
      "evaluate_df.shape =  (157057, 20)\n"
     ]
    }
   ],
   "source": [
    "# train_df = df_train_ori.head(1000)\n",
    "# evaluate_df = df_train_ori.tail(500)\n",
    "\n",
    "# test_df = df_test_ori.head(1000)\n",
    "\n",
    "# MODEL_DIR = \"tf_model_spitzer/withNormalization_drop50/relu\"\n",
    "# MODEL_DIR = \"tf_model_spitzer/adamOptimizer_with_drop50/relu\"\n",
    "MODEL_DIR = \"tf_model_spitzer/adamOptimizer/drop50/elu/\"\n",
    "\n",
    "print(\"train_df.shape = \"   , train_df.shape)\n",
    "print(\"test_df.shape = \"    , test_df.shape)\n",
    "print(\"evaluate_df.shape = \", evaluate_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3c9feb25-7e41-33b9-eb92-543e925b0214"
   },
   "source": [
    "## Filtering Categorical and Continuous features\n",
    "\n",
    "We store Categorical, Continuous and Target features names in different variables. This will be helpful in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "3bfe4dd5-5b24-b897-b1b1-952e6944b308",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# categorical_features = [feature for feature in features if 'cat' in feature]\n",
    "categorical_features  = []\n",
    "continuous_features   = [feature for feature in train_df.columns]# if 'cat' in feature]\n",
    "LABEL_COLUMN          = 'flux'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5f408c4f-60b3-1e75-281b-ace7ae729e43"
   },
   "source": [
    "## Converting Data into Tensors\n",
    "\n",
    "> When building a TF.Learn model, the input data is specified by means of an Input Builder function. This builder function will not be called until it is later passed to TF.Learn methods such as fit and evaluate. The purpose of this function is to construct the input data, which is represented in the form of Tensors or SparseTensors.\n",
    "\n",
    "> Note that input_fn will be called while constructing the TensorFlow graph, not while running the graph. What it is returning is a representation of the input data as the fundamental unit of TensorFlow computations, a Tensor (or SparseTensor).\n",
    "\n",
    "[More detail][2] on input_fn.\n",
    "\n",
    "[2]: https://www.tensorflow.org/versions/r0.11/tutorials/input_fn/index.html#building-input-functions-with-tf-contrib-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "04601f78-9447-20e9-20ce-3adcfd9a433c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting Data into Tensors\n",
    "def input_fn(df, training = True):\n",
    "    # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "    # the values of that column stored in a constant Tensor.\n",
    "    continuous_cols = {k: tf.constant(df[k].values)\n",
    "                       for k in continuous_features}\n",
    "\n",
    "    # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "    # to the values of that column stored in a tf.SparseTensor.\n",
    "    # categorical_cols = {k: tf.SparseTensor(\n",
    "    #     indices=[[i, 0] for i in range(df[k].size)],\n",
    "    #     values=df[k].values,\n",
    "    #     shape=[df[k].size, 1])\n",
    "    #     for k in categorical_features}\n",
    "\n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = continuous_cols\n",
    "    # feature_cols = dict(list(continuous_cols.items()) + list(categorical_cols.items()))\n",
    "    \n",
    "    if training:\n",
    "        # Converts the label column into a constant Tensor.\n",
    "        label = tf.constant(df[LABEL_COLUMN].values)\n",
    "\n",
    "        # Returns the feature columns and the label.\n",
    "        return feature_cols, label\n",
    "    \n",
    "    # Returns the feature columns    \n",
    "    return feature_cols\n",
    "\n",
    "def train_input_fn():\n",
    "    return input_fn(train_df, training=True)\n",
    "\n",
    "def eval_input_fn():\n",
    "    return input_fn(evaluate_df, training=False)\n",
    "\n",
    "def test_input_fn():\n",
    "    return input_fn(test_df, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f53238a-876b-bf8b-a5f9-77cdfafe67d9"
   },
   "source": [
    "## Selecting and Engineering Features for the Model\n",
    "\n",
    "We use tf.learn's concept of [FeatureColumn][FeatureColumn] which help in transforming raw data into suitable input features. \n",
    "\n",
    "These engineered features will be used when we construct our model.\n",
    "\n",
    "[FeatureColumn]: https://www.tensorflow.org/versions/r0.11/tutorials/linear/overview.html#feature-columns-and-transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "87b90027-c671-d7aa-88b2-4e2856a83f52",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engineered_features = []\n",
    "\n",
    "for continuous_feature in continuous_features:\n",
    "    engineered_features.append(\n",
    "        tf.contrib.layers.real_valued_column(continuous_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ca73de97-e020-4cf3-934f-44dde67523ae"
   },
   "source": [
    "## Defining The Regression Model\n",
    "\n",
    "Following is the simple DNNRegressor model. More detail about hidden_units, etc can be found [here][123].\n",
    "\n",
    "**model_dir** is used to save and restore our model. This is because once we have trained the model we don't want to train it again, if we only want to predict on new data-set.\n",
    "\n",
    "[123]: https://www.tensorflow.org/versions/r0.9/api_docs/python/contrib.learn.html#DNNRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.contrib.learn.RunConfig(tf_random_seed=42) # not shown in the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs  = features_scaled.shape[0]\n",
    "n_features= features_scaled.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471171, 19)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs  = x_train.shape[0]\n",
    "n_features= x_train.shape[1]\n",
    "n_inputs, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "6c5b7254-0e26-88d2-a65b-17f81c223ab4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden1 = n_features\n",
    "n_hidden2 = n_features\n",
    "n_hidden3 = n_features\n",
    "n_outputs = n_inputs   # because: regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('data'):\n",
    "    X   = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "    y   = tf.placeholder(tf.float32, shape=(None), name=\"y\")\n",
    "    unc = tf.placeholder(tf.float32, shape=(None), name=\"unc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "l1_reg       = 0.001\n",
    "l2_reg       = 0.0001\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "\n",
    "X_bnorm= tf.layers.batch_normalization(X)\n",
    "X_drop = tf.layers.dropout(X_bnorm, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init      = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    hidden1      = tf.layers.dense(X_drop , n_hidden1, name=\"hidden1\", \n",
    "                        activation=tf.nn.elu, kernel_initializer=he_init,\n",
    "                        kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(scale_l1=l1_reg, scale_l2=l2_reg))\n",
    "    \n",
    "    hidden1_bnorm= tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "    hidden1_drop = tf.layers.dropout(hidden1_bnorm, dropout_rate, training=training)\n",
    "    \n",
    "    hidden2      = tf.layers.dense(hidden1_drop, n_hidden2, name=\"hidden2\", \n",
    "                        activation=tf.nn.elu, kernel_initializer=he_init,\n",
    "                        kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(scale_l1=l1_reg, scale_l2=l2_reg))\n",
    "    \n",
    "    hidden2_bnorm= tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "    hidden2_drop = tf.layers.dropout(hidden2_bnorm, dropout_rate, training=training)\n",
    "    \n",
    "    hidden3      = tf.layers.dense(hidden2_drop, n_hidden3, name=\"hidden3\", \n",
    "                        activation=tf.nn.elu, kernel_initializer=he_init,\n",
    "                        kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(scale_l1=l1_reg, scale_l2=l2_reg))\n",
    "    \n",
    "    hidden3_bnorm= tf.layers.batch_normalization(hidden3, training=training, momentum=0.9)\n",
    "    hidden3_drop = tf.layers.dropout(hidden3_bnorm, dropout_rate, training=training)\n",
    "    \n",
    "    output  = tf.layers.dense(hidden3_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    def tf_nll(labels, output, uncs, coeff=1):\n",
    "        error = output - labels\n",
    "        return tf.reduce_sum(tf.divide(tf.squared_difference(output, labels) , tf.square(uncs)))# + tf.log(tf.square(uncs))\n",
    "        #return tf.reduce_sum(1 * (coeff * np.log(2*np.pi) + coeff * tf.log(uncs) + (0.5/uncs) * tf.pow(error, 2)))\n",
    "    \n",
    "    negloglike  = tf_nll(labels=y, output=output, uncs=unc)\n",
    "    \n",
    "    reg_losses  = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss        = tf.add_n([negloglike] + reg_losses, name=\"chisq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    accuracy  = tf.reduce_mean(tf.squared_difference(output, y, name=\"accuracy\"))\n",
    "    \n",
    "    SqErrRatio= tf.divide(accuracy,  tf.reduce_mean(tf.squared_difference(y, tf.reduce_mean(y))))\n",
    "    r2_acc    = 1.0 - SqErrRatio\n",
    "    \n",
    "    chsiqMean = tf_nll(labels=y, output=tf.reduce_mean(y), uncs=unc)\n",
    "    chisqModel= tf_nll(labels=y, output=output, uncs=unc)\n",
    "    rho2_acc  = 1.0 - chisqModel / chsiqMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "# with tf.name_scope(\"train\"):\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_logs/run-20171204202811/\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "print(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_summary  = tf.summary.scalar('train_acc' , accuracy  )\n",
    "loss_summary = tf.summary.scalar('loss'      , loss      )\n",
    "nll_summary  = tf.summary.scalar('negloglike', negloglike)\n",
    "r2s_summary  = tf.summary.scalar('r2_acc'    , r2_acc    )\n",
    "p2s_summary  = tf.summary.scalar('rho2_acc'  , rho2_acc  )\n",
    "val_summary  = tf.summary.scalar('val_acc'   , accuracy  )\n",
    "\n",
    "# hid1_hist    = tf.summary.histogram('hidden1', hidden1)\n",
    "# hid2_hist    = tf.summary.histogram('hidden1', hidden1)\n",
    "# hid3_hist    = tf.summary.histogram('hidden1', hidden1)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "n_epochs   = 10\n",
    "n_batches  = n_outputs // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size, trainingNow=True):\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    indices = np.random.randint(y_train.size, size=batch_size)\n",
    "    X_batch = x_train[indices]\n",
    "    y_batch = y_train.reshape(-1, 1)[indices]\n",
    "    u_batch = y_train_err.reshape(-1, 1)[indices]\n",
    "    \n",
    "    return {training: trainingNow, X: X_batch, y: y_batch, unc:u_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_acc_batch(epoch, batch_index, batch_size, batch_size_mod=10, trainingNow=False):\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    indices = np.random.randint(y_val.size, size=batch_size_mod*batch_size)\n",
    "    X_batch = x_val[indices]\n",
    "    y_batch = y_val.reshape(-1, 1)[indices]\n",
    "    u_batch = y_val_err.reshape(-1, 1)[indices]\n",
    "    return {training: trainingNow, X: X_batch, y: y_batch, unc:u_batch}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/spitzer\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_tensorboard(epoch, batch_index, feed_dict_here):\n",
    "    step = epoch * n_batches + batch_index\n",
    "    \n",
    "    feed_dict_here[training] = False\n",
    "    \n",
    "    list_of_summaries = [nll_summary, mse_summary, loss_summary, r2s_summary, p2s_summary]\n",
    "    \n",
    "    for summary in list_of_summaries:\n",
    "        file_writer.add_summary(summary=summary.eval(feed_dict=feed_dict_now), global_step=step)\n",
    "    \n",
    "    acc_feed_dict = fetch_acc_batch(epoch, batch_index, batch_size)\n",
    "    file_writer.add_summary(summary=val_summary.eval(feed_dict=acc_feed_dict), global_step=step)\n",
    "    \n",
    "    del feed_dict_here, acc_feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_update(feed_dict_now, epoch, batch_index, batch_size, n_batches, batch_size_mod=10):\n",
    "    \n",
    "    acc_test  = accuracy.eval(feed_dict=fetch_acc_batch(epoch, batch_index, batch_size))\n",
    "    acc_train = accuracy.eval(feed_dict=feed_dict_now)\n",
    "    \n",
    "    r2s_test  = r2_acc.eval(feed_dict=fetch_acc_batch(epoch, batch_index, batch_size))\n",
    "    rho2_test = rho2_acc.eval(feed_dict=fetch_acc_batch(epoch, batch_index, batch_size))\n",
    "    \n",
    "    step = epoch * n_batches + batch_index\n",
    "    \n",
    "    print(epoch, step, \"Train accuracy:\"  , acc_train, \n",
    "                       \"Test accuracy:\"   , acc_test , \n",
    "                       \"R2 Test Score:\"   , r2s_test , \n",
    "                       \"Rho2 Test Score:\" , rho2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10 Train accuracy: 0.158532 Test accuracy: 0.071215 R2 Test Score: -11006.3 Rho2 Test Score: -1.12962e+10\n",
      "0 20 Train accuracy: 0.125567 Test accuracy: 0.071215 R2 Test Score: -10273.9 Rho2 Test Score: -8.57222e+09\n",
      "0 30 Train accuracy: 0.104778 Test accuracy: 0.0553633 R2 Test Score: -8635.42 Rho2 Test Score: -7.6576e+09\n",
      "0 40 Train accuracy: 0.0805021 Test accuracy: 0.0399221 R2 Test Score: -6530.83 Rho2 Test Score: -6.62111e+09\n",
      "0 50 Train accuracy: 0.0745774 Test accuracy: 0.0364796 R2 Test Score: -5909.11 Rho2 Test Score: -5.39523e+09\n",
      "0 60 Train accuracy: 0.0450062 Test accuracy: 0.0359647 R2 Test Score: -5524.67 Rho2 Test Score: -4.10585e+09\n",
      "0 70 Train accuracy: 0.0422732 Test accuracy: 0.0356075 R2 Test Score: -5338.62 Rho2 Test Score: -3.28319e+09\n",
      "0 80 Train accuracy: 0.0359875 Test accuracy: 0.0237533 R2 Test Score: -3746.3 Rho2 Test Score: -2.57054e+09\n",
      "0 90 Train accuracy: 0.0305337 Test accuracy: 0.0188536 R2 Test Score: -2970.85 Rho2 Test Score: -2.10722e+09\n",
      "0 100 Train accuracy: 0.0218943 Test accuracy: 0.0167642 R2 Test Score: -2626.54 Rho2 Test Score: -1.50412e+09\n",
      "0 110 Train accuracy: 0.0185946 Test accuracy: 0.010654 R2 Test Score: -1734.19 Rho2 Test Score: -1.12446e+09\n",
      "0 120 Train accuracy: 0.0114023 Test accuracy: 0.00890187 R2 Test Score: -1531.34 Rho2 Test Score: -8.67392e+08\n",
      "0 130 Train accuracy: 0.00707138 Test accuracy: 0.00682 R2 Test Score: -1035.06 Rho2 Test Score: -6.10935e+08\n",
      "0 140 Train accuracy: 0.00501125 Test accuracy: 0.00555864 R2 Test Score: -783.547 Rho2 Test Score: -4.4361e+08\n",
      "0 150 Train accuracy: 0.00385177 Test accuracy: 0.00395655 R2 Test Score: -565.878 Rho2 Test Score: -3.02623e+08\n",
      "0 160 Train accuracy: 0.0036507 Test accuracy: 0.0039289 R2 Test Score: -555.528 Rho2 Test Score: -3.05151e+08\n",
      "0 170 Train accuracy: 0.00322754 Test accuracy: 0.00370746 R2 Test Score: -567.917 Rho2 Test Score: -3.19656e+08\n",
      "0 180 Train accuracy: 0.00578816 Test accuracy: 0.00342156 R2 Test Score: -497.099 Rho2 Test Score: -2.79458e+08\n",
      "0 190 Train accuracy: 0.00207758 Test accuracy: 0.00199758 R2 Test Score: -297.574 Rho2 Test Score: -1.75727e+08\n",
      "0 200 Train accuracy: 0.00210587 Test accuracy: 0.00164398 R2 Test Score: -226.467 Rho2 Test Score: -1.26951e+08\n",
      "0 210 Train accuracy: 0.00194184 Test accuracy: 0.00173811 R2 Test Score: -265.578 Rho2 Test Score: -1.54797e+08\n",
      "0 220 Train accuracy: 0.00134968 Test accuracy: 0.00140933 R2 Test Score: -213.915 Rho2 Test Score: -1.21805e+08\n",
      "0 230 Train accuracy: 0.00212708 Test accuracy: 0.00111273 R2 Test Score: -202.957 Rho2 Test Score: -1.1154e+08\n",
      "0 240 Train accuracy: 0.000834495 Test accuracy: 0.00106446 R2 Test Score: -160.194 Rho2 Test Score: -8.82165e+07\n",
      "0 250 Train accuracy: 0.000888972 Test accuracy: 0.00102365 R2 Test Score: -151.721 Rho2 Test Score: -9.0048e+07\n",
      "0 260 Train accuracy: 0.00186162 Test accuracy: 0.0014718 R2 Test Score: -243.525 Rho2 Test Score: -1.37735e+08\n",
      "0 270 Train accuracy: 0.00272516 Test accuracy: 0.00158431 R2 Test Score: -269.649 Rho2 Test Score: -1.65758e+08\n",
      "0 280 Train accuracy: 0.000947861 Test accuracy: 0.00130023 R2 Test Score: -206.547 Rho2 Test Score: -1.24579e+08\n",
      "0 290 Train accuracy: 0.0004785 Test accuracy: 0.000571992 R2 Test Score: -80.0382 Rho2 Test Score: -4.51518e+07\n",
      "0 300 Train accuracy: 0.000693604 Test accuracy: 0.000710321 R2 Test Score: -117.12 Rho2 Test Score: -6.48139e+07\n",
      "0 310 Train accuracy: 0.00119942 Test accuracy: 0.000588017 R2 Test Score: -98.7981 Rho2 Test Score: -5.41122e+07\n",
      "0 320 Train accuracy: 0.000731685 Test accuracy: 0.000480503 R2 Test Score: -78.8902 Rho2 Test Score: -4.54304e+07\n",
      "0 330 Train accuracy: 0.000749659 Test accuracy: 0.000804681 R2 Test Score: -130.833 Rho2 Test Score: -7.48925e+07\n",
      "0 340 Train accuracy: 0.000969955 Test accuracy: 0.000704501 R2 Test Score: -114.424 Rho2 Test Score: -6.66612e+07\n",
      "0 350 Train accuracy: 0.000707522 Test accuracy: 0.000467309 R2 Test Score: -74.3377 Rho2 Test Score: -4.41734e+07\n",
      "0 360 Train accuracy: 0.000545374 Test accuracy: 0.000597141 R2 Test Score: -88.214 Rho2 Test Score: -4.86139e+07\n",
      "0 370 Train accuracy: 0.000342176 Test accuracy: 0.0003844 R2 Test Score: -58.8351 Rho2 Test Score: -3.37737e+07\n",
      "0 380 Train accuracy: 0.000648558 Test accuracy: 0.000309433 R2 Test Score: -48.7075 Rho2 Test Score: -2.79598e+07\n",
      "0 390 Train accuracy: 0.000626296 Test accuracy: 0.000427757 R2 Test Score: -64.534 Rho2 Test Score: -3.82095e+07\n",
      "0 400 Train accuracy: 0.000791524 Test accuracy: 0.000336804 R2 Test Score: -50.8782 Rho2 Test Score: -2.98781e+07\n",
      "0 410 Train accuracy: 0.000392163 Test accuracy: 0.00055141 R2 Test Score: -84.6187 Rho2 Test Score: -4.74499e+07\n",
      "0 420 Train accuracy: 0.000647285 Test accuracy: 0.000556367 R2 Test Score: -80.2738 Rho2 Test Score: -4.70017e+07\n",
      "0 430 Train accuracy: 0.000520115 Test accuracy: 0.000415893 R2 Test Score: -55.6551 Rho2 Test Score: -3.34822e+07\n",
      "0 440 Train accuracy: 0.000536631 Test accuracy: 0.000307746 R2 Test Score: -45.5666 Rho2 Test Score: -2.62724e+07\n",
      "0 450 Train accuracy: 0.000265391 Test accuracy: 0.000325695 R2 Test Score: -51.9997 Rho2 Test Score: -2.89553e+07\n",
      "0 460 Train accuracy: 0.000395462 Test accuracy: 0.000334491 R2 Test Score: -55.858 Rho2 Test Score: -3.35438e+07\n",
      "0 470 Train accuracy: 0.000320799 Test accuracy: 0.000215309 R2 Test Score: -36.3898 Rho2 Test Score: -2.2335e+07\n",
      "0 480 Train accuracy: 0.000410502 Test accuracy: 0.000248747 R2 Test Score: -40.4551 Rho2 Test Score: -2.25735e+07\n",
      "0 490 Train accuracy: 0.000367137 Test accuracy: 0.000307608 R2 Test Score: -51.2898 Rho2 Test Score: -2.93633e+07\n",
      "0 500 Train accuracy: 0.000334478 Test accuracy: 0.000300516 R2 Test Score: -45.0384 Rho2 Test Score: -2.48768e+07\n",
      "0 510 Train accuracy: 0.000321337 Test accuracy: 0.000261292 R2 Test Score: -47.4421 Rho2 Test Score: -2.68051e+07\n",
      "0 520 Train accuracy: 0.000326694 Test accuracy: 0.000289902 R2 Test Score: -45.4823 Rho2 Test Score: -2.59881e+07\n",
      "0 530 Train accuracy: 0.000247992 Test accuracy: 0.00024454 R2 Test Score: -38.6343 Rho2 Test Score: -2.12661e+07\n",
      "0 540 Train accuracy: 0.000466036 Test accuracy: 0.00024627 R2 Test Score: -36.2947 Rho2 Test Score: -2.08878e+07\n",
      "0 550 Train accuracy: 0.000155841 Test accuracy: 0.000170384 R2 Test Score: -26.8 Rho2 Test Score: -1.52293e+07\n",
      "0 560 Train accuracy: 0.000123145 Test accuracy: 0.000172691 R2 Test Score: -24.4884 Rho2 Test Score: -1.35503e+07\n",
      "0 570 Train accuracy: 0.000347729 Test accuracy: 0.000174749 R2 Test Score: -25.906 Rho2 Test Score: -1.51599e+07\n",
      "0 580 Train accuracy: 0.000186283 Test accuracy: 0.000116824 R2 Test Score: -16.7389 Rho2 Test Score: -9.51404e+06\n",
      "0 590 Train accuracy: 0.000145892 Test accuracy: 9.32115e-05 R2 Test Score: -14.5301 Rho2 Test Score: -9.00862e+06\n",
      "0 600 Train accuracy: 9.67503e-05 Test accuracy: 0.000107632 R2 Test Score: -18.001 Rho2 Test Score: -1.09157e+07\n",
      "0 610 Train accuracy: 0.00014456 Test accuracy: 0.000107262 R2 Test Score: -16.5058 Rho2 Test Score: -9.61606e+06\n",
      "0 620 Train accuracy: 0.000189123 Test accuracy: 0.00017541 R2 Test Score: -23.2534 Rho2 Test Score: -1.32888e+07\n",
      "0 630 Train accuracy: 0.000273532 Test accuracy: 0.000185967 R2 Test Score: -24.7435 Rho2 Test Score: -1.43962e+07\n",
      "0 640 Train accuracy: 0.000148712 Test accuracy: 0.000152493 R2 Test Score: -24.3274 Rho2 Test Score: -1.42216e+07\n",
      "0 650 Train accuracy: 0.000118657 Test accuracy: 0.000136459 R2 Test Score: -21.215 Rho2 Test Score: -1.21103e+07\n",
      "0 660 Train accuracy: 0.000150302 Test accuracy: 0.000124267 R2 Test Score: -19.5157 Rho2 Test Score: -1.12723e+07\n",
      "0 670 Train accuracy: 0.000231799 Test accuracy: 0.000130404 R2 Test Score: -19.2953 Rho2 Test Score: -1.10901e+07\n",
      "0 680 Train accuracy: 0.000256654 Test accuracy: 0.000149269 R2 Test Score: -20.2722 Rho2 Test Score: -1.26465e+07\n",
      "0 690 Train accuracy: 0.000273426 Test accuracy: 0.000150383 R2 Test Score: -22.6831 Rho2 Test Score: -1.41136e+07\n",
      "0 700 Train accuracy: 0.000133432 Test accuracy: 8.56543e-05 R2 Test Score: -11.4594 Rho2 Test Score: -6.97448e+06\n",
      "0 710 Train accuracy: 6.63322e-05 Test accuracy: 9.52739e-05 R2 Test Score: -15.1466 Rho2 Test Score: -9.21522e+06\n",
      "0 720 Train accuracy: 0.000124668 Test accuracy: 0.000121589 R2 Test Score: -15.5859 Rho2 Test Score: -8.81075e+06\n",
      "0 730 Train accuracy: 8.3341e-05 Test accuracy: 9.20068e-05 R2 Test Score: -14.0609 Rho2 Test Score: -8.93091e+06\n",
      "0 740 Train accuracy: 0.000103088 Test accuracy: 8.41943e-05 R2 Test Score: -12.0586 Rho2 Test Score: -7.07985e+06\n",
      "0 750 Train accuracy: 0.000114712 Test accuracy: 7.37204e-05 R2 Test Score: -10.1301 Rho2 Test Score: -5.95839e+06\n",
      "0 760 Train accuracy: 7.00287e-05 Test accuracy: 8.61474e-05 R2 Test Score: -12.5619 Rho2 Test Score: -7.69009e+06\n",
      "0 770 Train accuracy: 0.000267834 Test accuracy: 0.00011662 R2 Test Score: -19.5516 Rho2 Test Score: -1.12056e+07\n",
      "0 780 Train accuracy: 7.7146e-05 Test accuracy: 0.000105878 R2 Test Score: -14.348 Rho2 Test Score: -8.57922e+06\n",
      "0 790 Train accuracy: 7.82714e-05 Test accuracy: 0.000135762 R2 Test Score: -19.5215 Rho2 Test Score: -1.12351e+07\n",
      "0 800 Train accuracy: 0.000117453 Test accuracy: 9.51766e-05 R2 Test Score: -15.3004 Rho2 Test Score: -8.9079e+06\n",
      "0 810 Train accuracy: 9.90575e-05 Test accuracy: 6.61249e-05 R2 Test Score: -9.76602 Rho2 Test Score: -6.10241e+06\n",
      "0 820 Train accuracy: 8.99323e-05 Test accuracy: 5.49329e-05 R2 Test Score: -8.32516 Rho2 Test Score: -5.58057e+06\n",
      "0 830 Train accuracy: 4.73812e-05 Test accuracy: 4.60622e-05 R2 Test Score: -5.77326 Rho2 Test Score: -3.81108e+06\n",
      "0 840 Train accuracy: 5.51418e-05 Test accuracy: 5.61535e-05 R2 Test Score: -7.46502 Rho2 Test Score: -4.4552e+06\n",
      "0 850 Train accuracy: 3.97006e-05 Test accuracy: 5.03979e-05 R2 Test Score: -6.76821 Rho2 Test Score: -4.5508e+06\n",
      "0 860 Train accuracy: 6.63494e-05 Test accuracy: 4.82276e-05 R2 Test Score: -6.91728 Rho2 Test Score: -5.02067e+06\n",
      "0 870 Train accuracy: 6.00271e-05 Test accuracy: 5.05677e-05 R2 Test Score: -6.78765 Rho2 Test Score: -4.50372e+06\n",
      "0 880 Train accuracy: 5.07523e-05 Test accuracy: 7.30026e-05 R2 Test Score: -9.87765 Rho2 Test Score: -5.90736e+06\n",
      "0 890 Train accuracy: 5.31142e-05 Test accuracy: 6.56539e-05 R2 Test Score: -9.25025 Rho2 Test Score: -5.61467e+06\n",
      "0 900 Train accuracy: 4.94833e-05 Test accuracy: 4.8646e-05 R2 Test Score: -6.52991 Rho2 Test Score: -4.38805e+06\n",
      "0 910 Train accuracy: 9.65668e-05 Test accuracy: 3.79976e-05 R2 Test Score: -5.25429 Rho2 Test Score: -3.53753e+06\n",
      "0 920 Train accuracy: 0.000119716 Test accuracy: 4.84824e-05 R2 Test Score: -6.02312 Rho2 Test Score: -4.22225e+06\n",
      "0 930 Train accuracy: 5.58674e-05 Test accuracy: 6.79761e-05 R2 Test Score: -11.2708 Rho2 Test Score: -6.71185e+06\n",
      "0 940 Train accuracy: 9.98835e-05 Test accuracy: 5.72219e-05 R2 Test Score: -7.52285 Rho2 Test Score: -4.73994e+06\n",
      "0 950 Train accuracy: 7.775e-05 Test accuracy: 4.46411e-05 R2 Test Score: -5.56421 Rho2 Test Score: -3.64118e+06\n",
      "0 960 Train accuracy: 5.27789e-05 Test accuracy: 4.3532e-05 R2 Test Score: -5.48614 Rho2 Test Score: -3.58424e+06\n",
      "0 970 Train accuracy: 4.94512e-05 Test accuracy: 3.84824e-05 R2 Test Score: -5.0052 Rho2 Test Score: -3.33771e+06\n",
      "0 980 Train accuracy: 3.71423e-05 Test accuracy: 3.92498e-05 R2 Test Score: -5.58631 Rho2 Test Score: -3.65298e+06\n",
      "0 990 Train accuracy: 4.58747e-05 Test accuracy: 4.0144e-05 R2 Test Score: -4.58434 Rho2 Test Score: -3.12974e+06\n",
      "0 1000 Train accuracy: 3.89518e-05 Test accuracy: 3.83116e-05 R2 Test Score: -5.58726 Rho2 Test Score: -3.671e+06\n",
      "0 1010 Train accuracy: 5.15532e-05 Test accuracy: 5.73903e-05 R2 Test Score: -7.60598 Rho2 Test Score: -4.86315e+06\n",
      "0 1020 Train accuracy: 0.000110197 Test accuracy: 8.0638e-05 R2 Test Score: -10.6219 Rho2 Test Score: -6.4689e+06\n",
      "0 1030 Train accuracy: 6.19854e-05 Test accuracy: 8.68068e-05 R2 Test Score: -13.8582 Rho2 Test Score: -8.40652e+06\n",
      "0 1040 Train accuracy: 4.8349e-05 Test accuracy: 5.67667e-05 R2 Test Score: -7.3382 Rho2 Test Score: -4.70426e+06\n",
      "0 1050 Train accuracy: 5.75649e-05 Test accuracy: 3.91664e-05 R2 Test Score: -5.92353 Rho2 Test Score: -3.70913e+06\n",
      "0 1060 Train accuracy: 5.02513e-05 Test accuracy: 5.11247e-05 R2 Test Score: -7.76407 Rho2 Test Score: -5.12089e+06\n",
      "0 1070 Train accuracy: 7.36699e-05 Test accuracy: 4.86093e-05 R2 Test Score: -7.01073 Rho2 Test Score: -4.67767e+06\n",
      "0 1080 Train accuracy: 2.7298e-05 Test accuracy: 3.00124e-05 R2 Test Score: -4.01874 Rho2 Test Score: -2.76548e+06\n",
      "0 1090 Train accuracy: 4.45222e-05 Test accuracy: 3.24806e-05 R2 Test Score: -4.09272 Rho2 Test Score: -2.96736e+06\n",
      "0 1100 Train accuracy: 5.77205e-05 Test accuracy: 3.26321e-05 R2 Test Score: -3.67897 Rho2 Test Score: -2.64787e+06\n",
      "0 1110 Train accuracy: 2.17369e-05 Test accuracy: 2.42403e-05 R2 Test Score: -2.72861 Rho2 Test Score: -2.08817e+06\n",
      "0 1120 Train accuracy: 1.96154e-05 Test accuracy: 2.40903e-05 R2 Test Score: -2.56798 Rho2 Test Score: -2.02568e+06\n",
      "0 1130 Train accuracy: 4.84616e-05 Test accuracy: 3.3011e-05 R2 Test Score: -4.09283 Rho2 Test Score: -2.83663e+06\n",
      "0 1140 Train accuracy: 2.8495e-05 Test accuracy: 1.97749e-05 R2 Test Score: -2.29942 Rho2 Test Score: -1.87991e+06\n",
      "0 1150 Train accuracy: 2.51324e-05 Test accuracy: 2.12906e-05 R2 Test Score: -2.23117 Rho2 Test Score: -1.82288e+06\n",
      "0 1160 Train accuracy: 6.65745e-05 Test accuracy: 4.84448e-05 R2 Test Score: -6.82899 Rho2 Test Score: -4.49639e+06\n",
      "0 1170 Train accuracy: 3.84436e-05 Test accuracy: 3.53161e-05 R2 Test Score: -4.18279 Rho2 Test Score: -2.88061e+06\n",
      "0 1180 Train accuracy: 3.78484e-05 Test accuracy: 3.71856e-05 R2 Test Score: -5.3765 Rho2 Test Score: -3.4536e+06\n",
      "0 1190 Train accuracy: 3.59937e-05 Test accuracy: 2.02107e-05 R2 Test Score: -2.39249 Rho2 Test Score: -1.91545e+06\n",
      "0 1200 Train accuracy: 2.87818e-05 Test accuracy: 1.94881e-05 R2 Test Score: -2.17238 Rho2 Test Score: -1.75714e+06\n",
      "0 1210 Train accuracy: 2.28225e-05 Test accuracy: 2.06208e-05 R2 Test Score: -2.1101 Rho2 Test Score: -1.75272e+06\n",
      "0 1220 Train accuracy: 1.99637e-05 Test accuracy: 2.51202e-05 R2 Test Score: -3.07958 Rho2 Test Score: -2.27384e+06\n",
      "0 1230 Train accuracy: 1.86731e-05 Test accuracy: 2.02506e-05 R2 Test Score: -2.39426 Rho2 Test Score: -1.87455e+06\n",
      "0 1240 Train accuracy: 1.80743e-05 Test accuracy: 1.84809e-05 R2 Test Score: -1.69172 Rho2 Test Score: -1.51234e+06\n",
      "0 1250 Train accuracy: 9.04709e-06 Test accuracy: 1.82154e-05 R2 Test Score: -1.57838 Rho2 Test Score: -1.48012e+06\n",
      "0 1260 Train accuracy: 2.382e-05 Test accuracy: 2.51915e-05 R2 Test Score: -2.71516 Rho2 Test Score: -2.10786e+06\n",
      "0 1270 Train accuracy: 2.01392e-05 Test accuracy: 1.73865e-05 R2 Test Score: -1.49358 Rho2 Test Score: -1.37116e+06\n",
      "0 1280 Train accuracy: 3.86399e-05 Test accuracy: 2.60187e-05 R2 Test Score: -3.14207 Rho2 Test Score: -2.61957e+06\n",
      "0 1290 Train accuracy: 1.7573e-05 Test accuracy: 1.39159e-05 R2 Test Score: -1.25018 Rho2 Test Score: -1.30704e+06\n",
      "0 1300 Train accuracy: 2.86358e-05 Test accuracy: 1.77532e-05 R2 Test Score: -1.66722 Rho2 Test Score: -1.46674e+06\n",
      "0 1310 Train accuracy: 1.74579e-05 Test accuracy: 1.73865e-05 R2 Test Score: -1.86847 Rho2 Test Score: -1.55216e+06\n",
      "0 1320 Train accuracy: 2.41781e-05 Test accuracy: 2.62365e-05 R2 Test Score: -3.10134 Rho2 Test Score: -2.33225e+06\n",
      "0 1330 Train accuracy: 5.41225e-05 Test accuracy: 2.99098e-05 R2 Test Score: -3.68256 Rho2 Test Score: -2.58085e+06\n",
      "0 1340 Train accuracy: 2.64559e-05 Test accuracy: 2.13128e-05 R2 Test Score: -2.53405 Rho2 Test Score: -2.12079e+06\n",
      "0 1350 Train accuracy: 1.44046e-05 Test accuracy: 1.5242e-05 R2 Test Score: -1.41704 Rho2 Test Score: -1.35999e+06\n",
      "0 1360 Train accuracy: 9.78413e-06 Test accuracy: 1.3173e-05 R2 Test Score: -0.928299 Rho2 Test Score: -1.12944e+06\n",
      "0 1370 Train accuracy: 1.59116e-05 Test accuracy: 1.13788e-05 R2 Test Score: -0.89394 Rho2 Test Score: -1.12855e+06\n",
      "0 1380 Train accuracy: 1.55798e-05 Test accuracy: 1.30511e-05 R2 Test Score: -0.847969 Rho2 Test Score: -1.05251e+06\n",
      "0 1390 Train accuracy: 1.78094e-05 Test accuracy: 1.78009e-05 R2 Test Score: -1.65337 Rho2 Test Score: -1.44916e+06\n",
      "0 1400 Train accuracy: 1.58819e-05 Test accuracy: 1.32317e-05 R2 Test Score: -1.13823 Rho2 Test Score: -1.22158e+06\n",
      "0 1410 Train accuracy: 7.80607e-06 Test accuracy: 9.99078e-06 R2 Test Score: -0.679489 Rho2 Test Score: -937847.0\n",
      "0 1420 Train accuracy: 1.41259e-05 Test accuracy: 1.62298e-05 R2 Test Score: -1.98264 Rho2 Test Score: -1.67529e+06\n",
      "0 1430 Train accuracy: 1.64398e-05 Test accuracy: 1.51905e-05 R2 Test Score: -1.28332 Rho2 Test Score: -1.26833e+06\n",
      "0 1440 Train accuracy: 2.91841e-05 Test accuracy: 1.16226e-05 R2 Test Score: -0.718319 Rho2 Test Score: -1.00687e+06\n",
      "0 1450 Train accuracy: 1.80839e-05 Test accuracy: 1.37179e-05 R2 Test Score: -1.13028 Rho2 Test Score: -1.26306e+06\n",
      "0 1460 Train accuracy: 1.29847e-05 Test accuracy: 1.08508e-05 R2 Test Score: -0.800512 Rho2 Test Score: -1.05406e+06\n",
      "0 1470 Train accuracy: 1.39364e-05 Test accuracy: 1.20313e-05 R2 Test Score: -1.06197 Rho2 Test Score: -1.20543e+06\n",
      "0 1480 Train accuracy: 6.94197e-06 Test accuracy: 8.95804e-06 R2 Test Score: -0.310616 Rho2 Test Score: -766403.0\n",
      "0 1490 Train accuracy: 1.06799e-05 Test accuracy: 1.02904e-05 R2 Test Score: -0.526246 Rho2 Test Score: -878209.0\n",
      "0 1500 Train accuracy: 9.38142e-06 Test accuracy: 8.93738e-06 R2 Test Score: -0.373054 Rho2 Test Score: -746031.0\n",
      "0 1510 Train accuracy: 1.26066e-05 Test accuracy: 9.12041e-06 R2 Test Score: -0.436396 Rho2 Test Score: -755436.0\n",
      "0 1520 Train accuracy: 1.71791e-05 Test accuracy: 1.12156e-05 R2 Test Score: -0.860253 Rho2 Test Score: -1.0697e+06\n",
      "0 1530 Train accuracy: 1.37627e-05 Test accuracy: 1.04209e-05 R2 Test Score: -0.573078 Rho2 Test Score: -866296.0\n",
      "0 1540 Train accuracy: 6.8185e-06 Test accuracy: 9.6698e-06 R2 Test Score: -0.477716 Rho2 Test Score: -810880.0\n",
      "0 1550 Train accuracy: 1.02404e-05 Test accuracy: 9.60698e-06 R2 Test Score: -0.487 Rho2 Test Score: -830095.0\n",
      "0 1560 Train accuracy: 1.33735e-05 Test accuracy: 8.69323e-06 R2 Test Score: -0.488964 Rho2 Test Score: -836138.0\n",
      "0 1570 Train accuracy: 8.39914e-06 Test accuracy: 1.09825e-05 R2 Test Score: -0.935968 Rho2 Test Score: -1.09796e+06\n",
      "0 1580 Train accuracy: 8.44115e-06 Test accuracy: 1.08774e-05 R2 Test Score: -0.618047 Rho2 Test Score: -929340.0\n",
      "0 1590 Train accuracy: 1.72875e-05 Test accuracy: 1.09911e-05 R2 Test Score: -0.749662 Rho2 Test Score: -1.03662e+06\n",
      "0 1600 Train accuracy: 9.98524e-06 Test accuracy: 9.95208e-06 R2 Test Score: -0.557729 Rho2 Test Score: -880673.0\n",
      "0 1610 Train accuracy: 1.28182e-05 Test accuracy: 6.08287e-06 R2 Test Score: -0.0168619 Rho2 Test Score: -602349.0\n",
      "0 1620 Train accuracy: 1.51987e-05 Test accuracy: 1.15965e-05 R2 Test Score: -0.828851 Rho2 Test Score: -1.02746e+06\n",
      "0 1630 Train accuracy: 6.78703e-06 Test accuracy: 1.10469e-05 R2 Test Score: -0.690395 Rho2 Test Score: -884734.0\n",
      "0 1640 Train accuracy: 1.04892e-05 Test accuracy: 8.61425e-06 R2 Test Score: -0.300583 Rho2 Test Score: -701372.0\n",
      "0 1650 Train accuracy: 1.17521e-05 Test accuracy: 8.96789e-06 R2 Test Score: -0.371734 Rho2 Test Score: -774405.0\n",
      "0 1660 Train accuracy: 7.74225e-06 Test accuracy: 7.57797e-06 R2 Test Score: -0.168756 Rho2 Test Score: -656834.0\n",
      "0 1670 Train accuracy: 9.54749e-06 Test accuracy: 4.89593e-06 R2 Test Score: 0.183979 Rho2 Test Score: -459283.0\n",
      "0 1680 Train accuracy: 1.14517e-05 Test accuracy: 6.66613e-06 R2 Test Score: 0.0559155 Rho2 Test Score: -538288.0\n",
      "0 1690 Train accuracy: 6.44706e-06 Test accuracy: 6.35989e-06 R2 Test Score: -0.0141157 Rho2 Test Score: -552539.0\n",
      "0 1700 Train accuracy: 1.55847e-05 Test accuracy: 9.42566e-06 R2 Test Score: -0.444291 Rho2 Test Score: -819723.0\n",
      "0 1710 Train accuracy: 3.33948e-06 Test accuracy: 6.20297e-06 R2 Test Score: 0.109428 Rho2 Test Score: -545437.0\n",
      "0 1720 Train accuracy: 6.09239e-06 Test accuracy: 7.56756e-06 R2 Test Score: -0.226577 Rho2 Test Score: -650934.0\n",
      "0 1730 Train accuracy: 1.10697e-05 Test accuracy: 6.66594e-06 R2 Test Score: -0.174553 Rho2 Test Score: -659300.0\n",
      "0 1740 Train accuracy: 1.83433e-05 Test accuracy: 6.48285e-06 R2 Test Score: -0.0241534 Rho2 Test Score: -580367.0\n",
      "0 1750 Train accuracy: 7.59269e-06 Test accuracy: 6.81573e-06 R2 Test Score: -0.117302 Rho2 Test Score: -637041.0\n",
      "0 1760 Train accuracy: 1.3112e-05 Test accuracy: 9.05944e-06 R2 Test Score: -0.305377 Rho2 Test Score: -762602.0\n",
      "0 1770 Train accuracy: 5.21276e-06 Test accuracy: 5.65669e-06 R2 Test Score: 0.0991282 Rho2 Test Score: -513889.0\n",
      "0 1780 Train accuracy: 9.87343e-06 Test accuracy: 6.66152e-06 R2 Test Score: -0.00856304 Rho2 Test Score: -562740.0\n",
      "0 1790 Train accuracy: 7.11286e-06 Test accuracy: 5.93445e-06 R2 Test Score: 0.153396 Rho2 Test Score: -443505.0\n",
      "0 1800 Train accuracy: 7.93761e-06 Test accuracy: 4.3483e-06 R2 Test Score: 0.256235 Rho2 Test Score: -408421.0\n",
      "0 1810 Train accuracy: 7.63945e-06 Test accuracy: 5.42436e-06 R2 Test Score: 0.166002 Rho2 Test Score: -457512.0\n",
      "0 1820 Train accuracy: 5.77605e-06 Test accuracy: 4.8349e-06 R2 Test Score: 0.302027 Rho2 Test Score: -390224.0\n",
      "0 1830 Train accuracy: 6.37081e-06 Test accuracy: 6.43747e-06 R2 Test Score: 0.0195943 Rho2 Test Score: -561456.0\n",
      "0 1840 Train accuracy: 5.68916e-06 Test accuracy: 5.75128e-06 R2 Test Score: 0.115667 Rho2 Test Score: -509481.0\n",
      "0 1850 Train accuracy: 3.61083e-06 Test accuracy: 5.07904e-06 R2 Test Score: 0.192122 Rho2 Test Score: -439775.0\n",
      "0 1860 Train accuracy: 4.88869e-06 Test accuracy: 4.873e-06 R2 Test Score: 0.189071 Rho2 Test Score: -451924.0\n",
      "0 1870 Train accuracy: 7.97839e-06 Test accuracy: 6.47191e-06 R2 Test Score: 0.0221055 Rho2 Test Score: -546302.0\n",
      "0 1880 Train accuracy: 4.10229e-06 Test accuracy: 4.51069e-06 R2 Test Score: 0.213099 Rho2 Test Score: -438065.0\n",
      "0 1890 Train accuracy: 4.46107e-06 Test accuracy: 5.11933e-06 R2 Test Score: 0.174126 Rho2 Test Score: -447304.0\n",
      "0 1900 Train accuracy: 3.99058e-06 Test accuracy: 4.46748e-06 R2 Test Score: 0.314026 Rho2 Test Score: -379900.0\n",
      "0 1910 Train accuracy: 9.02949e-06 Test accuracy: 4.5307e-06 R2 Test Score: 0.233701 Rho2 Test Score: -443638.0\n",
      "0 1920 Train accuracy: 5.44229e-06 Test accuracy: 4.40765e-06 R2 Test Score: 0.251347 Rho2 Test Score: -420240.0\n",
      "0 1930 Train accuracy: 7.6147e-06 Test accuracy: 5.89197e-06 R2 Test Score: 0.0631307 Rho2 Test Score: -527734.0\n",
      "0 1940 Train accuracy: 5.58593e-06 Test accuracy: 5.14007e-06 R2 Test Score: 0.240442 Rho2 Test Score: -439129.0\n",
      "0 1950 Train accuracy: 5.4431e-06 Test accuracy: 4.95697e-06 R2 Test Score: 0.197942 Rho2 Test Score: -436257.0\n",
      "0 1960 Train accuracy: 8.22696e-06 Test accuracy: 5.38304e-06 R2 Test Score: 0.0895784 Rho2 Test Score: -482210.0\n",
      "0 1970 Train accuracy: 9.1514e-06 Test accuracy: 6.23629e-06 R2 Test Score: -0.0612725 Rho2 Test Score: -593561.0\n",
      "0 1980 Train accuracy: 1.16939e-05 Test accuracy: 1.13737e-05 R2 Test Score: -0.691107 Rho2 Test Score: -883673.0\n",
      "0 1990 Train accuracy: 6.62074e-06 Test accuracy: 3.96389e-06 R2 Test Score: 0.348718 Rho2 Test Score: -366924.0\n",
      "0 2000 Train accuracy: 6.28718e-06 Test accuracy: 5.44564e-06 R2 Test Score: 0.178959 Rho2 Test Score: -454834.0\n",
      "0 2010 Train accuracy: 4.65352e-06 Test accuracy: 4.15237e-06 R2 Test Score: 0.339645 Rho2 Test Score: -353454.0\n",
      "0 2020 Train accuracy: 4.30744e-06 Test accuracy: 4.21389e-06 R2 Test Score: 0.36658 Rho2 Test Score: -365818.0\n",
      "0 2030 Train accuracy: 7.00218e-06 Test accuracy: 4.59076e-06 R2 Test Score: 0.35927 Rho2 Test Score: -348819.0\n",
      "0 2040 Train accuracy: 4.36046e-06 Test accuracy: 3.9056e-06 R2 Test Score: 0.378993 Rho2 Test Score: -331297.0\n",
      "0 2050 Train accuracy: 4.37137e-06 Test accuracy: 5.2885e-06 R2 Test Score: 0.206474 Rho2 Test Score: -426807.0\n",
      "0 2060 Train accuracy: 6.35564e-06 Test accuracy: 4.84903e-06 R2 Test Score: 0.163615 Rho2 Test Score: -462540.0\n",
      "0 2070 Train accuracy: 4.29896e-06 Test accuracy: 5.07904e-06 R2 Test Score: 0.25853 Rho2 Test Score: -415579.0\n",
      "0 2080 Train accuracy: 4.95429e-06 Test accuracy: 4.53639e-06 R2 Test Score: 0.3055 Rho2 Test Score: -390022.0\n",
      "0 2090 Train accuracy: 5.59034e-06 Test accuracy: 4.59076e-06 R2 Test Score: 0.278913 Rho2 Test Score: -414190.0\n",
      "0 2100 Train accuracy: 5.52345e-06 Test accuracy: 4.42547e-06 R2 Test Score: 0.32729 Rho2 Test Score: -382743.0\n",
      "0 2110 Train accuracy: 3.70242e-06 Test accuracy: 4.52485e-06 R2 Test Score: 0.308418 Rho2 Test Score: -388642.0\n",
      "0 2120 Train accuracy: 6.31337e-06 Test accuracy: 5.018e-06 R2 Test Score: 0.215741 Rho2 Test Score: -454630.0\n",
      "0 2130 Train accuracy: 4.06706e-06 Test accuracy: 3.81623e-06 R2 Test Score: 0.399012 Rho2 Test Score: -330379.0\n",
      "0 2140 Train accuracy: 6.94143e-06 Test accuracy: 5.46788e-06 R2 Test Score: 0.192015 Rho2 Test Score: -467223.0\n",
      "0 2150 Train accuracy: 6.41396e-06 Test accuracy: 4.40574e-06 R2 Test Score: 0.32069 Rho2 Test Score: -378667.0\n",
      "0 2160 Train accuracy: 5.13246e-06 Test accuracy: 4.40765e-06 R2 Test Score: 0.322617 Rho2 Test Score: -370328.0\n",
      "0 2170 Train accuracy: 5.46488e-06 Test accuracy: 4.0469e-06 R2 Test Score: 0.336862 Rho2 Test Score: -372270.0\n",
      "0 2180 Train accuracy: 5.75567e-06 Test accuracy: 3.92433e-06 R2 Test Score: 0.320988 Rho2 Test Score: -382579.0\n",
      "0 2190 Train accuracy: 4.63195e-06 Test accuracy: 4.46869e-06 R2 Test Score: 0.32097 Rho2 Test Score: -382109.0\n",
      "0 2200 Train accuracy: 4.14895e-06 Test accuracy: 4.45461e-06 R2 Test Score: 0.268143 Rho2 Test Score: -405622.0\n",
      "0 2210 Train accuracy: 4.33537e-06 Test accuracy: 4.25408e-06 R2 Test Score: 0.359155 Rho2 Test Score: -345900.0\n",
      "0 2220 Train accuracy: 5.78265e-06 Test accuracy: 3.5544e-06 R2 Test Score: 0.433334 Rho2 Test Score: -323438.0\n",
      "0 2230 Train accuracy: 4.04614e-06 Test accuracy: 3.45917e-06 R2 Test Score: 0.444299 Rho2 Test Score: -305959.0\n",
      "0 2240 Train accuracy: 4.23295e-06 Test accuracy: 3.14987e-06 R2 Test Score: 0.437238 Rho2 Test Score: -313786.0\n",
      "0 2250 Train accuracy: 4.92388e-06 Test accuracy: 4.3092e-06 R2 Test Score: 0.308576 Rho2 Test Score: -380145.0\n",
      "0 2260 Train accuracy: 6.49145e-06 Test accuracy: 4.07394e-06 R2 Test Score: 0.332865 Rho2 Test Score: -363602.0\n",
      "0 2270 Train accuracy: 5.77296e-06 Test accuracy: 4.71283e-06 R2 Test Score: 0.315175 Rho2 Test Score: -361586.0\n",
      "0 2280 Train accuracy: 3.57992e-06 Test accuracy: 4.48249e-06 R2 Test Score: 0.374334 Rho2 Test Score: -348208.0\n",
      "0 2290 Train accuracy: 4.78662e-06 Test accuracy: 4.0333e-06 R2 Test Score: 0.336127 Rho2 Test Score: -362650.0\n",
      "0 2300 Train accuracy: 3.45146e-06 Test accuracy: 3.36754e-06 R2 Test Score: 0.477861 Rho2 Test Score: -297619.0\n",
      "0 2310 Train accuracy: 4.05123e-06 Test accuracy: 3.90731e-06 R2 Test Score: 0.363165 Rho2 Test Score: -356850.0\n",
      "0 2320 Train accuracy: 4.14273e-06 Test accuracy: 4.27748e-06 R2 Test Score: 0.330078 Rho2 Test Score: -352802.0\n",
      "0 2330 Train accuracy: 4.32474e-06 Test accuracy: 3.83258e-06 R2 Test Score: 0.393232 Rho2 Test Score: -328354.0\n",
      "0 2340 Train accuracy: 4.55549e-06 Test accuracy: 4.13139e-06 R2 Test Score: 0.359613 Rho2 Test Score: -355794.0\n",
      "0 2350 Train accuracy: 4.46464e-06 Test accuracy: 3.73399e-06 R2 Test Score: 0.338013 Rho2 Test Score: -365942.0\n",
      "0 2360 Train accuracy: 5.02749e-06 Test accuracy: 4.16569e-06 R2 Test Score: 0.360766 Rho2 Test Score: -356864.0\n",
      "0 2370 Train accuracy: 5.50957e-06 Test accuracy: 4.40765e-06 R2 Test Score: 0.300936 Rho2 Test Score: -380847.0\n",
      "0 2380 Train accuracy: 5.32749e-06 Test accuracy: 4.34662e-06 R2 Test Score: 0.328243 Rho2 Test Score: -352903.0\n",
      "0 2390 Train accuracy: 3.8915e-06 Test accuracy: 3.7455e-06 R2 Test Score: 0.404387 Rho2 Test Score: -328456.0\n",
      "0 2400 Train accuracy: 5.62691e-06 Test accuracy: 4.22427e-06 R2 Test Score: 0.374198 Rho2 Test Score: -330860.0\n",
      "0 2410 Train accuracy: 5.23109e-06 Test accuracy: 4.47562e-06 R2 Test Score: 0.311834 Rho2 Test Score: -361249.0\n",
      "0 2420 Train accuracy: 4.76724e-06 Test accuracy: 4.00345e-06 R2 Test Score: 0.413728 Rho2 Test Score: -320088.0\n",
      "0 2430 Train accuracy: 3.29594e-06 Test accuracy: 4.28984e-06 R2 Test Score: 0.384911 Rho2 Test Score: -330817.0\n",
      "0 2440 Train accuracy: 4.22382e-06 Test accuracy: 3.80152e-06 R2 Test Score: 0.415171 Rho2 Test Score: -311102.0\n",
      "0 2450 Train accuracy: 3.68097e-06 Test accuracy: 3.27194e-06 R2 Test Score: 0.51907 Rho2 Test Score: -284294.0\n",
      "0 2460 Train accuracy: 5.02281e-06 Test accuracy: 4.46869e-06 R2 Test Score: 0.28167 Rho2 Test Score: -405158.0\n",
      "0 2470 Train accuracy: 3.48104e-06 Test accuracy: 3.7607e-06 R2 Test Score: 0.3686 Rho2 Test Score: -358590.0\n",
      "0 2480 Train accuracy: 5.99493e-06 Test accuracy: 3.4557e-06 R2 Test Score: 0.430811 Rho2 Test Score: -327118.0\n",
      "0 2490 Train accuracy: 3.99592e-06 Test accuracy: 3.20648e-06 R2 Test Score: 0.442799 Rho2 Test Score: -315093.0\n",
      "0 2500 Train accuracy: 6.91717e-06 Test accuracy: 3.85178e-06 R2 Test Score: 0.419791 Rho2 Test Score: -330884.0\n",
      "0 2510 Train accuracy: 3.83543e-06 Test accuracy: 4.07504e-06 R2 Test Score: 0.370341 Rho2 Test Score: -327392.0\n",
      "0 2520 Train accuracy: 3.17357e-06 Test accuracy: 3.81903e-06 R2 Test Score: 0.383827 Rho2 Test Score: -328745.0\n",
      "0 2530 Train accuracy: 5.02705e-06 Test accuracy: 3.31318e-06 R2 Test Score: 0.435645 Rho2 Test Score: -315720.0\n",
      "0 2540 Train accuracy: 5.1323e-06 Test accuracy: 3.44161e-06 R2 Test Score: 0.424182 Rho2 Test Score: -313138.0\n",
      "0 2550 Train accuracy: 3.56581e-06 Test accuracy: 3.77954e-06 R2 Test Score: 0.419129 Rho2 Test Score: -316971.0\n",
      "0 2560 Train accuracy: 5.67397e-06 Test accuracy: 3.90839e-06 R2 Test Score: 0.403602 Rho2 Test Score: -325330.0\n",
      "0 2570 Train accuracy: 4.79236e-06 Test accuracy: 3.45728e-06 R2 Test Score: 0.421713 Rho2 Test Score: -308136.0\n",
      "0 2580 Train accuracy: 3.35188e-06 Test accuracy: 4.0161e-06 R2 Test Score: 0.347415 Rho2 Test Score: -359420.0\n",
      "0 2590 Train accuracy: 2.71311e-06 Test accuracy: 3.81309e-06 R2 Test Score: 0.360545 Rho2 Test Score: -344683.0\n",
      "0 2600 Train accuracy: 4.04567e-06 Test accuracy: 4.25252e-06 R2 Test Score: 0.360396 Rho2 Test Score: -342318.0\n",
      "0 2610 Train accuracy: 5.00842e-06 Test accuracy: 3.57712e-06 R2 Test Score: 0.400417 Rho2 Test Score: -332884.0\n",
      "0 2620 Train accuracy: 4.09031e-06 Test accuracy: 3.47007e-06 R2 Test Score: 0.410302 Rho2 Test Score: -339140.0\n",
      "0 2630 Train accuracy: 3.41464e-06 Test accuracy: 3.59244e-06 R2 Test Score: 0.448254 Rho2 Test Score: -307003.0\n",
      "0 2640 Train accuracy: 4.43466e-06 Test accuracy: 3.26351e-06 R2 Test Score: 0.454335 Rho2 Test Score: -311636.0\n",
      "0 2650 Train accuracy: 3.71884e-06 Test accuracy: 3.39401e-06 R2 Test Score: 0.426047 Rho2 Test Score: -327952.0\n",
      "0 2660 Train accuracy: 5.13293e-06 Test accuracy: 3.80153e-06 R2 Test Score: 0.413203 Rho2 Test Score: -319303.0\n",
      "0 2670 Train accuracy: 3.99907e-06 Test accuracy: 4.16598e-06 R2 Test Score: 0.368332 Rho2 Test Score: -342290.0\n",
      "0 2680 Train accuracy: 4.9817e-06 Test accuracy: 3.62396e-06 R2 Test Score: 0.428836 Rho2 Test Score: -315795.0\n",
      "0 2690 Train accuracy: 5.71349e-06 Test accuracy: 3.48604e-06 R2 Test Score: 0.461489 Rho2 Test Score: -322597.0\n",
      "0 2700 Train accuracy: 4.3579e-06 Test accuracy: 4.65179e-06 R2 Test Score: 0.375885 Rho2 Test Score: -342401.0\n",
      "0 2710 Train accuracy: 3.90668e-06 Test accuracy: 3.82126e-06 R2 Test Score: 0.404535 Rho2 Test Score: -321203.0\n",
      "0 2720 Train accuracy: 2.67891e-06 Test accuracy: 3.57722e-06 R2 Test Score: 0.430258 Rho2 Test Score: -314173.0\n",
      "0 2730 Train accuracy: 3.65244e-06 Test accuracy: 4.21949e-06 R2 Test Score: 0.355452 Rho2 Test Score: -338531.0\n",
      "0 2740 Train accuracy: 3.37404e-06 Test accuracy: 4.19194e-06 R2 Test Score: 0.303391 Rho2 Test Score: -366746.0\n",
      "0 2750 Train accuracy: 4.21369e-06 Test accuracy: 4.28677e-06 R2 Test Score: 0.382239 Rho2 Test Score: -331115.0\n",
      "0 2760 Train accuracy: 4.49593e-06 Test accuracy: 3.53412e-06 R2 Test Score: 0.435242 Rho2 Test Score: -324883.0\n",
      "0 2770 Train accuracy: 6.70077e-06 Test accuracy: 4.09592e-06 R2 Test Score: 0.339364 Rho2 Test Score: -358211.0\n",
      "0 2780 Train accuracy: 3.93417e-06 Test accuracy: 4.40765e-06 R2 Test Score: 0.371114 Rho2 Test Score: -353970.0\n",
      "0 2790 Train accuracy: 4.39083e-06 Test accuracy: 3.46004e-06 R2 Test Score: 0.48565 Rho2 Test Score: -298184.0\n",
      "0 2800 Train accuracy: 3.09425e-06 Test accuracy: 4.45549e-06 R2 Test Score: 0.374681 Rho2 Test Score: -327294.0\n",
      "0 2810 Train accuracy: 3.07037e-06 Test accuracy: 3.89414e-06 R2 Test Score: 0.355383 Rho2 Test Score: -373214.0\n",
      "0 2820 Train accuracy: 4.68334e-06 Test accuracy: 3.83695e-06 R2 Test Score: 0.337692 Rho2 Test Score: -375437.0\n",
      "0 2830 Train accuracy: 4.36964e-06 Test accuracy: 4.10108e-06 R2 Test Score: 0.355712 Rho2 Test Score: -348139.0\n",
      "0 2840 Train accuracy: 4.6401e-06 Test accuracy: 4.13223e-06 R2 Test Score: 0.393623 Rho2 Test Score: -343032.0\n",
      "0 2850 Train accuracy: 4.69985e-06 Test accuracy: 3.49311e-06 R2 Test Score: 0.426135 Rho2 Test Score: -324264.0\n",
      "0 2860 Train accuracy: 3.98055e-06 Test accuracy: 4.34662e-06 R2 Test Score: 0.402704 Rho2 Test Score: -328903.0\n",
      "0 2870 Train accuracy: 4.09622e-06 Test accuracy: 3.93246e-06 R2 Test Score: 0.344632 Rho2 Test Score: -359561.0\n",
      "0 2880 Train accuracy: 4.60259e-06 Test accuracy: 4.08907e-06 R2 Test Score: 0.340747 Rho2 Test Score: -371460.0\n",
      "0 2890 Train accuracy: 4.08875e-06 Test accuracy: 3.33298e-06 R2 Test Score: 0.416559 Rho2 Test Score: -321026.0\n",
      "0 2900 Train accuracy: 4.08866e-06 Test accuracy: 4.52972e-06 R2 Test Score: 0.26138 Rho2 Test Score: -418094.0\n",
      "0 2910 Train accuracy: 5.11411e-06 Test accuracy: 3.90919e-06 R2 Test Score: 0.404385 Rho2 Test Score: -324589.0\n",
      "0 2920 Train accuracy: 3.63953e-06 Test accuracy: 2.77057e-06 R2 Test Score: 0.505458 Rho2 Test Score: -292029.0\n",
      "0 2930 Train accuracy: 3.93724e-06 Test accuracy: 3.69035e-06 R2 Test Score: 0.424497 Rho2 Test Score: -318974.0\n",
      "0 2940 Train accuracy: 3.35738e-06 Test accuracy: 3.5466e-06 R2 Test Score: 0.398309 Rho2 Test Score: -344470.0\n",
      "0 2950 Train accuracy: 4.73837e-06 Test accuracy: 4.97353e-06 R2 Test Score: 0.272813 Rho2 Test Score: -381539.0\n",
      "0 2960 Train accuracy: 3.69229e-06 Test accuracy: 3.60166e-06 R2 Test Score: 0.413166 Rho2 Test Score: -328040.0\n",
      "0 2970 Train accuracy: 3.28657e-06 Test accuracy: 3.64648e-06 R2 Test Score: 0.477415 Rho2 Test Score: -293516.0\n",
      "0 2980 Train accuracy: 4.02328e-06 Test accuracy: 3.79328e-06 R2 Test Score: 0.388175 Rho2 Test Score: -343702.0\n",
      "0 2990 Train accuracy: 4.25841e-06 Test accuracy: 3.98183e-06 R2 Test Score: 0.358599 Rho2 Test Score: -346479.0\n",
      "0 3000 Train accuracy: 5.87594e-06 Test accuracy: 4.6302e-06 R2 Test Score: 0.232251 Rho2 Test Score: -430838.0\n",
      "0 3010 Train accuracy: 2.67461e-06 Test accuracy: 3.56144e-06 R2 Test Score: 0.469755 Rho2 Test Score: -317065.0\n",
      "0 3020 Train accuracy: 5.64083e-06 Test accuracy: 3.50286e-06 R2 Test Score: 0.413846 Rho2 Test Score: -316691.0\n",
      "0 3030 Train accuracy: 4.03056e-06 Test accuracy: 4.21651e-06 R2 Test Score: 0.318362 Rho2 Test Score: -363365.0\n",
      "0 3040 Train accuracy: 3.84514e-06 Test accuracy: 3.97359e-06 R2 Test Score: 0.39544 Rho2 Test Score: -322251.0\n",
      "0 3050 Train accuracy: 3.14454e-06 Test accuracy: 3.22754e-06 R2 Test Score: 0.483777 Rho2 Test Score: -289913.0\n",
      "0 3060 Train accuracy: 4.08245e-06 Test accuracy: 4.26933e-06 R2 Test Score: 0.379196 Rho2 Test Score: -339824.0\n",
      "0 3070 Train accuracy: 4.78428e-06 Test accuracy: 4.65179e-06 R2 Test Score: 0.300511 Rho2 Test Score: -398206.0\n",
      "0 3080 Train accuracy: 4.28221e-06 Test accuracy: 4.05785e-06 R2 Test Score: 0.356317 Rho2 Test Score: -354809.0\n",
      "0 3090 Train accuracy: 3.34814e-06 Test accuracy: 3.18915e-06 R2 Test Score: 0.465481 Rho2 Test Score: -308882.0\n",
      "0 3100 Train accuracy: 3.07684e-06 Test accuracy: 3.92386e-06 R2 Test Score: 0.349767 Rho2 Test Score: -364181.0\n",
      "0 3110 Train accuracy: 2.83108e-06 Test accuracy: 3.48305e-06 R2 Test Score: 0.418856 Rho2 Test Score: -325813.0\n",
      "0 3120 Train accuracy: 4.09128e-06 Test accuracy: 4.40765e-06 R2 Test Score: 0.35386 Rho2 Test Score: -352397.0\n",
      "0 3130 Train accuracy: 6.43518e-06 Test accuracy: 3.9116e-06 R2 Test Score: 0.347627 Rho2 Test Score: -358301.0\n",
      "0 3140 Train accuracy: 3.79814e-06 Test accuracy: 3.63275e-06 R2 Test Score: 0.420126 Rho2 Test Score: -320629.0\n",
      "0 3150 Train accuracy: 4.64517e-06 Test accuracy: 3.73985e-06 R2 Test Score: 0.404904 Rho2 Test Score: -338724.0\n",
      "0 3160 Train accuracy: 4.79367e-06 Test accuracy: 3.5391e-06 R2 Test Score: 0.397262 Rho2 Test Score: -325291.0\n",
      "0 3170 Train accuracy: 3.48789e-06 Test accuracy: 4.40765e-06 R2 Test Score: 0.398929 Rho2 Test Score: -325768.0\n",
      "0 3180 Train accuracy: 4.90559e-06 Test accuracy: 4.08718e-06 R2 Test Score: 0.379562 Rho2 Test Score: -334456.0\n",
      "0 3190 Train accuracy: 6.00667e-06 Test accuracy: 4.34708e-06 R2 Test Score: 0.287787 Rho2 Test Score: -390218.0\n",
      "0 3200 Train accuracy: 4.27442e-06 Test accuracy: 4.35195e-06 R2 Test Score: 0.325609 Rho2 Test Score: -373565.0\n",
      "0 3210 Train accuracy: 4.76881e-06 Test accuracy: 3.88812e-06 R2 Test Score: 0.402164 Rho2 Test Score: -341132.0\n",
      "0 3220 Train accuracy: 3.40328e-06 Test accuracy: 3.96328e-06 R2 Test Score: 0.359979 Rho2 Test Score: -343672.0\n",
      "0 3230 Train accuracy: 4.85363e-06 Test accuracy: 3.20984e-06 R2 Test Score: 0.465115 Rho2 Test Score: -317015.0\n",
      "0 3240 Train accuracy: 4.20231e-06 Test accuracy: 3.82046e-06 R2 Test Score: 0.41061 Rho2 Test Score: -328036.0\n",
      "0 3250 Train accuracy: 4.67022e-06 Test accuracy: 3.72388e-06 R2 Test Score: 0.448559 Rho2 Test Score: -312990.0\n",
      "0 3260 Train accuracy: 2.59698e-06 Test accuracy: 3.90016e-06 R2 Test Score: 0.388019 Rho2 Test Score: -342254.0\n",
      "0 3270 Train accuracy: 4.50038e-06 Test accuracy: 4.03078e-06 R2 Test Score: 0.353444 Rho2 Test Score: -349311.0\n",
      "0 3280 Train accuracy: 3.43651e-06 Test accuracy: 3.30412e-06 R2 Test Score: 0.51558 Rho2 Test Score: -285923.0\n",
      "0 3290 Train accuracy: 6.71626e-06 Test accuracy: 4.74034e-06 R2 Test Score: 0.248607 Rho2 Test Score: -404711.0\n",
      "0 3300 Train accuracy: 4.74504e-06 Test accuracy: 4.34662e-06 R2 Test Score: 0.345311 Rho2 Test Score: -352523.0\n",
      "0 3310 Train accuracy: 3.29566e-06 Test accuracy: 3.56946e-06 R2 Test Score: 0.39841 Rho2 Test Score: -338043.0\n",
      "0 3320 Train accuracy: 5.22791e-06 Test accuracy: 4.17395e-06 R2 Test Score: 0.37008 Rho2 Test Score: -350846.0\n",
      "0 3330 Train accuracy: 5.46724e-06 Test accuracy: 4.08568e-06 R2 Test Score: 0.373656 Rho2 Test Score: -358316.0\n",
      "0 3340 Train accuracy: 4.36398e-06 Test accuracy: 3.76022e-06 R2 Test Score: 0.412261 Rho2 Test Score: -323768.0\n",
      "0 3350 Train accuracy: 3.91318e-06 Test accuracy: 3.93584e-06 R2 Test Score: 0.403619 Rho2 Test Score: -337936.0\n",
      "0 3360 Train accuracy: 3.88401e-06 Test accuracy: 3.67568e-06 R2 Test Score: 0.404742 Rho2 Test Score: -329758.0\n",
      "0 3370 Train accuracy: 4.6704e-06 Test accuracy: 3.32145e-06 R2 Test Score: 0.441814 Rho2 Test Score: -320083.0\n",
      "0 3380 Train accuracy: 4.1184e-06 Test accuracy: 3.64095e-06 R2 Test Score: 0.416698 Rho2 Test Score: -333155.0\n",
      "0 3390 Train accuracy: 5.26434e-06 Test accuracy: 4.11604e-06 R2 Test Score: 0.442529 Rho2 Test Score: -325629.0\n",
      "0 3400 Train accuracy: 5.76661e-06 Test accuracy: 3.9337e-06 R2 Test Score: 0.38137 Rho2 Test Score: -347858.0\n",
      "0 3410 Train accuracy: 4.18728e-06 Test accuracy: 4.42328e-06 R2 Test Score: 0.385733 Rho2 Test Score: -327362.0\n",
      "0 3420 Train accuracy: 3.84525e-06 Test accuracy: 4.60269e-06 R2 Test Score: 0.315281 Rho2 Test Score: -385343.0\n",
      "0 3430 Train accuracy: 4.63541e-06 Test accuracy: 4.32792e-06 R2 Test Score: 0.329673 Rho2 Test Score: -386286.0\n",
      "0 3440 Train accuracy: 3.66007e-06 Test accuracy: 3.82158e-06 R2 Test Score: 0.395444 Rho2 Test Score: -351515.0\n",
      "0 3450 Train accuracy: 3.38997e-06 Test accuracy: 3.93677e-06 R2 Test Score: 0.420987 Rho2 Test Score: -320719.0\n",
      "0 3460 Train accuracy: 4.72577e-06 Test accuracy: 4.52584e-06 R2 Test Score: 0.391192 Rho2 Test Score: -356372.0\n",
      "0 3470 Train accuracy: 3.78698e-06 Test accuracy: 3.96013e-06 R2 Test Score: 0.363016 Rho2 Test Score: -345995.0\n",
      "0 3480 Train accuracy: 3.66447e-06 Test accuracy: 3.82146e-06 R2 Test Score: 0.348669 Rho2 Test Score: -355452.0\n",
      "0 3490 Train accuracy: 3.62781e-06 Test accuracy: 3.3838e-06 R2 Test Score: 0.446627 Rho2 Test Score: -312298.0\n",
      "0 3500 Train accuracy: 4.79871e-06 Test accuracy: 4.30083e-06 R2 Test Score: 0.258812 Rho2 Test Score: -398521.0\n",
      "0 3510 Train accuracy: 6.2759e-06 Test accuracy: 4.2485e-06 R2 Test Score: 0.331935 Rho2 Test Score: -365512.0\n",
      "0 3520 Train accuracy: 5.56891e-06 Test accuracy: 3.85001e-06 R2 Test Score: 0.415372 Rho2 Test Score: -322526.0\n",
      "0 3530 Train accuracy: 5.68413e-06 Test accuracy: 4.52972e-06 R2 Test Score: 0.243199 Rho2 Test Score: -418165.0\n",
      "0 3540 Train accuracy: 5.92573e-06 Test accuracy: 4.40765e-06 R2 Test Score: 0.323013 Rho2 Test Score: -355511.0\n",
      "0 3550 Train accuracy: 4.48296e-06 Test accuracy: 4.07162e-06 R2 Test Score: 0.352903 Rho2 Test Score: -361454.0\n",
      "0 3560 Train accuracy: 2.96694e-06 Test accuracy: 3.5535e-06 R2 Test Score: 0.406747 Rho2 Test Score: -331772.0\n",
      "0 3570 Train accuracy: 5.87378e-06 Test accuracy: 3.51191e-06 R2 Test Score: 0.448442 Rho2 Test Score: -306147.0\n",
      "0 3580 Train accuracy: 5.99562e-06 Test accuracy: 3.56512e-06 R2 Test Score: 0.424197 Rho2 Test Score: -335258.0\n",
      "0 3590 Train accuracy: 5.10335e-06 Test accuracy: 3.38823e-06 R2 Test Score: 0.468762 Rho2 Test Score: -295888.0\n",
      "0 3600 Train accuracy: 2.87402e-06 Test accuracy: 3.65721e-06 R2 Test Score: 0.438918 Rho2 Test Score: -313256.0\n",
      "0 3610 Train accuracy: 4.07328e-06 Test accuracy: 4.38442e-06 R2 Test Score: 0.280947 Rho2 Test Score: -397792.0\n",
      "0 3620 Train accuracy: 5.06682e-06 Test accuracy: 3.67345e-06 R2 Test Score: 0.379078 Rho2 Test Score: -344929.0\n",
      "0 3630 Train accuracy: 4.25129e-06 Test accuracy: 3.38912e-06 R2 Test Score: 0.451159 Rho2 Test Score: -322802.0\n",
      "0 3640 Train accuracy: 5.42152e-06 Test accuracy: 3.86063e-06 R2 Test Score: 0.403851 Rho2 Test Score: -318480.0\n",
      "0 3650 Train accuracy: 3.78657e-06 Test accuracy: 3.99752e-06 R2 Test Score: 0.369196 Rho2 Test Score: -344684.0\n",
      "0 3660 Train accuracy: 6.43743e-06 Test accuracy: 4.41914e-06 R2 Test Score: 0.345492 Rho2 Test Score: -363042.0\n",
      "0 3670 Train accuracy: 3.45426e-06 Test accuracy: 4.25488e-06 R2 Test Score: 0.355001 Rho2 Test Score: -338292.0\n",
      "0 3680 Train accuracy: 4.49071e-06 Test accuracy: 4.34662e-06 R2 Test Score: 0.255481 Rho2 Test Score: -412505.0\n",
      "0 3690 Train accuracy: 4.08092e-06 Test accuracy: 5.74657e-06 R2 Test Score: 0.161406 Rho2 Test Score: -442067.0\n",
      "0 3700 Train accuracy: 4.82393e-06 Test accuracy: 5.26214e-06 R2 Test Score: 0.216798 Rho2 Test Score: -442263.0\n",
      "0 3710 Train accuracy: 5.96637e-06 Test accuracy: 3.79213e-06 R2 Test Score: 0.380488 Rho2 Test Score: -359900.0\n",
      "0 3720 Train accuracy: 5.77867e-06 Test accuracy: 4.40595e-06 R2 Test Score: 0.319969 Rho2 Test Score: -373000.0\n",
      "0 3730 Train accuracy: 5.86594e-06 Test accuracy: 4.73985e-06 R2 Test Score: 0.261321 Rho2 Test Score: -412355.0\n",
      "0 3740 Train accuracy: 4.4729e-06 Test accuracy: 3.76908e-06 R2 Test Score: 0.403974 Rho2 Test Score: -328093.0\n",
      "0 3750 Train accuracy: 3.7768e-06 Test accuracy: 3.89144e-06 R2 Test Score: 0.329977 Rho2 Test Score: -372000.0\n",
      "0 3760 Train accuracy: 4.02628e-06 Test accuracy: 3.86854e-06 R2 Test Score: 0.354871 Rho2 Test Score: -353713.0\n",
      "0 3770 Train accuracy: 3.54509e-06 Test accuracy: 3.89264e-06 R2 Test Score: 0.40252 Rho2 Test Score: -349904.0\n",
      "0 3780 Train accuracy: 4.09325e-06 Test accuracy: 3.89553e-06 R2 Test Score: 0.448929 Rho2 Test Score: -320054.0\n",
      "0 3790 Train accuracy: 4.49984e-06 Test accuracy: 4.1672e-06 R2 Test Score: 0.391659 Rho2 Test Score: -330642.0\n",
      "0 3800 Train accuracy: 3.13564e-06 Test accuracy: 4.48359e-06 R2 Test Score: 0.341892 Rho2 Test Score: -359048.0\n",
      "0 3810 Train accuracy: 5.22824e-06 Test accuracy: 4.22846e-06 R2 Test Score: 0.310501 Rho2 Test Score: -371177.0\n",
      "0 3820 Train accuracy: 5.76787e-06 Test accuracy: 4.20725e-06 R2 Test Score: 0.311756 Rho2 Test Score: -372284.0\n",
      "0 3830 Train accuracy: 7.17845e-06 Test accuracy: 5.26214e-06 R2 Test Score: 0.165357 Rho2 Test Score: -448123.0\n",
      "0 3840 Train accuracy: 6.85725e-06 Test accuracy: 7.67551e-06 R2 Test Score: -0.292207 Rho2 Test Score: -738807.0\n",
      "0 3850 Train accuracy: 4.41582e-06 Test accuracy: 4.50007e-06 R2 Test Score: 0.300833 Rho2 Test Score: -401021.0\n",
      "0 3860 Train accuracy: 4.01847e-06 Test accuracy: 3.68786e-06 R2 Test Score: 0.479393 Rho2 Test Score: -298376.0\n",
      "0 3870 Train accuracy: 4.30875e-06 Test accuracy: 3.78949e-06 R2 Test Score: 0.362208 Rho2 Test Score: -357285.0\n",
      "0 3880 Train accuracy: 6.70556e-06 Test accuracy: 4.35923e-06 R2 Test Score: 0.299693 Rho2 Test Score: -392108.0\n",
      "0 3890 Train accuracy: 4.59396e-06 Test accuracy: 4.05198e-06 R2 Test Score: 0.398192 Rho2 Test Score: -318526.0\n",
      "0 3900 Train accuracy: 5.66701e-06 Test accuracy: 4.05568e-06 R2 Test Score: 0.366171 Rho2 Test Score: -347965.0\n",
      "0 3910 Train accuracy: 3.50748e-06 Test accuracy: 3.68793e-06 R2 Test Score: 0.417282 Rho2 Test Score: -315025.0\n",
      "0 3920 Train accuracy: 8.85206e-06 Test accuracy: 5.42053e-06 R2 Test Score: 0.0749843 Rho2 Test Score: -549572.0\n",
      "0 3930 Train accuracy: 4.99614e-06 Test accuracy: 5.68835e-06 R2 Test Score: 0.188428 Rho2 Test Score: -444184.0\n",
      "0 3940 Train accuracy: 4.26292e-06 Test accuracy: 5.2796e-06 R2 Test Score: 0.256035 Rho2 Test Score: -417401.0\n",
      "0 3950 Train accuracy: 4.25484e-06 Test accuracy: 3.93191e-06 R2 Test Score: 0.395241 Rho2 Test Score: -322780.0\n",
      "0 3960 Train accuracy: 4.75472e-06 Test accuracy: 4.19706e-06 R2 Test Score: 0.362619 Rho2 Test Score: -361946.0\n",
      "0 3970 Train accuracy: 4.25064e-06 Test accuracy: 4.1776e-06 R2 Test Score: 0.414653 Rho2 Test Score: -325117.0\n",
      "0 3980 Train accuracy: 4.0868e-06 Test accuracy: 4.34662e-06 R2 Test Score: 0.307863 Rho2 Test Score: -374233.0\n",
      "0 3990 Train accuracy: 4.82593e-06 Test accuracy: 4.10763e-06 R2 Test Score: 0.299724 Rho2 Test Score: -393764.0\n",
      "0 4000 Train accuracy: 6.72625e-06 Test accuracy: 4.05267e-06 R2 Test Score: 0.298058 Rho2 Test Score: -366178.0\n",
      "0 4010 Train accuracy: 5.09598e-06 Test accuracy: 4.30946e-06 R2 Test Score: 0.306671 Rho2 Test Score: -369340.0\n",
      "0 4020 Train accuracy: 6.47341e-06 Test accuracy: 4.34662e-06 R2 Test Score: 0.363998 Rho2 Test Score: -343182.0\n",
      "0 4030 Train accuracy: 9.18754e-06 Test accuracy: 7.24652e-06 R2 Test Score: -0.271783 Rho2 Test Score: -711426.0\n",
      "0 4040 Train accuracy: 7.07301e-06 Test accuracy: 4.70549e-06 R2 Test Score: 0.29316 Rho2 Test Score: -403369.0\n",
      "0 4050 Train accuracy: 5.65175e-06 Test accuracy: 5.50406e-06 R2 Test Score: 0.11394 Rho2 Test Score: -484507.0\n",
      "0 4060 Train accuracy: 4.15976e-06 Test accuracy: 3.43676e-06 R2 Test Score: 0.400591 Rho2 Test Score: -338168.0\n",
      "0 4070 Train accuracy: 5.29287e-06 Test accuracy: 5.13131e-06 R2 Test Score: 0.244692 Rho2 Test Score: -440298.0\n",
      "0 4080 Train accuracy: 7.99097e-06 Test accuracy: 4.31962e-06 R2 Test Score: 0.291959 Rho2 Test Score: -397195.0\n",
      "0 4090 Train accuracy: 5.24003e-06 Test accuracy: 4.72693e-06 R2 Test Score: 0.225996 Rho2 Test Score: -423831.0\n",
      "0 4100 Train accuracy: 3.19411e-06 Test accuracy: 3.8655e-06 R2 Test Score: 0.360882 Rho2 Test Score: -346864.0\n",
      "0 4110 Train accuracy: 5.12082e-06 Test accuracy: 4.34662e-06 R2 Test Score: 0.366905 Rho2 Test Score: -361885.0\n",
      "0 4120 Train accuracy: 4.45916e-06 Test accuracy: 3.91285e-06 R2 Test Score: 0.308898 Rho2 Test Score: -375876.0\n",
      "0 4130 Train accuracy: 3.48978e-06 Test accuracy: 3.57135e-06 R2 Test Score: 0.443291 Rho2 Test Score: -316600.0\n",
      "0 4140 Train accuracy: 6.2171e-06 Test accuracy: 5.25627e-06 R2 Test Score: 0.138168 Rho2 Test Score: -490025.0\n",
      "0 4150 Train accuracy: 3.25766e-06 Test accuracy: 3.17687e-06 R2 Test Score: 0.375439 Rho2 Test Score: -350393.0\n",
      "0 4160 Train accuracy: 5.35237e-06 Test accuracy: 4.40386e-06 R2 Test Score: 0.340589 Rho2 Test Score: -362461.0\n",
      "0 4170 Train accuracy: 4.81715e-06 Test accuracy: 3.76673e-06 R2 Test Score: 0.353162 Rho2 Test Score: -353120.0\n",
      "0 4180 Train accuracy: 5.06788e-06 Test accuracy: 3.91582e-06 R2 Test Score: 0.377499 Rho2 Test Score: -334292.0\n",
      "0 4190 Train accuracy: 6.81345e-06 Test accuracy: 7.16381e-06 R2 Test Score: -0.0645244 Rho2 Test Score: -586651.0\n",
      "0 4200 Train accuracy: 7.01007e-06 Test accuracy: 6.43677e-06 R2 Test Score: -0.0472223 Rho2 Test Score: -578755.0\n",
      "0 4210 Train accuracy: 7.58448e-06 Test accuracy: 4.85307e-06 R2 Test Score: 0.219995 Rho2 Test Score: -433101.0\n",
      "0 4220 Train accuracy: 4.61243e-06 Test accuracy: 3.67811e-06 R2 Test Score: 0.402326 Rho2 Test Score: -328156.0\n",
      "0 4230 Train accuracy: 4.90418e-06 Test accuracy: 5.36513e-06 R2 Test Score: 0.162031 Rho2 Test Score: -473956.0\n",
      "0 4240 Train accuracy: 8.57075e-06 Test accuracy: 4.59076e-06 R2 Test Score: 0.174694 Rho2 Test Score: -453642.0\n",
      "0 4250 Train accuracy: 8.54116e-06 Test accuracy: 5.65238e-06 R2 Test Score: 0.153252 Rho2 Test Score: -471066.0\n",
      "0 4260 Train accuracy: 5.58745e-06 Test accuracy: 5.64461e-06 R2 Test Score: 0.108187 Rho2 Test Score: -514093.0\n",
      "0 4270 Train accuracy: 1.07643e-05 Test accuracy: 5.53072e-06 R2 Test Score: 0.173614 Rho2 Test Score: -485695.0\n",
      "0 4280 Train accuracy: 5.57034e-06 Test accuracy: 4.81979e-06 R2 Test Score: 0.238137 Rho2 Test Score: -426299.0\n",
      "0 4290 Train accuracy: 4.06685e-06 Test accuracy: 3.60271e-06 R2 Test Score: 0.381266 Rho2 Test Score: -356070.0\n",
      "0 4300 Train accuracy: 7.68567e-06 Test accuracy: 4.75105e-06 R2 Test Score: 0.303853 Rho2 Test Score: -401447.0\n",
      "0 4310 Train accuracy: 3.62102e-06 Test accuracy: 4.56327e-06 R2 Test Score: 0.317186 Rho2 Test Score: -361825.0\n",
      "0 4320 Train accuracy: 5.683e-06 Test accuracy: 4.63009e-06 R2 Test Score: 0.257956 Rho2 Test Score: -412477.0\n",
      "0 4330 Train accuracy: 6.00135e-06 Test accuracy: 4.63483e-06 R2 Test Score: 0.262773 Rho2 Test Score: -389141.0\n",
      "0 4340 Train accuracy: 5.77531e-06 Test accuracy: 5.1913e-06 R2 Test Score: 0.210051 Rho2 Test Score: -436670.0\n",
      "0 4350 Train accuracy: 7.63466e-06 Test accuracy: 6.62061e-06 R2 Test Score: -0.060323 Rho2 Test Score: -605618.0\n",
      "0 4360 Train accuracy: 7.16346e-06 Test accuracy: 5.92176e-06 R2 Test Score: 0.054822 Rho2 Test Score: -542863.0\n",
      "0 4370 Train accuracy: 6.37533e-06 Test accuracy: 4.35943e-06 R2 Test Score: 0.282815 Rho2 Test Score: -410480.0\n",
      "0 4380 Train accuracy: 5.73653e-06 Test accuracy: 4.40765e-06 R2 Test Score: 0.354246 Rho2 Test Score: -354031.0\n",
      "0 4390 Train accuracy: 3.41833e-06 Test accuracy: 3.30546e-06 R2 Test Score: 0.515329 Rho2 Test Score: -274126.0\n",
      "0 4400 Train accuracy: 5.2028e-06 Test accuracy: 3.76618e-06 R2 Test Score: 0.41648 Rho2 Test Score: -320173.0\n",
      "0 4410 Train accuracy: 6.05066e-06 Test accuracy: 4.76226e-06 R2 Test Score: 0.259012 Rho2 Test Score: -416967.0\n",
      "0 4420 Train accuracy: 4.23918e-06 Test accuracy: 3.4634e-06 R2 Test Score: 0.463893 Rho2 Test Score: -308496.0\n",
      "0 4430 Train accuracy: 4.3893e-06 Test accuracy: 4.5721e-06 R2 Test Score: 0.29642 Rho2 Test Score: -376515.0\n",
      "0 4440 Train accuracy: 5.25387e-06 Test accuracy: 3.70046e-06 R2 Test Score: 0.429423 Rho2 Test Score: -313524.0\n",
      "0 4450 Train accuracy: 4.60359e-06 Test accuracy: 4.14652e-06 R2 Test Score: 0.385928 Rho2 Test Score: -345664.0\n",
      "0 4460 Train accuracy: 3.81368e-06 Test accuracy: 3.86711e-06 R2 Test Score: 0.360827 Rho2 Test Score: -362672.0\n",
      "0 4470 Train accuracy: 4.86691e-06 Test accuracy: 4.88482e-06 R2 Test Score: 0.224635 Rho2 Test Score: -436562.0\n",
      "0 4480 Train accuracy: 4.97577e-06 Test accuracy: 3.6493e-06 R2 Test Score: 0.392143 Rho2 Test Score: -351214.0\n",
      "0 4490 Train accuracy: 3.8561e-06 Test accuracy: 4.31551e-06 R2 Test Score: 0.365689 Rho2 Test Score: -330535.0\n",
      "0 4500 Train accuracy: 3.77214e-06 Test accuracy: 4.34662e-06 R2 Test Score: 0.340019 Rho2 Test Score: -371010.0\n",
      "0 4510 Train accuracy: 4.08645e-06 Test accuracy: 4.22166e-06 R2 Test Score: 0.343791 Rho2 Test Score: -360852.0\n",
      "0 4520 Train accuracy: 7.53955e-06 Test accuracy: 4.766e-06 R2 Test Score: 0.224303 Rho2 Test Score: -443090.0\n",
      "0 4530 Train accuracy: 7.72607e-06 Test accuracy: 6.63329e-06 R2 Test Score: -0.01693 Rho2 Test Score: -560313.0\n",
      "0 4540 Train accuracy: 1.00145e-05 Test accuracy: 8.60822e-06 R2 Test Score: -0.327228 Rho2 Test Score: -693465.0\n",
      "0 4550 Train accuracy: 1.02796e-05 Test accuracy: 1.00918e-05 R2 Test Score: -0.647813 Rho2 Test Score: -868203.0\n",
      "0 4560 Train accuracy: 1.10054e-05 Test accuracy: 1.05646e-05 R2 Test Score: -0.565177 Rho2 Test Score: -907144.0\n",
      "0 4570 Train accuracy: 7.18482e-06 Test accuracy: 5.17962e-06 R2 Test Score: 0.196382 Rho2 Test Score: -462630.0\n",
      "0 4580 Train accuracy: 5.60662e-06 Test accuracy: 4.73549e-06 R2 Test Score: 0.215506 Rho2 Test Score: -422420.0\n",
      "0 4590 Train accuracy: 4.46947e-06 Test accuracy: 5.26468e-06 R2 Test Score: 0.247277 Rho2 Test Score: -433733.0\n",
      "0 4600 Train accuracy: 5.61551e-06 Test accuracy: 4.59847e-06 R2 Test Score: 0.275229 Rho2 Test Score: -402044.0\n",
      "0 4610 Train accuracy: 3.94336e-06 Test accuracy: 3.94461e-06 R2 Test Score: 0.358672 Rho2 Test Score: -344890.0\n",
      "0 4620 Train accuracy: 4.2995e-06 Test accuracy: 4.46887e-06 R2 Test Score: 0.323484 Rho2 Test Score: -384137.0\n",
      "0 4630 Train accuracy: 4.07534e-06 Test accuracy: 4.74751e-06 R2 Test Score: 0.337274 Rho2 Test Score: -356042.0\n",
      "0 4640 Train accuracy: 5.65219e-06 Test accuracy: 5.14383e-06 R2 Test Score: 0.109117 Rho2 Test Score: -519809.0\n",
      "0 4650 Train accuracy: 4.47779e-06 Test accuracy: 5.87049e-06 R2 Test Score: 0.175332 Rho2 Test Score: -481307.0\n",
      "0 4660 Train accuracy: 6.02798e-06 Test accuracy: 4.34662e-06 R2 Test Score: 0.317126 Rho2 Test Score: -384386.0\n",
      "0 4670 Train accuracy: 5.81226e-06 Test accuracy: 4.22039e-06 R2 Test Score: 0.322683 Rho2 Test Score: -359629.0\n",
      "0 4680 Train accuracy: 3.88796e-06 Test accuracy: 4.76914e-06 R2 Test Score: 0.319501 Rho2 Test Score: -378808.0\n",
      "0 4690 Train accuracy: 3.54034e-06 Test accuracy: 3.66071e-06 R2 Test Score: 0.433802 Rho2 Test Score: -306261.0\n",
      "0 4700 Train accuracy: 6.35209e-06 Test accuracy: 5.38607e-06 R2 Test Score: 0.124508 Rho2 Test Score: -485289.0\n",
      "0 4710 Train accuracy: 6.87652e-06 Test accuracy: 4.59204e-06 R2 Test Score: 0.339053 Rho2 Test Score: -366647.0\n",
      "0 4720 Train accuracy: 2.62102e-06 Test accuracy: 4.06816e-06 R2 Test Score: 0.377848 Rho2 Test Score: -340570.0\n",
      "0 4730 Train accuracy: 4.45768e-06 Test accuracy: 3.9815e-06 R2 Test Score: 0.390439 Rho2 Test Score: -345328.0\n",
      "0 4740 Train accuracy: 5.01223e-06 Test accuracy: 4.53492e-06 R2 Test Score: 0.371373 Rho2 Test Score: -342072.0\n",
      "0 4750 Train accuracy: 6.17984e-06 Test accuracy: 6.06787e-06 R2 Test Score: 0.00902963 Rho2 Test Score: -572009.0\n",
      "0 4760 Train accuracy: 5.37362e-06 Test accuracy: 4.23956e-06 R2 Test Score: 0.305158 Rho2 Test Score: -388086.0\n",
      "0 4770 Train accuracy: 6.22355e-06 Test accuracy: 5.38429e-06 R2 Test Score: 0.10126 Rho2 Test Score: -498472.0\n",
      "0 4780 Train accuracy: 5.42385e-06 Test accuracy: 5.3686e-06 R2 Test Score: 0.163707 Rho2 Test Score: -473938.0\n",
      "0 4790 Train accuracy: 5.43555e-06 Test accuracy: 4.31811e-06 R2 Test Score: 0.208325 Rho2 Test Score: -447925.0\n",
      "0 4800 Train accuracy: 4.75614e-06 Test accuracy: 4.24853e-06 R2 Test Score: 0.335102 Rho2 Test Score: -375228.0\n",
      "0 4810 Train accuracy: 4.51043e-06 Test accuracy: 4.47985e-06 R2 Test Score: 0.303328 Rho2 Test Score: -379496.0\n",
      "0 4820 Train accuracy: 8.50208e-06 Test accuracy: 6.1389e-06 R2 Test Score: 0.0534968 Rho2 Test Score: -557200.0\n",
      "0 4830 Train accuracy: 1.05594e-05 Test accuracy: 1.00628e-05 R2 Test Score: -0.559769 Rho2 Test Score: -834232.0\n",
      "0 4840 Train accuracy: 1.11455e-05 Test accuracy: 7.6286e-06 R2 Test Score: -0.221129 Rho2 Test Score: -677062.0\n",
      "0 4850 Train accuracy: 1.35976e-05 Test accuracy: 9.56332e-06 R2 Test Score: -0.545792 Rho2 Test Score: -858083.0\n",
      "0 4860 Train accuracy: 1.13525e-05 Test accuracy: 7.81122e-06 R2 Test Score: -0.328705 Rho2 Test Score: -713888.0\n",
      "0 4870 Train accuracy: 7.92201e-06 Test accuracy: 9.92928e-06 R2 Test Score: -0.6948 Rho2 Test Score: -951736.0\n",
      "0 4880 Train accuracy: 1.04022e-05 Test accuracy: 9.39825e-06 R2 Test Score: -0.411965 Rho2 Test Score: -772204.0\n",
      "0 4890 Train accuracy: 7.23391e-06 Test accuracy: 4.79244e-06 R2 Test Score: 0.291219 Rho2 Test Score: -411956.0\n",
      "0 4900 Train accuracy: 6.09539e-06 Test accuracy: 5.80567e-06 R2 Test Score: 0.0981032 Rho2 Test Score: -486386.0\n",
      "0 4910 Train accuracy: 6.72417e-06 Test accuracy: 5.43557e-06 R2 Test Score: 0.173282 Rho2 Test Score: -491211.0\n",
      "0 4920 Train accuracy: 6.16302e-06 Test accuracy: 5.17011e-06 R2 Test Score: 0.181667 Rho2 Test Score: -467070.0\n",
      "0 4930 Train accuracy: 7.25284e-06 Test accuracy: 4.48016e-06 R2 Test Score: 0.342284 Rho2 Test Score: -364650.0\n",
      "0 4940 Train accuracy: 4.26706e-06 Test accuracy: 3.96909e-06 R2 Test Score: 0.386256 Rho2 Test Score: -336949.0\n",
      "0 4950 Train accuracy: 3.53489e-06 Test accuracy: 4.30311e-06 R2 Test Score: 0.389005 Rho2 Test Score: -337355.0\n",
      "0 4960 Train accuracy: 4.24221e-06 Test accuracy: 4.17476e-06 R2 Test Score: 0.393502 Rho2 Test Score: -336019.0\n",
      "0 4970 Train accuracy: 4.69361e-06 Test accuracy: 4.41913e-06 R2 Test Score: 0.338117 Rho2 Test Score: -362457.0\n",
      "0 4980 Train accuracy: 6.48916e-06 Test accuracy: 4.77853e-06 R2 Test Score: 0.236579 Rho2 Test Score: -431086.0\n",
      "0 4990 Train accuracy: 4.57605e-06 Test accuracy: 4.40925e-06 R2 Test Score: 0.405272 Rho2 Test Score: -335320.0\n",
      "0 5000 Train accuracy: 6.38861e-06 Test accuracy: 4.39059e-06 R2 Test Score: 0.267926 Rho2 Test Score: -397653.0\n",
      "0 5010 Train accuracy: 7.13707e-06 Test accuracy: 5.21414e-06 R2 Test Score: 0.083218 Rho2 Test Score: -538659.0\n",
      "0 5020 Train accuracy: 5.27068e-06 Test accuracy: 4.1817e-06 R2 Test Score: 0.43593 Rho2 Test Score: -306965.0\n",
      "0 5030 Train accuracy: 5.02256e-06 Test accuracy: 3.6991e-06 R2 Test Score: 0.433625 Rho2 Test Score: -326313.0\n",
      "0 5040 Train accuracy: 4.56016e-06 Test accuracy: 4.34662e-06 R2 Test Score: 0.30806 Rho2 Test Score: -390585.0\n",
      "0 5050 Train accuracy: 7.01695e-06 Test accuracy: 5.50438e-06 R2 Test Score: 0.213716 Rho2 Test Score: -437536.0\n",
      "0 5060 Train accuracy: 3.26476e-06 Test accuracy: 3.62615e-06 R2 Test Score: 0.427827 Rho2 Test Score: -315091.0\n",
      "0 5070 Train accuracy: 3.64979e-06 Test accuracy: 3.60105e-06 R2 Test Score: 0.434233 Rho2 Test Score: -311311.0\n",
      "0 5080 Train accuracy: 5.52569e-06 Test accuracy: 3.83746e-06 R2 Test Score: 0.359146 Rho2 Test Score: -365254.0\n",
      "0 5090 Train accuracy: 4.56835e-06 Test accuracy: 3.95908e-06 R2 Test Score: 0.351077 Rho2 Test Score: -346638.0\n",
      "0 5100 Train accuracy: 5.97568e-06 Test accuracy: 4.37601e-06 R2 Test Score: 0.240514 Rho2 Test Score: -417091.0\n",
      "0 5110 Train accuracy: 5.26517e-06 Test accuracy: 5.16816e-06 R2 Test Score: 0.238259 Rho2 Test Score: -422115.0\n",
      "0 5120 Train accuracy: 5.44693e-06 Test accuracy: 4.65179e-06 R2 Test Score: 0.226463 Rho2 Test Score: -444881.0\n",
      "0 5130 Train accuracy: 7.21475e-06 Test accuracy: 6.1969e-06 R2 Test Score: -0.020346 Rho2 Test Score: -593405.0\n",
      "0 5140 Train accuracy: 5.01319e-06 Test accuracy: 4.95696e-06 R2 Test Score: 0.294073 Rho2 Test Score: -401668.0\n",
      "0 5150 Train accuracy: 5.98054e-06 Test accuracy: 7.45712e-06 R2 Test Score: -0.275613 Rho2 Test Score: -715586.0\n",
      "0 5160 Train accuracy: 4.73151e-06 Test accuracy: 4.34662e-06 R2 Test Score: 0.352359 Rho2 Test Score: -354130.0\n",
      "0 5170 Train accuracy: 4.18631e-06 Test accuracy: 3.82471e-06 R2 Test Score: 0.366075 Rho2 Test Score: -354643.0\n",
      "0 5180 Train accuracy: 5.90285e-06 Test accuracy: 3.9359e-06 R2 Test Score: 0.39465 Rho2 Test Score: -331704.0\n",
      "0 5190 Train accuracy: 3.75263e-06 Test accuracy: 4.88843e-06 R2 Test Score: 0.191091 Rho2 Test Score: -465067.0\n",
      "0 5200 Train accuracy: 6.08986e-06 Test accuracy: 6.28045e-06 R2 Test Score: 0.0733103 Rho2 Test Score: -549090.0\n",
      "0 5210 Train accuracy: 7.64817e-06 Test accuracy: 5.56841e-06 R2 Test Score: 0.201634 Rho2 Test Score: -440958.0\n",
      "0 5220 Train accuracy: 8.01348e-06 Test accuracy: 6.06183e-06 R2 Test Score: -0.00562859 Rho2 Test Score: -543347.0\n",
      "0 5230 Train accuracy: 7.66626e-06 Test accuracy: 4.19475e-06 R2 Test Score: 0.328635 Rho2 Test Score: -383525.0\n",
      "0 5240 Train accuracy: 7.51342e-06 Test accuracy: 5.26898e-06 R2 Test Score: 0.130992 Rho2 Test Score: -507858.0\n",
      "0 5250 Train accuracy: 4.67254e-06 Test accuracy: 3.27341e-06 R2 Test Score: 0.432386 Rho2 Test Score: -320710.0\n",
      "0 5260 Train accuracy: 4.31227e-06 Test accuracy: 3.49669e-06 R2 Test Score: 0.414621 Rho2 Test Score: -345369.0\n",
      "0 5270 Train accuracy: 4.23734e-06 Test accuracy: 3.60433e-06 R2 Test Score: 0.395061 Rho2 Test Score: -339804.0\n",
      "0 5280 Train accuracy: 5.99166e-06 Test accuracy: 4.34662e-06 R2 Test Score: 0.305246 Rho2 Test Score: -365362.0\n",
      "0 5290 Train accuracy: 3.59639e-06 Test accuracy: 3.24429e-06 R2 Test Score: 0.484319 Rho2 Test Score: -303417.0\n",
      "0 5300 Train accuracy: 4.3592e-06 Test accuracy: 3.30663e-06 R2 Test Score: 0.443809 Rho2 Test Score: -317700.0\n",
      "0 5310 Train accuracy: 7.44655e-06 Test accuracy: 4.90139e-06 R2 Test Score: 0.215313 Rho2 Test Score: -439596.0\n",
      "0 5320 Train accuracy: 3.86122e-06 Test accuracy: 3.18429e-06 R2 Test Score: 0.511816 Rho2 Test Score: -271741.0\n",
      "0 5330 Train accuracy: 4.28757e-06 Test accuracy: 3.45598e-06 R2 Test Score: 0.444685 Rho2 Test Score: -310978.0\n",
      "0 5340 Train accuracy: 7.95649e-06 Test accuracy: 4.95427e-06 R2 Test Score: 0.224482 Rho2 Test Score: -414531.0\n",
      "0 5350 Train accuracy: 7.2961e-06 Test accuracy: 8.24565e-06 R2 Test Score: -0.199044 Rho2 Test Score: -646333.0\n",
      "0 5360 Train accuracy: 4.58943e-06 Test accuracy: 3.7425e-06 R2 Test Score: 0.330432 Rho2 Test Score: -376149.0\n",
      "0 5370 Train accuracy: 4.82116e-06 Test accuracy: 5.68939e-06 R2 Test Score: 0.119719 Rho2 Test Score: -509944.0\n",
      "0 5380 Train accuracy: 4.79501e-06 Test accuracy: 3.63764e-06 R2 Test Score: 0.421217 Rho2 Test Score: -315732.0\n",
      "0 5390 Train accuracy: 5.05094e-06 Test accuracy: 4.22448e-06 R2 Test Score: 0.401237 Rho2 Test Score: -328715.0\n",
      "0 5400 Train accuracy: 6.73951e-06 Test accuracy: 5.56024e-06 R2 Test Score: 0.110887 Rho2 Test Score: -501106.0\n",
      "0 5410 Train accuracy: 3.82257e-06 Test accuracy: 5.34581e-06 R2 Test Score: 0.201302 Rho2 Test Score: -440850.0\n",
      "0 5420 Train accuracy: 4.83978e-06 Test accuracy: 4.11758e-06 R2 Test Score: 0.235113 Rho2 Test Score: -404850.0\n",
      "0 5430 Train accuracy: 5.9433e-06 Test accuracy: 4.64823e-06 R2 Test Score: 0.267645 Rho2 Test Score: -423777.0\n",
      "0 5440 Train accuracy: 7.60139e-06 Test accuracy: 6.32403e-06 R2 Test Score: 0.0158069 Rho2 Test Score: -570103.0\n",
      "0 5450 Train accuracy: 6.86729e-06 Test accuracy: 5.33492e-06 R2 Test Score: 0.163354 Rho2 Test Score: -475597.0\n",
      "0 5460 Train accuracy: 1.12404e-05 Test accuracy: 8.86075e-06 R2 Test Score: -0.362397 Rho2 Test Score: -756297.0\n",
      "0 5470 Train accuracy: 6.14128e-06 Test accuracy: 4.46869e-06 R2 Test Score: 0.370137 Rho2 Test Score: -351218.0\n",
      "0 5480 Train accuracy: 4.05176e-06 Test accuracy: 4.43065e-06 R2 Test Score: 0.283751 Rho2 Test Score: -418081.0\n",
      "0 5490 Train accuracy: 5.51896e-06 Test accuracy: 6.1633e-06 R2 Test Score: 0.10962 Rho2 Test Score: -474626.0\n",
      "0 5500 Train accuracy: 4.47402e-06 Test accuracy: 4.7743e-06 R2 Test Score: 0.293694 Rho2 Test Score: -394117.0\n",
      "0 5510 Train accuracy: 5.38167e-06 Test accuracy: 4.74622e-06 R2 Test Score: 0.152818 Rho2 Test Score: -462951.0\n",
      "0 5520 Train accuracy: 7.00664e-06 Test accuracy: 4.72508e-06 R2 Test Score: 0.246876 Rho2 Test Score: -432144.0\n",
      "0 5530 Train accuracy: 9.64199e-06 Test accuracy: 1.0471e-05 R2 Test Score: -0.679872 Rho2 Test Score: -929020.0\n",
      "0 5540 Train accuracy: 1.39834e-05 Test accuracy: 1.0536e-05 R2 Test Score: -0.744589 Rho2 Test Score: -1.00919e+06\n",
      "0 5550 Train accuracy: 9.7037e-06 Test accuracy: 6.70515e-06 R2 Test Score: -0.0847446 Rho2 Test Score: -609942.0\n",
      "0 5560 Train accuracy: 8.39063e-06 Test accuracy: 8.5268e-06 R2 Test Score: -0.468165 Rho2 Test Score: -811906.0\n",
      "0 5570 Train accuracy: 6.30832e-06 Test accuracy: 5.785e-06 R2 Test Score: 0.0894905 Rho2 Test Score: -522637.0\n",
      "0 5580 Train accuracy: 6.11626e-06 Test accuracy: 7.05321e-06 R2 Test Score: -0.0966561 Rho2 Test Score: -599513.0\n",
      "0 5590 Train accuracy: 9.89845e-06 Test accuracy: 6.58077e-06 R2 Test Score: 0.042736 Rho2 Test Score: -540211.0\n",
      "0 5600 Train accuracy: 6.4756e-06 Test accuracy: 8.58173e-06 R2 Test Score: -0.452134 Rho2 Test Score: -806292.0\n",
      "0 5610 Train accuracy: 4.41343e-06 Test accuracy: 4.34662e-06 R2 Test Score: 0.261599 Rho2 Test Score: -411553.0\n",
      "0 5620 Train accuracy: 6.20841e-06 Test accuracy: 5.34263e-06 R2 Test Score: 0.205103 Rho2 Test Score: -441320.0\n",
      "0 5630 Train accuracy: 4.58817e-06 Test accuracy: 3.33981e-06 R2 Test Score: 0.430783 Rho2 Test Score: -321335.0\n",
      "0 5640 Train accuracy: 5.49852e-06 Test accuracy: 5.26705e-06 R2 Test Score: 0.1749 Rho2 Test Score: -477304.0\n",
      "0 5650 Train accuracy: 8.54209e-06 Test accuracy: 7.86487e-06 R2 Test Score: -0.271675 Rho2 Test Score: -690057.0\n",
      "0 5660 Train accuracy: 5.49789e-06 Test accuracy: 4.75932e-06 R2 Test Score: 0.282645 Rho2 Test Score: -382756.0\n",
      "0 5670 Train accuracy: 6.19575e-06 Test accuracy: 5.02764e-06 R2 Test Score: 0.188185 Rho2 Test Score: -456417.0\n",
      "0 5680 Train accuracy: 3.71724e-06 Test accuracy: 3.20566e-06 R2 Test Score: 0.511807 Rho2 Test Score: -269083.0\n",
      "0 5690 Train accuracy: 7.32972e-06 Test accuracy: 6.58522e-06 R2 Test Score: 0.0214506 Rho2 Test Score: -528910.0\n",
      "0 5700 Train accuracy: 4.12154e-06 Test accuracy: 4.38586e-06 R2 Test Score: 0.382762 Rho2 Test Score: -346155.0\n",
      "0 5710 Train accuracy: 6.46051e-06 Test accuracy: 4.77388e-06 R2 Test Score: 0.199485 Rho2 Test Score: -455184.0\n",
      "0 5720 Train accuracy: 3.08656e-06 Test accuracy: 3.53343e-06 R2 Test Score: 0.465807 Rho2 Test Score: -308446.0\n",
      "0 5730 Train accuracy: 7.05543e-06 Test accuracy: 5.75192e-06 R2 Test Score: 0.147568 Rho2 Test Score: -477905.0\n",
      "0 5740 Train accuracy: 4.37807e-06 Test accuracy: 4.08033e-06 R2 Test Score: 0.392976 Rho2 Test Score: -332393.0\n",
      "0 5750 Train accuracy: 4.73482e-06 Test accuracy: 4.40765e-06 R2 Test Score: 0.268976 Rho2 Test Score: -410652.0\n",
      "0 5760 Train accuracy: 5.69303e-06 Test accuracy: 5.29544e-06 R2 Test Score: 0.175511 Rho2 Test Score: -476642.0\n",
      "0 5770 Train accuracy: 4.3092e-06 Test accuracy: 5.66999e-06 R2 Test Score: 0.103739 Rho2 Test Score: -490391.0\n",
      "0 5780 Train accuracy: 4.65119e-06 Test accuracy: 4.88919e-06 R2 Test Score: 0.251596 Rho2 Test Score: -421272.0\n",
      "0 5790 Train accuracy: 5.4037e-06 Test accuracy: 4.46869e-06 R2 Test Score: 0.344575 Rho2 Test Score: -355292.0\n",
      "0 5800 Train accuracy: 8.74832e-06 Test accuracy: 6.52869e-06 R2 Test Score: -0.161157 Rho2 Test Score: -658612.0\n",
      "0 5810 Train accuracy: 7.09098e-06 Test accuracy: 6.81542e-06 R2 Test Score: -0.0320765 Rho2 Test Score: -585926.0\n",
      "0 5820 Train accuracy: 7.37513e-06 Test accuracy: 6.50021e-06 R2 Test Score: 0.0235062 Rho2 Test Score: -528129.0\n",
      "0 5830 Train accuracy: 5.36484e-06 Test accuracy: 6.07715e-06 R2 Test Score: 0.0763629 Rho2 Test Score: -499277.0\n"
     ]
    }
   ],
   "source": [
    "batch_write_skip = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in tqdm_notebook(range(n_epochs), total=n_epochs, desc='epoch'):\n",
    "        for batch_index in tqdm_notebook(range(n_batches), total=n_batches, desc='batch'):\n",
    "            feed_dict_now = fetch_batch(epoch, batch_index, batch_size)\n",
    "            \n",
    "            if batch_index % batch_write_skip == 0 and batch_index > 0: \n",
    "                write_to_tensorboard(epoch, batch_index, feed_dict_now)\n",
    "                print_update(feed_dict_now, epoch, batch_index, batch_size, n_batches)\n",
    "            \n",
    "            sess.run(training_op, feed_dict=feed_dict_now)\n",
    "        \n",
    "        print_update(feed_dict_now, epoch, batch_index, batch_size, n_batches, batch_size_mod=100)\n",
    "    \n",
    "    save_path = saver.save(sess, MODEL_DIR + \"/spitzer_calibration_20_20_20_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "09e6b3b1-0c96-df80-85e7-31e958018309"
   },
   "source": [
    "## Training and Evaluating Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add progress bar through python `logging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "109a3d46-d5b6-93f6-d792-14765b531b8d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Our Model\n",
    "nFitSteps = 100000\n",
    "start = time()\n",
    "wrap  = regressor.fit(input_fn=train_input_fn, steps=nFitSteps)\n",
    "print('TF Regressor took {} seconds'.format(time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7655bb62-0183-1778-4c86-a69ec5d3a3cf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluating Our Model\n",
    "print('Evaluating ...')\n",
    "results = regressor.evaluate(input_fn=test_input_fn, steps=1)\n",
    "\n",
    "for key in sorted(results):\n",
    "    print(\"{}: {}\".format(key, results[key]))\n",
    "\n",
    "print(\"Val Acc: {:.3f}\".format((1-results['loss'])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Track Scalable Growth**\n",
    "\n",
    "Shrunk data set to 23559 Training samples and 7853 Val/Test samples\n",
    "\n",
    "| n_iters | time (s) | val acc | multicore | gpu |\n",
    "|------------------------------------------------|\n",
    "|  100    |   5.869  |  6.332 | yes | no |\n",
    "|  200    |   6.380  | 13.178 | yes | no |\n",
    "|  500    |   8.656  | 54.220 | yes | no |\n",
    "|  1000   |  12.170  | 66.596 | yes | no |\n",
    "|  2000   |  19.891  | 62.996 | yes | no |\n",
    "|  5000   |  43.589  | 76.586 | yes | no |\n",
    "|  10000  |  80.581  | 66.872 | yes | no |\n",
    "|  20000  | 162.435  | 78.927 | yes | no |\n",
    "|  50000  | 535.584  | 75.493 | yes | no |\n",
    "|  100000 | 1062.656 | 73.162 | yes | no |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nItersList = [100,200,500,1000,2000,5000,10000,20000,50000,100000]\n",
    "rtimesList = [5.869, 6.380, 8.656, 12.170, 19.891, 43.589, 80.581, 162.435, 535.584, 1062.656]\n",
    "valAccList = [6.332, 13.178, 54.220, 66.596, 62.996, 76.586, 66.872, 78.927, 75.493, 73.162]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.loglog(nItersList, rtimesList,'o-');\n",
    "plt.twinx()\n",
    "plt.semilogx(nItersList, valAccList,'o-', color='orange');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c418c90-cbda-b772-f3be-bc9107bca2f8"
   },
   "source": [
    "## Predicting output for test data\n",
    "\n",
    "Most of the time prediction script would be separate from training script (we need not to train on same data again) but I am providing both in same script here; as I am not sure if we can create multiple notebook and somehow share data between them in Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def de_median(x):\n",
    "    return x - np.median(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_output = list(regressor.predict(input_fn=test_input_fn))\n",
    "# x = list(predicted_output)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "c4e45f26-f03e-9079-65a6-b13871b24a46",
    "scrolled": false
   },
   "source": [
    "# print([predicted_output() for _ in range(10)])\n",
    "plt.plot((predicted_output - np.median(predicted_output)) / np.std(predicted_output),'.',alpha=0.1);\n",
    "plt.plot((test_df['flux'].values - np.median(test_df['flux'].values)) / np.std(test_df['flux'].values),'.',alpha=0.1);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(de_median(x - test_df['flux'].values)/x,'.',alpha=0.1);\n",
    "plt.ylim(-1.,1.);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r2_score(test_df['flux'].values,predicted_output)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Full notebook took {} seconds'.format(time()-start0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "saveDir = 'tfSaveModels'\n",
    "regressor.export_savedmodel(saveDir, regressor)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "saveDir = 'tfSaveModels'\n",
    "reg_args = {'feature_columns': fc, 'hidden_units': hu_array, ...}\n",
    "regressor = tf.contrib.learn.DNNRegressor(**reg_args)\n",
    "pickle.dump(reg_args, open('reg_args.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "reg_args = pickle.load(open('reg_args.pkl', 'rb'))\n",
    "# On another machine and so my model dir path changed:\n",
    "reg_args['model_dir'] = NEW_MODEL_DIR\n",
    "regressor = tf.contrib.learn.DNNRegressor(**reg_args)"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 2,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "2d500db3df2046149e85164d007e8800": {
     "views": [
      {
       "cell_index": 45
      }
     ]
    },
    "35953be77da5401fb485c9bab4a40b85": {
     "views": [
      {
       "cell_index": 45
      }
     ]
    },
    "35b6ddeb369645b7a05f1f613458fb22": {
     "views": [
      {
       "cell_index": 45
      }
     ]
    },
    "7690439e6da7460e9c37d6b3579085d2": {
     "views": [
      {
       "cell_index": 45
      }
     ]
    },
    "82a44358006449739fe476669bca8282": {
     "views": [
      {
       "cell_index": 45
      }
     ]
    },
    "9d12b21545754a5390ae28596d7b12f1": {
     "views": [
      {
       "cell_index": 45
      }
     ]
    },
    "c9cd47601c514ede80f826af2b06250a": {
     "views": [
      {
       "cell_index": 45
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
