{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fa1a9421-50b6-7ae4-0b03-212f533f8fcb"
   },
   "source": [
    "## TF-DNNRegressor - eLU - Spitzer Calibration Data\n",
    "\n",
    "This script show a simple example of using [tf.contrib.learn][1] library to create our model.\n",
    "\n",
    "The code is divided in following steps:\n",
    "\n",
    " - Load CSVs data\n",
    " - Filtering Categorical and Continuous features\n",
    " - Converting Data into Tensors\n",
    " - Selecting and Engineering Features for the Model\n",
    " - Defining The Regression Model\n",
    " - Training and Evaluating Our Model\n",
    " - Predicting output for test data\n",
    "\n",
    "*v0.1: Added code for data loading, modeling and  prediction model.*\n",
    "\n",
    "*v0.2: Removed unnecessary output logs.*\n",
    "\n",
    "*PS: I was able to get a score of 1295.07972 using this script with 70% (of train.csv) data used for training and rest for evaluation. Script took 2hrs for training and 3000 steps were used.*\n",
    "\n",
    "[1]: https://www.tensorflow.org/versions/r0.11/tutorials/tflearn/index.html#tf-contrib-learn-quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "5259819c-f765-e649-5e67-7bcde8665463"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathan/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, minmax_scale\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from time import time\n",
    "start0 = time()\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0e7f9a40-2b80-bdea-d5aa-d52a87577e8c"
   },
   "source": [
    "## Load CSVs data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "cb7edeab-c13e-8a29-34dc-2e9d0bd7059b"
   },
   "source": [
    "df_train_ori = pd.read_csv('train.csv')\n",
    "df_test_ori = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nSkip = 20\n",
    "spitzerDataRaw  = pd.read_csv('pmap_ch2_0p1s_x4_rmulti_s3_7.csv')#[::nSkip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pix1</th>\n",
       "      <th>pix2</th>\n",
       "      <th>pix3</th>\n",
       "      <th>pix4</th>\n",
       "      <th>pix5</th>\n",
       "      <th>pix6</th>\n",
       "      <th>pix7</th>\n",
       "      <th>pix8</th>\n",
       "      <th>pix9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>577.447021</td>\n",
       "      <td>3465.876709</td>\n",
       "      <td>1118.598145</td>\n",
       "      <td>550.165466</td>\n",
       "      <td>2460.376953</td>\n",
       "      <td>994.374207</td>\n",
       "      <td>141.741592</td>\n",
       "      <td>521.385254</td>\n",
       "      <td>694.330688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>569.863098</td>\n",
       "      <td>3387.739258</td>\n",
       "      <td>1087.530762</td>\n",
       "      <td>556.717407</td>\n",
       "      <td>2552.070557</td>\n",
       "      <td>1021.892700</td>\n",
       "      <td>134.061081</td>\n",
       "      <td>511.347778</td>\n",
       "      <td>666.346069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552.641235</td>\n",
       "      <td>3405.411377</td>\n",
       "      <td>1082.131104</td>\n",
       "      <td>558.981445</td>\n",
       "      <td>2560.040771</td>\n",
       "      <td>1058.485352</td>\n",
       "      <td>146.488220</td>\n",
       "      <td>513.809570</td>\n",
       "      <td>691.019653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>571.821167</td>\n",
       "      <td>3340.533691</td>\n",
       "      <td>1073.962036</td>\n",
       "      <td>568.324768</td>\n",
       "      <td>2643.155273</td>\n",
       "      <td>1024.431641</td>\n",
       "      <td>147.687546</td>\n",
       "      <td>525.451538</td>\n",
       "      <td>727.683472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>538.292114</td>\n",
       "      <td>3248.569336</td>\n",
       "      <td>1021.301208</td>\n",
       "      <td>548.598145</td>\n",
       "      <td>2691.563965</td>\n",
       "      <td>1066.199707</td>\n",
       "      <td>154.170990</td>\n",
       "      <td>541.407532</td>\n",
       "      <td>718.537537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>553.332214</td>\n",
       "      <td>3183.050293</td>\n",
       "      <td>1026.863281</td>\n",
       "      <td>578.003784</td>\n",
       "      <td>2679.043457</td>\n",
       "      <td>1085.607422</td>\n",
       "      <td>154.789886</td>\n",
       "      <td>517.848328</td>\n",
       "      <td>722.456909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>541.202332</td>\n",
       "      <td>3137.938232</td>\n",
       "      <td>1035.948364</td>\n",
       "      <td>589.566528</td>\n",
       "      <td>2743.923584</td>\n",
       "      <td>1072.393555</td>\n",
       "      <td>158.401291</td>\n",
       "      <td>539.485718</td>\n",
       "      <td>724.184937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>547.699829</td>\n",
       "      <td>3057.428467</td>\n",
       "      <td>1034.417603</td>\n",
       "      <td>590.498108</td>\n",
       "      <td>2818.238770</td>\n",
       "      <td>1118.098633</td>\n",
       "      <td>167.132004</td>\n",
       "      <td>540.160950</td>\n",
       "      <td>760.767334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>534.724976</td>\n",
       "      <td>3015.335205</td>\n",
       "      <td>1007.406494</td>\n",
       "      <td>574.792358</td>\n",
       "      <td>2876.417236</td>\n",
       "      <td>1141.010498</td>\n",
       "      <td>158.403992</td>\n",
       "      <td>527.611633</td>\n",
       "      <td>765.304077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>529.423950</td>\n",
       "      <td>3010.478027</td>\n",
       "      <td>973.444946</td>\n",
       "      <td>578.736755</td>\n",
       "      <td>2875.608887</td>\n",
       "      <td>1106.352905</td>\n",
       "      <td>163.936325</td>\n",
       "      <td>540.751587</td>\n",
       "      <td>746.721191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>543.917847</td>\n",
       "      <td>2959.855225</td>\n",
       "      <td>1002.942017</td>\n",
       "      <td>600.163574</td>\n",
       "      <td>2921.819580</td>\n",
       "      <td>1124.441772</td>\n",
       "      <td>153.599976</td>\n",
       "      <td>548.917603</td>\n",
       "      <td>735.936890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>415.543091</td>\n",
       "      <td>1085.521729</td>\n",
       "      <td>446.313171</td>\n",
       "      <td>1641.077881</td>\n",
       "      <td>4778.175781</td>\n",
       "      <td>319.484497</td>\n",
       "      <td>418.018860</td>\n",
       "      <td>810.050232</td>\n",
       "      <td>567.318176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>386.626038</td>\n",
       "      <td>990.775635</td>\n",
       "      <td>455.084015</td>\n",
       "      <td>1653.742676</td>\n",
       "      <td>4831.918457</td>\n",
       "      <td>318.239410</td>\n",
       "      <td>428.828278</td>\n",
       "      <td>836.078308</td>\n",
       "      <td>576.660156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>352.204071</td>\n",
       "      <td>909.555847</td>\n",
       "      <td>448.546478</td>\n",
       "      <td>1697.422363</td>\n",
       "      <td>4942.354004</td>\n",
       "      <td>315.904175</td>\n",
       "      <td>434.170258</td>\n",
       "      <td>822.891968</td>\n",
       "      <td>576.275024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>356.336823</td>\n",
       "      <td>830.570557</td>\n",
       "      <td>446.085022</td>\n",
       "      <td>1725.667725</td>\n",
       "      <td>4915.351074</td>\n",
       "      <td>312.517944</td>\n",
       "      <td>455.358154</td>\n",
       "      <td>826.022034</td>\n",
       "      <td>565.244507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>346.803467</td>\n",
       "      <td>827.159546</td>\n",
       "      <td>445.986084</td>\n",
       "      <td>1783.878418</td>\n",
       "      <td>4926.551270</td>\n",
       "      <td>299.384399</td>\n",
       "      <td>454.638336</td>\n",
       "      <td>856.649109</td>\n",
       "      <td>572.549500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>319.600525</td>\n",
       "      <td>778.882324</td>\n",
       "      <td>452.693604</td>\n",
       "      <td>1766.618774</td>\n",
       "      <td>4900.985352</td>\n",
       "      <td>295.125763</td>\n",
       "      <td>455.190002</td>\n",
       "      <td>873.377808</td>\n",
       "      <td>581.590820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>318.834503</td>\n",
       "      <td>772.429138</td>\n",
       "      <td>450.526428</td>\n",
       "      <td>1804.441284</td>\n",
       "      <td>4888.824219</td>\n",
       "      <td>292.672455</td>\n",
       "      <td>460.712219</td>\n",
       "      <td>873.199463</td>\n",
       "      <td>557.806458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>304.534882</td>\n",
       "      <td>747.088501</td>\n",
       "      <td>433.972229</td>\n",
       "      <td>1812.826172</td>\n",
       "      <td>4949.355469</td>\n",
       "      <td>292.384491</td>\n",
       "      <td>459.964996</td>\n",
       "      <td>883.855896</td>\n",
       "      <td>556.133789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>297.334167</td>\n",
       "      <td>711.377441</td>\n",
       "      <td>427.756256</td>\n",
       "      <td>1824.887939</td>\n",
       "      <td>4977.011230</td>\n",
       "      <td>288.222961</td>\n",
       "      <td>442.602753</td>\n",
       "      <td>883.110352</td>\n",
       "      <td>578.062012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>291.785248</td>\n",
       "      <td>696.303345</td>\n",
       "      <td>434.420410</td>\n",
       "      <td>1808.774414</td>\n",
       "      <td>4967.067383</td>\n",
       "      <td>288.906006</td>\n",
       "      <td>465.871857</td>\n",
       "      <td>848.896790</td>\n",
       "      <td>584.660889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>275.729004</td>\n",
       "      <td>684.407776</td>\n",
       "      <td>436.270630</td>\n",
       "      <td>1839.743286</td>\n",
       "      <td>4958.617188</td>\n",
       "      <td>292.503906</td>\n",
       "      <td>473.751465</td>\n",
       "      <td>874.837891</td>\n",
       "      <td>576.924805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>267.109802</td>\n",
       "      <td>668.616150</td>\n",
       "      <td>428.174561</td>\n",
       "      <td>1800.920166</td>\n",
       "      <td>5008.872559</td>\n",
       "      <td>289.184784</td>\n",
       "      <td>474.868591</td>\n",
       "      <td>872.165161</td>\n",
       "      <td>579.913147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>272.932220</td>\n",
       "      <td>664.508301</td>\n",
       "      <td>437.244934</td>\n",
       "      <td>1776.687744</td>\n",
       "      <td>5002.353027</td>\n",
       "      <td>288.002045</td>\n",
       "      <td>470.427643</td>\n",
       "      <td>882.427673</td>\n",
       "      <td>600.059509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>269.932495</td>\n",
       "      <td>641.567139</td>\n",
       "      <td>432.846771</td>\n",
       "      <td>1789.313354</td>\n",
       "      <td>5064.815918</td>\n",
       "      <td>303.351807</td>\n",
       "      <td>484.210266</td>\n",
       "      <td>894.807068</td>\n",
       "      <td>602.440247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>265.979431</td>\n",
       "      <td>648.039856</td>\n",
       "      <td>432.684937</td>\n",
       "      <td>1755.147705</td>\n",
       "      <td>5088.369141</td>\n",
       "      <td>288.072937</td>\n",
       "      <td>482.552338</td>\n",
       "      <td>882.221191</td>\n",
       "      <td>592.758179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>564.362610</td>\n",
       "      <td>1541.839600</td>\n",
       "      <td>431.914764</td>\n",
       "      <td>1446.160400</td>\n",
       "      <td>4362.256348</td>\n",
       "      <td>345.700531</td>\n",
       "      <td>401.668152</td>\n",
       "      <td>821.724915</td>\n",
       "      <td>520.839478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>506.248077</td>\n",
       "      <td>1415.009277</td>\n",
       "      <td>443.126648</td>\n",
       "      <td>1487.735840</td>\n",
       "      <td>4414.598633</td>\n",
       "      <td>347.017731</td>\n",
       "      <td>393.479889</td>\n",
       "      <td>795.969360</td>\n",
       "      <td>549.537842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>515.429810</td>\n",
       "      <td>1315.711914</td>\n",
       "      <td>448.500977</td>\n",
       "      <td>1547.112671</td>\n",
       "      <td>4509.289062</td>\n",
       "      <td>323.252441</td>\n",
       "      <td>394.831543</td>\n",
       "      <td>821.549072</td>\n",
       "      <td>529.604004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>476.979919</td>\n",
       "      <td>1244.519043</td>\n",
       "      <td>429.070221</td>\n",
       "      <td>1599.996094</td>\n",
       "      <td>4529.244629</td>\n",
       "      <td>330.517914</td>\n",
       "      <td>398.165985</td>\n",
       "      <td>820.598389</td>\n",
       "      <td>528.216064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785255</th>\n",
       "      <td>475.147705</td>\n",
       "      <td>2842.766357</td>\n",
       "      <td>591.782959</td>\n",
       "      <td>633.689026</td>\n",
       "      <td>3837.167969</td>\n",
       "      <td>771.729736</td>\n",
       "      <td>238.382645</td>\n",
       "      <td>637.508911</td>\n",
       "      <td>712.187988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785256</th>\n",
       "      <td>475.464050</td>\n",
       "      <td>2835.344727</td>\n",
       "      <td>583.647827</td>\n",
       "      <td>640.641846</td>\n",
       "      <td>3873.379639</td>\n",
       "      <td>750.115234</td>\n",
       "      <td>248.780396</td>\n",
       "      <td>651.230103</td>\n",
       "      <td>716.854736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785257</th>\n",
       "      <td>481.847534</td>\n",
       "      <td>2817.340820</td>\n",
       "      <td>578.740479</td>\n",
       "      <td>637.754578</td>\n",
       "      <td>3877.218262</td>\n",
       "      <td>781.961548</td>\n",
       "      <td>241.087128</td>\n",
       "      <td>650.161316</td>\n",
       "      <td>733.283325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785258</th>\n",
       "      <td>465.787201</td>\n",
       "      <td>2804.904053</td>\n",
       "      <td>569.343262</td>\n",
       "      <td>647.821289</td>\n",
       "      <td>3857.104492</td>\n",
       "      <td>767.500122</td>\n",
       "      <td>240.323898</td>\n",
       "      <td>628.991760</td>\n",
       "      <td>726.848083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785259</th>\n",
       "      <td>473.557251</td>\n",
       "      <td>2804.657471</td>\n",
       "      <td>561.592529</td>\n",
       "      <td>633.638123</td>\n",
       "      <td>3901.351562</td>\n",
       "      <td>773.559692</td>\n",
       "      <td>231.294861</td>\n",
       "      <td>641.328003</td>\n",
       "      <td>733.939087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785260</th>\n",
       "      <td>476.991699</td>\n",
       "      <td>2791.731445</td>\n",
       "      <td>579.058228</td>\n",
       "      <td>643.239990</td>\n",
       "      <td>3923.991455</td>\n",
       "      <td>765.078247</td>\n",
       "      <td>235.408035</td>\n",
       "      <td>661.097107</td>\n",
       "      <td>718.653198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785261</th>\n",
       "      <td>476.635651</td>\n",
       "      <td>2750.070068</td>\n",
       "      <td>563.718689</td>\n",
       "      <td>635.713318</td>\n",
       "      <td>3949.495361</td>\n",
       "      <td>772.300659</td>\n",
       "      <td>246.790222</td>\n",
       "      <td>632.139221</td>\n",
       "      <td>730.945618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785262</th>\n",
       "      <td>481.483948</td>\n",
       "      <td>2736.324219</td>\n",
       "      <td>567.592407</td>\n",
       "      <td>660.586548</td>\n",
       "      <td>3944.277100</td>\n",
       "      <td>766.693787</td>\n",
       "      <td>241.954529</td>\n",
       "      <td>633.959961</td>\n",
       "      <td>734.623413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785263</th>\n",
       "      <td>472.956024</td>\n",
       "      <td>2736.230225</td>\n",
       "      <td>562.863098</td>\n",
       "      <td>659.623413</td>\n",
       "      <td>4004.276367</td>\n",
       "      <td>764.286011</td>\n",
       "      <td>239.283524</td>\n",
       "      <td>660.717346</td>\n",
       "      <td>713.998047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785264</th>\n",
       "      <td>466.867065</td>\n",
       "      <td>2732.458008</td>\n",
       "      <td>569.005798</td>\n",
       "      <td>666.330994</td>\n",
       "      <td>4024.462891</td>\n",
       "      <td>769.562134</td>\n",
       "      <td>245.346207</td>\n",
       "      <td>663.933350</td>\n",
       "      <td>725.252869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785265</th>\n",
       "      <td>480.046295</td>\n",
       "      <td>2715.175537</td>\n",
       "      <td>557.256165</td>\n",
       "      <td>645.355225</td>\n",
       "      <td>4037.946533</td>\n",
       "      <td>742.447998</td>\n",
       "      <td>258.840576</td>\n",
       "      <td>654.178589</td>\n",
       "      <td>706.549988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785266</th>\n",
       "      <td>475.449463</td>\n",
       "      <td>2769.531494</td>\n",
       "      <td>536.281189</td>\n",
       "      <td>655.740417</td>\n",
       "      <td>4031.584229</td>\n",
       "      <td>730.941650</td>\n",
       "      <td>255.209244</td>\n",
       "      <td>658.127136</td>\n",
       "      <td>719.966553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785267</th>\n",
       "      <td>502.837372</td>\n",
       "      <td>2716.243408</td>\n",
       "      <td>554.105591</td>\n",
       "      <td>677.264648</td>\n",
       "      <td>3980.764404</td>\n",
       "      <td>730.059265</td>\n",
       "      <td>259.389648</td>\n",
       "      <td>667.066284</td>\n",
       "      <td>708.509277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785268</th>\n",
       "      <td>480.399200</td>\n",
       "      <td>2769.769531</td>\n",
       "      <td>562.770813</td>\n",
       "      <td>679.593140</td>\n",
       "      <td>3983.617920</td>\n",
       "      <td>712.878052</td>\n",
       "      <td>243.168762</td>\n",
       "      <td>654.615112</td>\n",
       "      <td>689.989075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785269</th>\n",
       "      <td>480.807373</td>\n",
       "      <td>2759.118408</td>\n",
       "      <td>561.283997</td>\n",
       "      <td>699.134888</td>\n",
       "      <td>4009.483398</td>\n",
       "      <td>721.416626</td>\n",
       "      <td>262.486328</td>\n",
       "      <td>654.512329</td>\n",
       "      <td>713.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785270</th>\n",
       "      <td>523.245300</td>\n",
       "      <td>2958.067139</td>\n",
       "      <td>543.905029</td>\n",
       "      <td>697.710938</td>\n",
       "      <td>3776.409424</td>\n",
       "      <td>666.272888</td>\n",
       "      <td>263.427338</td>\n",
       "      <td>651.643799</td>\n",
       "      <td>670.494263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785271</th>\n",
       "      <td>495.875732</td>\n",
       "      <td>2955.598145</td>\n",
       "      <td>552.345215</td>\n",
       "      <td>657.297363</td>\n",
       "      <td>3747.275879</td>\n",
       "      <td>703.948242</td>\n",
       "      <td>250.976898</td>\n",
       "      <td>683.805420</td>\n",
       "      <td>657.604126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785272</th>\n",
       "      <td>516.237366</td>\n",
       "      <td>2964.318604</td>\n",
       "      <td>577.262634</td>\n",
       "      <td>668.211304</td>\n",
       "      <td>3793.730225</td>\n",
       "      <td>709.129517</td>\n",
       "      <td>256.492645</td>\n",
       "      <td>667.471802</td>\n",
       "      <td>683.404236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785273</th>\n",
       "      <td>501.681335</td>\n",
       "      <td>2919.767578</td>\n",
       "      <td>550.606995</td>\n",
       "      <td>638.708374</td>\n",
       "      <td>3802.873535</td>\n",
       "      <td>717.027649</td>\n",
       "      <td>250.493073</td>\n",
       "      <td>644.383301</td>\n",
       "      <td>702.473267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785274</th>\n",
       "      <td>510.000214</td>\n",
       "      <td>2995.055420</td>\n",
       "      <td>551.373047</td>\n",
       "      <td>660.349304</td>\n",
       "      <td>3745.079590</td>\n",
       "      <td>722.202332</td>\n",
       "      <td>247.733948</td>\n",
       "      <td>642.584961</td>\n",
       "      <td>695.485779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785275</th>\n",
       "      <td>521.931641</td>\n",
       "      <td>3013.854004</td>\n",
       "      <td>554.299988</td>\n",
       "      <td>631.953613</td>\n",
       "      <td>3710.329346</td>\n",
       "      <td>728.134094</td>\n",
       "      <td>243.888962</td>\n",
       "      <td>643.002563</td>\n",
       "      <td>691.181763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785276</th>\n",
       "      <td>495.807709</td>\n",
       "      <td>2989.654053</td>\n",
       "      <td>585.109985</td>\n",
       "      <td>639.402100</td>\n",
       "      <td>3714.461670</td>\n",
       "      <td>749.341003</td>\n",
       "      <td>235.572479</td>\n",
       "      <td>646.545593</td>\n",
       "      <td>676.286865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785277</th>\n",
       "      <td>512.751160</td>\n",
       "      <td>2958.394531</td>\n",
       "      <td>582.242310</td>\n",
       "      <td>652.946533</td>\n",
       "      <td>3707.906250</td>\n",
       "      <td>729.102295</td>\n",
       "      <td>234.076538</td>\n",
       "      <td>652.182556</td>\n",
       "      <td>688.491699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785278</th>\n",
       "      <td>504.160034</td>\n",
       "      <td>2989.499268</td>\n",
       "      <td>587.763611</td>\n",
       "      <td>658.194824</td>\n",
       "      <td>3695.002930</td>\n",
       "      <td>736.696594</td>\n",
       "      <td>231.465927</td>\n",
       "      <td>651.476196</td>\n",
       "      <td>681.635620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785279</th>\n",
       "      <td>516.517090</td>\n",
       "      <td>2976.608643</td>\n",
       "      <td>586.910278</td>\n",
       "      <td>637.642395</td>\n",
       "      <td>3714.598877</td>\n",
       "      <td>752.355835</td>\n",
       "      <td>235.402084</td>\n",
       "      <td>637.375977</td>\n",
       "      <td>700.220703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785280</th>\n",
       "      <td>506.839264</td>\n",
       "      <td>3055.761230</td>\n",
       "      <td>596.532349</td>\n",
       "      <td>647.959351</td>\n",
       "      <td>3610.522217</td>\n",
       "      <td>731.254211</td>\n",
       "      <td>234.202530</td>\n",
       "      <td>658.012695</td>\n",
       "      <td>675.393433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785281</th>\n",
       "      <td>495.875641</td>\n",
       "      <td>3055.138428</td>\n",
       "      <td>590.330444</td>\n",
       "      <td>623.768982</td>\n",
       "      <td>3686.792236</td>\n",
       "      <td>732.337524</td>\n",
       "      <td>234.055603</td>\n",
       "      <td>654.529114</td>\n",
       "      <td>690.857361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785282</th>\n",
       "      <td>510.378479</td>\n",
       "      <td>3120.925781</td>\n",
       "      <td>585.978516</td>\n",
       "      <td>623.555176</td>\n",
       "      <td>3621.902100</td>\n",
       "      <td>728.794128</td>\n",
       "      <td>230.795486</td>\n",
       "      <td>644.155518</td>\n",
       "      <td>694.674377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785283</th>\n",
       "      <td>520.521362</td>\n",
       "      <td>3118.752197</td>\n",
       "      <td>582.462891</td>\n",
       "      <td>632.206604</td>\n",
       "      <td>3600.458984</td>\n",
       "      <td>756.452515</td>\n",
       "      <td>224.326355</td>\n",
       "      <td>641.459534</td>\n",
       "      <td>696.739502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785284</th>\n",
       "      <td>520.870972</td>\n",
       "      <td>3144.105225</td>\n",
       "      <td>609.523743</td>\n",
       "      <td>617.985229</td>\n",
       "      <td>3561.716309</td>\n",
       "      <td>752.712402</td>\n",
       "      <td>225.764816</td>\n",
       "      <td>620.627991</td>\n",
       "      <td>680.224121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>785285 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              pix1         pix2         pix3         pix4         pix5  \\\n",
       "0       577.447021  3465.876709  1118.598145   550.165466  2460.376953   \n",
       "1       569.863098  3387.739258  1087.530762   556.717407  2552.070557   \n",
       "2       552.641235  3405.411377  1082.131104   558.981445  2560.040771   \n",
       "3       571.821167  3340.533691  1073.962036   568.324768  2643.155273   \n",
       "4       538.292114  3248.569336  1021.301208   548.598145  2691.563965   \n",
       "5       553.332214  3183.050293  1026.863281   578.003784  2679.043457   \n",
       "6       541.202332  3137.938232  1035.948364   589.566528  2743.923584   \n",
       "7       547.699829  3057.428467  1034.417603   590.498108  2818.238770   \n",
       "8       534.724976  3015.335205  1007.406494   574.792358  2876.417236   \n",
       "9       529.423950  3010.478027   973.444946   578.736755  2875.608887   \n",
       "10      543.917847  2959.855225  1002.942017   600.163574  2921.819580   \n",
       "11      415.543091  1085.521729   446.313171  1641.077881  4778.175781   \n",
       "12      386.626038   990.775635   455.084015  1653.742676  4831.918457   \n",
       "13      352.204071   909.555847   448.546478  1697.422363  4942.354004   \n",
       "14      356.336823   830.570557   446.085022  1725.667725  4915.351074   \n",
       "15      346.803467   827.159546   445.986084  1783.878418  4926.551270   \n",
       "16      319.600525   778.882324   452.693604  1766.618774  4900.985352   \n",
       "17      318.834503   772.429138   450.526428  1804.441284  4888.824219   \n",
       "18      304.534882   747.088501   433.972229  1812.826172  4949.355469   \n",
       "19      297.334167   711.377441   427.756256  1824.887939  4977.011230   \n",
       "20      291.785248   696.303345   434.420410  1808.774414  4967.067383   \n",
       "21      275.729004   684.407776   436.270630  1839.743286  4958.617188   \n",
       "22      267.109802   668.616150   428.174561  1800.920166  5008.872559   \n",
       "23      272.932220   664.508301   437.244934  1776.687744  5002.353027   \n",
       "24      269.932495   641.567139   432.846771  1789.313354  5064.815918   \n",
       "25      265.979431   648.039856   432.684937  1755.147705  5088.369141   \n",
       "26      564.362610  1541.839600   431.914764  1446.160400  4362.256348   \n",
       "27      506.248077  1415.009277   443.126648  1487.735840  4414.598633   \n",
       "28      515.429810  1315.711914   448.500977  1547.112671  4509.289062   \n",
       "29      476.979919  1244.519043   429.070221  1599.996094  4529.244629   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "785255  475.147705  2842.766357   591.782959   633.689026  3837.167969   \n",
       "785256  475.464050  2835.344727   583.647827   640.641846  3873.379639   \n",
       "785257  481.847534  2817.340820   578.740479   637.754578  3877.218262   \n",
       "785258  465.787201  2804.904053   569.343262   647.821289  3857.104492   \n",
       "785259  473.557251  2804.657471   561.592529   633.638123  3901.351562   \n",
       "785260  476.991699  2791.731445   579.058228   643.239990  3923.991455   \n",
       "785261  476.635651  2750.070068   563.718689   635.713318  3949.495361   \n",
       "785262  481.483948  2736.324219   567.592407   660.586548  3944.277100   \n",
       "785263  472.956024  2736.230225   562.863098   659.623413  4004.276367   \n",
       "785264  466.867065  2732.458008   569.005798   666.330994  4024.462891   \n",
       "785265  480.046295  2715.175537   557.256165   645.355225  4037.946533   \n",
       "785266  475.449463  2769.531494   536.281189   655.740417  4031.584229   \n",
       "785267  502.837372  2716.243408   554.105591   677.264648  3980.764404   \n",
       "785268  480.399200  2769.769531   562.770813   679.593140  3983.617920   \n",
       "785269  480.807373  2759.118408   561.283997   699.134888  4009.483398   \n",
       "785270  523.245300  2958.067139   543.905029   697.710938  3776.409424   \n",
       "785271  495.875732  2955.598145   552.345215   657.297363  3747.275879   \n",
       "785272  516.237366  2964.318604   577.262634   668.211304  3793.730225   \n",
       "785273  501.681335  2919.767578   550.606995   638.708374  3802.873535   \n",
       "785274  510.000214  2995.055420   551.373047   660.349304  3745.079590   \n",
       "785275  521.931641  3013.854004   554.299988   631.953613  3710.329346   \n",
       "785276  495.807709  2989.654053   585.109985   639.402100  3714.461670   \n",
       "785277  512.751160  2958.394531   582.242310   652.946533  3707.906250   \n",
       "785278  504.160034  2989.499268   587.763611   658.194824  3695.002930   \n",
       "785279  516.517090  2976.608643   586.910278   637.642395  3714.598877   \n",
       "785280  506.839264  3055.761230   596.532349   647.959351  3610.522217   \n",
       "785281  495.875641  3055.138428   590.330444   623.768982  3686.792236   \n",
       "785282  510.378479  3120.925781   585.978516   623.555176  3621.902100   \n",
       "785283  520.521362  3118.752197   582.462891   632.206604  3600.458984   \n",
       "785284  520.870972  3144.105225   609.523743   617.985229  3561.716309   \n",
       "\n",
       "               pix6        pix7        pix8        pix9  \n",
       "0        994.374207  141.741592  521.385254  694.330688  \n",
       "1       1021.892700  134.061081  511.347778  666.346069  \n",
       "2       1058.485352  146.488220  513.809570  691.019653  \n",
       "3       1024.431641  147.687546  525.451538  727.683472  \n",
       "4       1066.199707  154.170990  541.407532  718.537537  \n",
       "5       1085.607422  154.789886  517.848328  722.456909  \n",
       "6       1072.393555  158.401291  539.485718  724.184937  \n",
       "7       1118.098633  167.132004  540.160950  760.767334  \n",
       "8       1141.010498  158.403992  527.611633  765.304077  \n",
       "9       1106.352905  163.936325  540.751587  746.721191  \n",
       "10      1124.441772  153.599976  548.917603  735.936890  \n",
       "11       319.484497  418.018860  810.050232  567.318176  \n",
       "12       318.239410  428.828278  836.078308  576.660156  \n",
       "13       315.904175  434.170258  822.891968  576.275024  \n",
       "14       312.517944  455.358154  826.022034  565.244507  \n",
       "15       299.384399  454.638336  856.649109  572.549500  \n",
       "16       295.125763  455.190002  873.377808  581.590820  \n",
       "17       292.672455  460.712219  873.199463  557.806458  \n",
       "18       292.384491  459.964996  883.855896  556.133789  \n",
       "19       288.222961  442.602753  883.110352  578.062012  \n",
       "20       288.906006  465.871857  848.896790  584.660889  \n",
       "21       292.503906  473.751465  874.837891  576.924805  \n",
       "22       289.184784  474.868591  872.165161  579.913147  \n",
       "23       288.002045  470.427643  882.427673  600.059509  \n",
       "24       303.351807  484.210266  894.807068  602.440247  \n",
       "25       288.072937  482.552338  882.221191  592.758179  \n",
       "26       345.700531  401.668152  821.724915  520.839478  \n",
       "27       347.017731  393.479889  795.969360  549.537842  \n",
       "28       323.252441  394.831543  821.549072  529.604004  \n",
       "29       330.517914  398.165985  820.598389  528.216064  \n",
       "...             ...         ...         ...         ...  \n",
       "785255   771.729736  238.382645  637.508911  712.187988  \n",
       "785256   750.115234  248.780396  651.230103  716.854736  \n",
       "785257   781.961548  241.087128  650.161316  733.283325  \n",
       "785258   767.500122  240.323898  628.991760  726.848083  \n",
       "785259   773.559692  231.294861  641.328003  733.939087  \n",
       "785260   765.078247  235.408035  661.097107  718.653198  \n",
       "785261   772.300659  246.790222  632.139221  730.945618  \n",
       "785262   766.693787  241.954529  633.959961  734.623413  \n",
       "785263   764.286011  239.283524  660.717346  713.998047  \n",
       "785264   769.562134  245.346207  663.933350  725.252869  \n",
       "785265   742.447998  258.840576  654.178589  706.549988  \n",
       "785266   730.941650  255.209244  658.127136  719.966553  \n",
       "785267   730.059265  259.389648  667.066284  708.509277  \n",
       "785268   712.878052  243.168762  654.615112  689.989075  \n",
       "785269   721.416626  262.486328  654.512329  713.022461  \n",
       "785270   666.272888  263.427338  651.643799  670.494263  \n",
       "785271   703.948242  250.976898  683.805420  657.604126  \n",
       "785272   709.129517  256.492645  667.471802  683.404236  \n",
       "785273   717.027649  250.493073  644.383301  702.473267  \n",
       "785274   722.202332  247.733948  642.584961  695.485779  \n",
       "785275   728.134094  243.888962  643.002563  691.181763  \n",
       "785276   749.341003  235.572479  646.545593  676.286865  \n",
       "785277   729.102295  234.076538  652.182556  688.491699  \n",
       "785278   736.696594  231.465927  651.476196  681.635620  \n",
       "785279   752.355835  235.402084  637.375977  700.220703  \n",
       "785280   731.254211  234.202530  658.012695  675.393433  \n",
       "785281   732.337524  234.055603  654.529114  690.857361  \n",
       "785282   728.794128  230.795486  644.155518  694.674377  \n",
       "785283   756.452515  224.326355  641.459534  696.739502  \n",
       "785284   752.712402  225.764816  620.627991  680.224121  \n",
       "\n",
       "[785285 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PLDpixels = pd.DataFrame({key:spitzerDataRaw[key] for key in spitzerDataRaw.columns.values if 'pix' in key})\n",
    "PLDpixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PLDnorm = np.sum(np.array(PLDpixels),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pix1</th>\n",
       "      <th>pix2</th>\n",
       "      <th>pix3</th>\n",
       "      <th>pix4</th>\n",
       "      <th>pix5</th>\n",
       "      <th>pix6</th>\n",
       "      <th>pix7</th>\n",
       "      <th>pix8</th>\n",
       "      <th>pix9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.054868</td>\n",
       "      <td>0.329321</td>\n",
       "      <td>0.106287</td>\n",
       "      <td>0.052276</td>\n",
       "      <td>0.233781</td>\n",
       "      <td>0.094484</td>\n",
       "      <td>0.013468</td>\n",
       "      <td>0.049541</td>\n",
       "      <td>0.065974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.054337</td>\n",
       "      <td>0.323024</td>\n",
       "      <td>0.103697</td>\n",
       "      <td>0.053084</td>\n",
       "      <td>0.243342</td>\n",
       "      <td>0.097438</td>\n",
       "      <td>0.012783</td>\n",
       "      <td>0.048758</td>\n",
       "      <td>0.063537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.052289</td>\n",
       "      <td>0.322207</td>\n",
       "      <td>0.102387</td>\n",
       "      <td>0.052889</td>\n",
       "      <td>0.242221</td>\n",
       "      <td>0.100150</td>\n",
       "      <td>0.013860</td>\n",
       "      <td>0.048615</td>\n",
       "      <td>0.065382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.053828</td>\n",
       "      <td>0.314461</td>\n",
       "      <td>0.101097</td>\n",
       "      <td>0.053499</td>\n",
       "      <td>0.248813</td>\n",
       "      <td>0.096435</td>\n",
       "      <td>0.013903</td>\n",
       "      <td>0.049463</td>\n",
       "      <td>0.068500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051126</td>\n",
       "      <td>0.308546</td>\n",
       "      <td>0.097002</td>\n",
       "      <td>0.052105</td>\n",
       "      <td>0.255642</td>\n",
       "      <td>0.101267</td>\n",
       "      <td>0.014643</td>\n",
       "      <td>0.051422</td>\n",
       "      <td>0.068246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.052693</td>\n",
       "      <td>0.303119</td>\n",
       "      <td>0.097787</td>\n",
       "      <td>0.055043</td>\n",
       "      <td>0.255123</td>\n",
       "      <td>0.103381</td>\n",
       "      <td>0.014740</td>\n",
       "      <td>0.049314</td>\n",
       "      <td>0.068799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.051333</td>\n",
       "      <td>0.297631</td>\n",
       "      <td>0.098259</td>\n",
       "      <td>0.055920</td>\n",
       "      <td>0.260259</td>\n",
       "      <td>0.101716</td>\n",
       "      <td>0.015024</td>\n",
       "      <td>0.051170</td>\n",
       "      <td>0.068688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.051502</td>\n",
       "      <td>0.287502</td>\n",
       "      <td>0.097271</td>\n",
       "      <td>0.055527</td>\n",
       "      <td>0.265011</td>\n",
       "      <td>0.105139</td>\n",
       "      <td>0.015716</td>\n",
       "      <td>0.050794</td>\n",
       "      <td>0.071538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.050441</td>\n",
       "      <td>0.284439</td>\n",
       "      <td>0.095029</td>\n",
       "      <td>0.054221</td>\n",
       "      <td>0.271334</td>\n",
       "      <td>0.107632</td>\n",
       "      <td>0.014942</td>\n",
       "      <td>0.049770</td>\n",
       "      <td>0.072192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.050299</td>\n",
       "      <td>0.286019</td>\n",
       "      <td>0.092485</td>\n",
       "      <td>0.054984</td>\n",
       "      <td>0.273205</td>\n",
       "      <td>0.105112</td>\n",
       "      <td>0.015575</td>\n",
       "      <td>0.051376</td>\n",
       "      <td>0.070944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.051354</td>\n",
       "      <td>0.279453</td>\n",
       "      <td>0.094692</td>\n",
       "      <td>0.056664</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.106164</td>\n",
       "      <td>0.014502</td>\n",
       "      <td>0.051826</td>\n",
       "      <td>0.069483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.039645</td>\n",
       "      <td>0.103565</td>\n",
       "      <td>0.042581</td>\n",
       "      <td>0.156569</td>\n",
       "      <td>0.455867</td>\n",
       "      <td>0.030481</td>\n",
       "      <td>0.039882</td>\n",
       "      <td>0.077284</td>\n",
       "      <td>0.054126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.036899</td>\n",
       "      <td>0.094558</td>\n",
       "      <td>0.043433</td>\n",
       "      <td>0.157831</td>\n",
       "      <td>0.461151</td>\n",
       "      <td>0.030372</td>\n",
       "      <td>0.040927</td>\n",
       "      <td>0.079794</td>\n",
       "      <td>0.055036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.033545</td>\n",
       "      <td>0.086630</td>\n",
       "      <td>0.042721</td>\n",
       "      <td>0.161670</td>\n",
       "      <td>0.470731</td>\n",
       "      <td>0.030088</td>\n",
       "      <td>0.041352</td>\n",
       "      <td>0.078376</td>\n",
       "      <td>0.054887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.034154</td>\n",
       "      <td>0.079609</td>\n",
       "      <td>0.042756</td>\n",
       "      <td>0.165402</td>\n",
       "      <td>0.471128</td>\n",
       "      <td>0.029954</td>\n",
       "      <td>0.043645</td>\n",
       "      <td>0.079173</td>\n",
       "      <td>0.054178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.032986</td>\n",
       "      <td>0.078675</td>\n",
       "      <td>0.042420</td>\n",
       "      <td>0.169673</td>\n",
       "      <td>0.468588</td>\n",
       "      <td>0.028476</td>\n",
       "      <td>0.043243</td>\n",
       "      <td>0.081480</td>\n",
       "      <td>0.054458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.030660</td>\n",
       "      <td>0.074720</td>\n",
       "      <td>0.043428</td>\n",
       "      <td>0.169475</td>\n",
       "      <td>0.470161</td>\n",
       "      <td>0.028312</td>\n",
       "      <td>0.043667</td>\n",
       "      <td>0.083785</td>\n",
       "      <td>0.055793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.030600</td>\n",
       "      <td>0.074133</td>\n",
       "      <td>0.043239</td>\n",
       "      <td>0.173180</td>\n",
       "      <td>0.469202</td>\n",
       "      <td>0.028089</td>\n",
       "      <td>0.044217</td>\n",
       "      <td>0.083805</td>\n",
       "      <td>0.053535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.029170</td>\n",
       "      <td>0.071559</td>\n",
       "      <td>0.041568</td>\n",
       "      <td>0.173640</td>\n",
       "      <td>0.474071</td>\n",
       "      <td>0.028006</td>\n",
       "      <td>0.044057</td>\n",
       "      <td>0.084660</td>\n",
       "      <td>0.053269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.028507</td>\n",
       "      <td>0.068203</td>\n",
       "      <td>0.041011</td>\n",
       "      <td>0.174959</td>\n",
       "      <td>0.477166</td>\n",
       "      <td>0.027633</td>\n",
       "      <td>0.042434</td>\n",
       "      <td>0.084667</td>\n",
       "      <td>0.055421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.028092</td>\n",
       "      <td>0.067038</td>\n",
       "      <td>0.041825</td>\n",
       "      <td>0.174144</td>\n",
       "      <td>0.478215</td>\n",
       "      <td>0.027815</td>\n",
       "      <td>0.044853</td>\n",
       "      <td>0.081729</td>\n",
       "      <td>0.056289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.026480</td>\n",
       "      <td>0.065728</td>\n",
       "      <td>0.041898</td>\n",
       "      <td>0.176681</td>\n",
       "      <td>0.476205</td>\n",
       "      <td>0.028091</td>\n",
       "      <td>0.045497</td>\n",
       "      <td>0.084016</td>\n",
       "      <td>0.055405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.025709</td>\n",
       "      <td>0.064353</td>\n",
       "      <td>0.041211</td>\n",
       "      <td>0.173335</td>\n",
       "      <td>0.482094</td>\n",
       "      <td>0.027833</td>\n",
       "      <td>0.045705</td>\n",
       "      <td>0.083944</td>\n",
       "      <td>0.055815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.026257</td>\n",
       "      <td>0.063928</td>\n",
       "      <td>0.042064</td>\n",
       "      <td>0.170923</td>\n",
       "      <td>0.481243</td>\n",
       "      <td>0.027707</td>\n",
       "      <td>0.045257</td>\n",
       "      <td>0.084893</td>\n",
       "      <td>0.057728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.025749</td>\n",
       "      <td>0.061199</td>\n",
       "      <td>0.041289</td>\n",
       "      <td>0.170683</td>\n",
       "      <td>0.483133</td>\n",
       "      <td>0.028937</td>\n",
       "      <td>0.046189</td>\n",
       "      <td>0.085356</td>\n",
       "      <td>0.057467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.025487</td>\n",
       "      <td>0.062098</td>\n",
       "      <td>0.041461</td>\n",
       "      <td>0.168185</td>\n",
       "      <td>0.487587</td>\n",
       "      <td>0.027604</td>\n",
       "      <td>0.046240</td>\n",
       "      <td>0.084538</td>\n",
       "      <td>0.056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.054076</td>\n",
       "      <td>0.147736</td>\n",
       "      <td>0.041385</td>\n",
       "      <td>0.138568</td>\n",
       "      <td>0.417982</td>\n",
       "      <td>0.033124</td>\n",
       "      <td>0.038487</td>\n",
       "      <td>0.078736</td>\n",
       "      <td>0.049906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.136680</td>\n",
       "      <td>0.042803</td>\n",
       "      <td>0.143705</td>\n",
       "      <td>0.426419</td>\n",
       "      <td>0.033519</td>\n",
       "      <td>0.038007</td>\n",
       "      <td>0.076885</td>\n",
       "      <td>0.053081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.049535</td>\n",
       "      <td>0.126447</td>\n",
       "      <td>0.043103</td>\n",
       "      <td>0.148685</td>\n",
       "      <td>0.433365</td>\n",
       "      <td>0.031066</td>\n",
       "      <td>0.037945</td>\n",
       "      <td>0.078955</td>\n",
       "      <td>0.050898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.046052</td>\n",
       "      <td>0.120159</td>\n",
       "      <td>0.041427</td>\n",
       "      <td>0.154480</td>\n",
       "      <td>0.437299</td>\n",
       "      <td>0.031912</td>\n",
       "      <td>0.038443</td>\n",
       "      <td>0.079229</td>\n",
       "      <td>0.050999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785255</th>\n",
       "      <td>0.044239</td>\n",
       "      <td>0.264681</td>\n",
       "      <td>0.055099</td>\n",
       "      <td>0.059001</td>\n",
       "      <td>0.357266</td>\n",
       "      <td>0.071853</td>\n",
       "      <td>0.022195</td>\n",
       "      <td>0.059356</td>\n",
       "      <td>0.066309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785256</th>\n",
       "      <td>0.044125</td>\n",
       "      <td>0.263130</td>\n",
       "      <td>0.054165</td>\n",
       "      <td>0.059454</td>\n",
       "      <td>0.359463</td>\n",
       "      <td>0.069613</td>\n",
       "      <td>0.023088</td>\n",
       "      <td>0.060436</td>\n",
       "      <td>0.066527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785257</th>\n",
       "      <td>0.044618</td>\n",
       "      <td>0.260880</td>\n",
       "      <td>0.053590</td>\n",
       "      <td>0.059055</td>\n",
       "      <td>0.359022</td>\n",
       "      <td>0.072408</td>\n",
       "      <td>0.022324</td>\n",
       "      <td>0.060203</td>\n",
       "      <td>0.067900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785258</th>\n",
       "      <td>0.043496</td>\n",
       "      <td>0.261929</td>\n",
       "      <td>0.053167</td>\n",
       "      <td>0.060495</td>\n",
       "      <td>0.360187</td>\n",
       "      <td>0.071671</td>\n",
       "      <td>0.022442</td>\n",
       "      <td>0.058737</td>\n",
       "      <td>0.067875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785259</th>\n",
       "      <td>0.044032</td>\n",
       "      <td>0.260779</td>\n",
       "      <td>0.052217</td>\n",
       "      <td>0.058916</td>\n",
       "      <td>0.362750</td>\n",
       "      <td>0.071926</td>\n",
       "      <td>0.021506</td>\n",
       "      <td>0.059631</td>\n",
       "      <td>0.068242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785260</th>\n",
       "      <td>0.044185</td>\n",
       "      <td>0.258607</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.059585</td>\n",
       "      <td>0.363492</td>\n",
       "      <td>0.070872</td>\n",
       "      <td>0.021807</td>\n",
       "      <td>0.061240</td>\n",
       "      <td>0.066571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785261</th>\n",
       "      <td>0.044306</td>\n",
       "      <td>0.255635</td>\n",
       "      <td>0.052401</td>\n",
       "      <td>0.059093</td>\n",
       "      <td>0.367128</td>\n",
       "      <td>0.071790</td>\n",
       "      <td>0.022941</td>\n",
       "      <td>0.058761</td>\n",
       "      <td>0.067946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785262</th>\n",
       "      <td>0.044716</td>\n",
       "      <td>0.254128</td>\n",
       "      <td>0.052714</td>\n",
       "      <td>0.061350</td>\n",
       "      <td>0.366313</td>\n",
       "      <td>0.071204</td>\n",
       "      <td>0.022471</td>\n",
       "      <td>0.058877</td>\n",
       "      <td>0.068226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785263</th>\n",
       "      <td>0.043735</td>\n",
       "      <td>0.253021</td>\n",
       "      <td>0.052048</td>\n",
       "      <td>0.060996</td>\n",
       "      <td>0.370278</td>\n",
       "      <td>0.070674</td>\n",
       "      <td>0.022127</td>\n",
       "      <td>0.061097</td>\n",
       "      <td>0.066024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785264</th>\n",
       "      <td>0.042977</td>\n",
       "      <td>0.251533</td>\n",
       "      <td>0.052379</td>\n",
       "      <td>0.061338</td>\n",
       "      <td>0.370467</td>\n",
       "      <td>0.070841</td>\n",
       "      <td>0.022585</td>\n",
       "      <td>0.061118</td>\n",
       "      <td>0.066762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785265</th>\n",
       "      <td>0.044458</td>\n",
       "      <td>0.251456</td>\n",
       "      <td>0.051608</td>\n",
       "      <td>0.059767</td>\n",
       "      <td>0.373960</td>\n",
       "      <td>0.068759</td>\n",
       "      <td>0.023972</td>\n",
       "      <td>0.060584</td>\n",
       "      <td>0.065435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785266</th>\n",
       "      <td>0.043890</td>\n",
       "      <td>0.255661</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>0.060533</td>\n",
       "      <td>0.372163</td>\n",
       "      <td>0.067475</td>\n",
       "      <td>0.023559</td>\n",
       "      <td>0.060753</td>\n",
       "      <td>0.066462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785267</th>\n",
       "      <td>0.046575</td>\n",
       "      <td>0.251592</td>\n",
       "      <td>0.051324</td>\n",
       "      <td>0.062732</td>\n",
       "      <td>0.368718</td>\n",
       "      <td>0.067622</td>\n",
       "      <td>0.024026</td>\n",
       "      <td>0.061787</td>\n",
       "      <td>0.065626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785268</th>\n",
       "      <td>0.044577</td>\n",
       "      <td>0.257012</td>\n",
       "      <td>0.052221</td>\n",
       "      <td>0.063061</td>\n",
       "      <td>0.369648</td>\n",
       "      <td>0.066149</td>\n",
       "      <td>0.022564</td>\n",
       "      <td>0.060743</td>\n",
       "      <td>0.064025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785269</th>\n",
       "      <td>0.044268</td>\n",
       "      <td>0.254033</td>\n",
       "      <td>0.051678</td>\n",
       "      <td>0.064370</td>\n",
       "      <td>0.369154</td>\n",
       "      <td>0.066421</td>\n",
       "      <td>0.024167</td>\n",
       "      <td>0.060261</td>\n",
       "      <td>0.065648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785270</th>\n",
       "      <td>0.048669</td>\n",
       "      <td>0.275139</td>\n",
       "      <td>0.050590</td>\n",
       "      <td>0.064896</td>\n",
       "      <td>0.351255</td>\n",
       "      <td>0.061972</td>\n",
       "      <td>0.024502</td>\n",
       "      <td>0.060611</td>\n",
       "      <td>0.062365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785271</th>\n",
       "      <td>0.046323</td>\n",
       "      <td>0.276102</td>\n",
       "      <td>0.051598</td>\n",
       "      <td>0.061403</td>\n",
       "      <td>0.350058</td>\n",
       "      <td>0.065761</td>\n",
       "      <td>0.023445</td>\n",
       "      <td>0.063879</td>\n",
       "      <td>0.061431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785272</th>\n",
       "      <td>0.047640</td>\n",
       "      <td>0.273556</td>\n",
       "      <td>0.053271</td>\n",
       "      <td>0.061664</td>\n",
       "      <td>0.350096</td>\n",
       "      <td>0.065440</td>\n",
       "      <td>0.023670</td>\n",
       "      <td>0.061596</td>\n",
       "      <td>0.063066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785273</th>\n",
       "      <td>0.046764</td>\n",
       "      <td>0.272163</td>\n",
       "      <td>0.051324</td>\n",
       "      <td>0.059536</td>\n",
       "      <td>0.354481</td>\n",
       "      <td>0.066837</td>\n",
       "      <td>0.023349</td>\n",
       "      <td>0.060065</td>\n",
       "      <td>0.065480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785274</th>\n",
       "      <td>0.047354</td>\n",
       "      <td>0.278096</td>\n",
       "      <td>0.051196</td>\n",
       "      <td>0.061315</td>\n",
       "      <td>0.347737</td>\n",
       "      <td>0.067058</td>\n",
       "      <td>0.023003</td>\n",
       "      <td>0.059665</td>\n",
       "      <td>0.064577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785275</th>\n",
       "      <td>0.048603</td>\n",
       "      <td>0.280657</td>\n",
       "      <td>0.051618</td>\n",
       "      <td>0.058849</td>\n",
       "      <td>0.345514</td>\n",
       "      <td>0.067805</td>\n",
       "      <td>0.022711</td>\n",
       "      <td>0.059878</td>\n",
       "      <td>0.064364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785276</th>\n",
       "      <td>0.046198</td>\n",
       "      <td>0.278569</td>\n",
       "      <td>0.054519</td>\n",
       "      <td>0.059578</td>\n",
       "      <td>0.346105</td>\n",
       "      <td>0.069822</td>\n",
       "      <td>0.021950</td>\n",
       "      <td>0.060244</td>\n",
       "      <td>0.063015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785277</th>\n",
       "      <td>0.047840</td>\n",
       "      <td>0.276019</td>\n",
       "      <td>0.054323</td>\n",
       "      <td>0.060920</td>\n",
       "      <td>0.345948</td>\n",
       "      <td>0.068025</td>\n",
       "      <td>0.021839</td>\n",
       "      <td>0.060849</td>\n",
       "      <td>0.064236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785278</th>\n",
       "      <td>0.046960</td>\n",
       "      <td>0.278458</td>\n",
       "      <td>0.054748</td>\n",
       "      <td>0.061308</td>\n",
       "      <td>0.344173</td>\n",
       "      <td>0.068620</td>\n",
       "      <td>0.021560</td>\n",
       "      <td>0.060682</td>\n",
       "      <td>0.063491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785279</th>\n",
       "      <td>0.048014</td>\n",
       "      <td>0.276697</td>\n",
       "      <td>0.054558</td>\n",
       "      <td>0.059273</td>\n",
       "      <td>0.345299</td>\n",
       "      <td>0.069937</td>\n",
       "      <td>0.021882</td>\n",
       "      <td>0.059249</td>\n",
       "      <td>0.065091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785280</th>\n",
       "      <td>0.047295</td>\n",
       "      <td>0.285146</td>\n",
       "      <td>0.055665</td>\n",
       "      <td>0.060464</td>\n",
       "      <td>0.336913</td>\n",
       "      <td>0.068236</td>\n",
       "      <td>0.021854</td>\n",
       "      <td>0.061402</td>\n",
       "      <td>0.063024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785281</th>\n",
       "      <td>0.046069</td>\n",
       "      <td>0.283838</td>\n",
       "      <td>0.054845</td>\n",
       "      <td>0.057951</td>\n",
       "      <td>0.342521</td>\n",
       "      <td>0.068038</td>\n",
       "      <td>0.021745</td>\n",
       "      <td>0.060809</td>\n",
       "      <td>0.064184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785282</th>\n",
       "      <td>0.047428</td>\n",
       "      <td>0.290018</td>\n",
       "      <td>0.054453</td>\n",
       "      <td>0.057945</td>\n",
       "      <td>0.336572</td>\n",
       "      <td>0.067724</td>\n",
       "      <td>0.021447</td>\n",
       "      <td>0.059859</td>\n",
       "      <td>0.064554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785283</th>\n",
       "      <td>0.048316</td>\n",
       "      <td>0.289487</td>\n",
       "      <td>0.054065</td>\n",
       "      <td>0.058682</td>\n",
       "      <td>0.334200</td>\n",
       "      <td>0.070215</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.059541</td>\n",
       "      <td>0.064672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785284</th>\n",
       "      <td>0.048527</td>\n",
       "      <td>0.292924</td>\n",
       "      <td>0.056787</td>\n",
       "      <td>0.057575</td>\n",
       "      <td>0.331831</td>\n",
       "      <td>0.070127</td>\n",
       "      <td>0.021034</td>\n",
       "      <td>0.057821</td>\n",
       "      <td>0.063374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>785285 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pix1      pix2      pix3      pix4      pix5      pix6      pix7  \\\n",
       "0       0.054868  0.329321  0.106287  0.052276  0.233781  0.094484  0.013468   \n",
       "1       0.054337  0.323024  0.103697  0.053084  0.243342  0.097438  0.012783   \n",
       "2       0.052289  0.322207  0.102387  0.052889  0.242221  0.100150  0.013860   \n",
       "3       0.053828  0.314461  0.101097  0.053499  0.248813  0.096435  0.013903   \n",
       "4       0.051126  0.308546  0.097002  0.052105  0.255642  0.101267  0.014643   \n",
       "5       0.052693  0.303119  0.097787  0.055043  0.255123  0.103381  0.014740   \n",
       "6       0.051333  0.297631  0.098259  0.055920  0.260259  0.101716  0.015024   \n",
       "7       0.051502  0.287502  0.097271  0.055527  0.265011  0.105139  0.015716   \n",
       "8       0.050441  0.284439  0.095029  0.054221  0.271334  0.107632  0.014942   \n",
       "9       0.050299  0.286019  0.092485  0.054984  0.273205  0.105112  0.015575   \n",
       "10      0.051354  0.279453  0.094692  0.056664  0.275862  0.106164  0.014502   \n",
       "11      0.039645  0.103565  0.042581  0.156569  0.455867  0.030481  0.039882   \n",
       "12      0.036899  0.094558  0.043433  0.157831  0.461151  0.030372  0.040927   \n",
       "13      0.033545  0.086630  0.042721  0.161670  0.470731  0.030088  0.041352   \n",
       "14      0.034154  0.079609  0.042756  0.165402  0.471128  0.029954  0.043645   \n",
       "15      0.032986  0.078675  0.042420  0.169673  0.468588  0.028476  0.043243   \n",
       "16      0.030660  0.074720  0.043428  0.169475  0.470161  0.028312  0.043667   \n",
       "17      0.030600  0.074133  0.043239  0.173180  0.469202  0.028089  0.044217   \n",
       "18      0.029170  0.071559  0.041568  0.173640  0.474071  0.028006  0.044057   \n",
       "19      0.028507  0.068203  0.041011  0.174959  0.477166  0.027633  0.042434   \n",
       "20      0.028092  0.067038  0.041825  0.174144  0.478215  0.027815  0.044853   \n",
       "21      0.026480  0.065728  0.041898  0.176681  0.476205  0.028091  0.045497   \n",
       "22      0.025709  0.064353  0.041211  0.173335  0.482094  0.027833  0.045705   \n",
       "23      0.026257  0.063928  0.042064  0.170923  0.481243  0.027707  0.045257   \n",
       "24      0.025749  0.061199  0.041289  0.170683  0.483133  0.028937  0.046189   \n",
       "25      0.025487  0.062098  0.041461  0.168185  0.487587  0.027604  0.046240   \n",
       "26      0.054076  0.147736  0.041385  0.138568  0.417982  0.033124  0.038487   \n",
       "27      0.048900  0.136680  0.042803  0.143705  0.426419  0.033519  0.038007   \n",
       "28      0.049535  0.126447  0.043103  0.148685  0.433365  0.031066  0.037945   \n",
       "29      0.046052  0.120159  0.041427  0.154480  0.437299  0.031912  0.038443   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "785255  0.044239  0.264681  0.055099  0.059001  0.357266  0.071853  0.022195   \n",
       "785256  0.044125  0.263130  0.054165  0.059454  0.359463  0.069613  0.023088   \n",
       "785257  0.044618  0.260880  0.053590  0.059055  0.359022  0.072408  0.022324   \n",
       "785258  0.043496  0.261929  0.053167  0.060495  0.360187  0.071671  0.022442   \n",
       "785259  0.044032  0.260779  0.052217  0.058916  0.362750  0.071926  0.021506   \n",
       "785260  0.044185  0.258607  0.053640  0.059585  0.363492  0.070872  0.021807   \n",
       "785261  0.044306  0.255635  0.052401  0.059093  0.367128  0.071790  0.022941   \n",
       "785262  0.044716  0.254128  0.052714  0.061350  0.366313  0.071204  0.022471   \n",
       "785263  0.043735  0.253021  0.052048  0.060996  0.370278  0.070674  0.022127   \n",
       "785264  0.042977  0.251533  0.052379  0.061338  0.370467  0.070841  0.022585   \n",
       "785265  0.044458  0.251456  0.051608  0.059767  0.373960  0.068759  0.023972   \n",
       "785266  0.043890  0.255661  0.049505  0.060533  0.372163  0.067475  0.023559   \n",
       "785267  0.046575  0.251592  0.051324  0.062732  0.368718  0.067622  0.024026   \n",
       "785268  0.044577  0.257012  0.052221  0.063061  0.369648  0.066149  0.022564   \n",
       "785269  0.044268  0.254033  0.051678  0.064370  0.369154  0.066421  0.024167   \n",
       "785270  0.048669  0.275139  0.050590  0.064896  0.351255  0.061972  0.024502   \n",
       "785271  0.046323  0.276102  0.051598  0.061403  0.350058  0.065761  0.023445   \n",
       "785272  0.047640  0.273556  0.053271  0.061664  0.350096  0.065440  0.023670   \n",
       "785273  0.046764  0.272163  0.051324  0.059536  0.354481  0.066837  0.023349   \n",
       "785274  0.047354  0.278096  0.051196  0.061315  0.347737  0.067058  0.023003   \n",
       "785275  0.048603  0.280657  0.051618  0.058849  0.345514  0.067805  0.022711   \n",
       "785276  0.046198  0.278569  0.054519  0.059578  0.346105  0.069822  0.021950   \n",
       "785277  0.047840  0.276019  0.054323  0.060920  0.345948  0.068025  0.021839   \n",
       "785278  0.046960  0.278458  0.054748  0.061308  0.344173  0.068620  0.021560   \n",
       "785279  0.048014  0.276697  0.054558  0.059273  0.345299  0.069937  0.021882   \n",
       "785280  0.047295  0.285146  0.055665  0.060464  0.336913  0.068236  0.021854   \n",
       "785281  0.046069  0.283838  0.054845  0.057951  0.342521  0.068038  0.021745   \n",
       "785282  0.047428  0.290018  0.054453  0.057945  0.336572  0.067724  0.021447   \n",
       "785283  0.048316  0.289487  0.054065  0.058682  0.334200  0.070215  0.020822   \n",
       "785284  0.048527  0.292924  0.056787  0.057575  0.331831  0.070127  0.021034   \n",
       "\n",
       "            pix8      pix9  \n",
       "0       0.049541  0.065974  \n",
       "1       0.048758  0.063537  \n",
       "2       0.048615  0.065382  \n",
       "3       0.049463  0.068500  \n",
       "4       0.051422  0.068246  \n",
       "5       0.049314  0.068799  \n",
       "6       0.051170  0.068688  \n",
       "7       0.050794  0.071538  \n",
       "8       0.049770  0.072192  \n",
       "9       0.051376  0.070944  \n",
       "10      0.051826  0.069483  \n",
       "11      0.077284  0.054126  \n",
       "12      0.079794  0.055036  \n",
       "13      0.078376  0.054887  \n",
       "14      0.079173  0.054178  \n",
       "15      0.081480  0.054458  \n",
       "16      0.083785  0.055793  \n",
       "17      0.083805  0.053535  \n",
       "18      0.084660  0.053269  \n",
       "19      0.084667  0.055421  \n",
       "20      0.081729  0.056289  \n",
       "21      0.084016  0.055405  \n",
       "22      0.083944  0.055815  \n",
       "23      0.084893  0.057728  \n",
       "24      0.085356  0.057467  \n",
       "25      0.084538  0.056800  \n",
       "26      0.078736  0.049906  \n",
       "27      0.076885  0.053081  \n",
       "28      0.078955  0.050898  \n",
       "29      0.079229  0.050999  \n",
       "...          ...       ...  \n",
       "785255  0.059356  0.066309  \n",
       "785256  0.060436  0.066527  \n",
       "785257  0.060203  0.067900  \n",
       "785258  0.058737  0.067875  \n",
       "785259  0.059631  0.068242  \n",
       "785260  0.061240  0.066571  \n",
       "785261  0.058761  0.067946  \n",
       "785262  0.058877  0.068226  \n",
       "785263  0.061097  0.066024  \n",
       "785264  0.061118  0.066762  \n",
       "785265  0.060584  0.065435  \n",
       "785266  0.060753  0.066462  \n",
       "785267  0.061787  0.065626  \n",
       "785268  0.060743  0.064025  \n",
       "785269  0.060261  0.065648  \n",
       "785270  0.060611  0.062365  \n",
       "785271  0.063879  0.061431  \n",
       "785272  0.061596  0.063066  \n",
       "785273  0.060065  0.065480  \n",
       "785274  0.059665  0.064577  \n",
       "785275  0.059878  0.064364  \n",
       "785276  0.060244  0.063015  \n",
       "785277  0.060849  0.064236  \n",
       "785278  0.060682  0.063491  \n",
       "785279  0.059249  0.065091  \n",
       "785280  0.061402  0.063024  \n",
       "785281  0.060809  0.064184  \n",
       "785282  0.059859  0.064554  \n",
       "785283  0.059541  0.064672  \n",
       "785284  0.057821  0.063374  \n",
       "\n",
       "[785285 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PLDpixels = (PLDpixels.T / PLDnorm).T\n",
    "PLDpixels"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[plt.plot(PLDpixels[key]) for key in PLDpixels.columns.values];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spitzerData = spitzerDataRaw.copy()\n",
    "for key in spitzerDataRaw.columns: \n",
    "    if key in PLDpixels.columns:\n",
    "        spitzerData[key] = PLDpixels[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed that PLD Pixels have been Normalized to Spec\n"
     ]
    }
   ],
   "source": [
    "testPLD = np.array(pd.DataFrame({key:spitzerData[key] for key in spitzerData.columns.values if 'pix' in key}))\n",
    "assert(not sum(abs(testPLD - np.array(PLDpixels))).all())\n",
    "print('Confirmed that PLD Pixels have been Normalized to Spec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notFeatures     = ['flux', 'fluxerr', 'xerr', 'yerr', 'xycov']\n",
    "feature_columns = spitzerData.drop(notFeatures,axis=1).columns.values\n",
    "features        = spitzerData.drop(notFeatures,axis=1).values\n",
    "labels          = spitzerData['flux'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stdScaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157057 validation samples\n",
      "471171 train samples\n",
      "157057 test samples\n"
     ]
    }
   ],
   "source": [
    "features_scaled = stdScaler.fit_transform(features)\n",
    "labels_scaled   = stdScaler.fit_transform(labels[:,None]).ravel()\n",
    "\n",
    "x_valtest, x_train, y_valtest, y_train = train_test_split(features_scaled, labels_scaled, test_size=0.6, random_state=42)\n",
    "x_val, x_test, y_val, y_test           = train_test_split(x_valtest, y_valtest, test_size=0.5, random_state=42)\n",
    "\n",
    "# x_val   = minmax_scale(x_val.astype('float32'))\n",
    "# x_train = minmax_scale(x_train.astype('float32'))\n",
    "# x_test  = minmax_scale(x_test.astype('float32'))\n",
    "\n",
    "# y_val   = minmax_scale(y_val.astype('float32'))\n",
    "# y_train = minmax_scale(y_train.astype('float32'))\n",
    "# y_test  = minmax_scale(y_test.astype('float32'))\n",
    "\n",
    "print(x_val.shape[0]  , 'validation samples')\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0] , 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df    = pd.DataFrame(np.c_[x_train, y_train], columns=list(feature_columns) + ['flux'])\n",
    "test_df     = pd.DataFrame(np.c_[x_test , y_test ], columns=list(feature_columns) + ['flux'])\n",
    "evaluate_df = pd.DataFrame(np.c_[x_val  , y_val  ], columns=list(feature_columns) + ['flux'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.scatter(train_df['xpos'].values, train_df['ypos'].values, c=train_df['flux'].values, alpha=0.1);\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8745286a-f9f6-67db-4c2a-e38bc6673045"
   },
   "source": [
    "We only take first 1000 rows for training/testing and last 500 row for evaluation.\n",
    "\n",
    "\n",
    "This done so that this script does not consume a lot of kaggle system resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "95a34f94-b934-2e8f-fbd7-6e5c05425557"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape =  (471171, 20)\n",
      "test_df.shape =  (157057, 20)\n",
      "evaluate_df.shape =  (157057, 20)\n"
     ]
    }
   ],
   "source": [
    "# train_df = df_train_ori.head(1000)\n",
    "# evaluate_df = df_train_ori.tail(500)\n",
    "\n",
    "# test_df = df_test_ori.head(1000)\n",
    "\n",
    "# MODEL_DIR = \"tf_model_spitzer/withNormalization_drop50/relu\"\n",
    "# MODEL_DIR = \"tf_model_spitzer/adamOptimizer_with_drop50/relu\"\n",
    "MODEL_DIR = \"tf_model_spitzer/adamOptimizer_with_drop50/elu/\"\n",
    "\n",
    "print(\"train_df.shape = \"   , train_df.shape)\n",
    "print(\"test_df.shape = \"    , test_df.shape)\n",
    "print(\"evaluate_df.shape = \", evaluate_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3c9feb25-7e41-33b9-eb92-543e925b0214"
   },
   "source": [
    "## Filtering Categorical and Continuous features\n",
    "\n",
    "We store Categorical, Continuous and Target features names in different variables. This will be helpful in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "3bfe4dd5-5b24-b897-b1b1-952e6944b308",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# categorical_features = [feature for feature in features if 'cat' in feature]\n",
    "categorical_features  = []\n",
    "continuous_features   = [feature for feature in train_df.columns]# if 'cat' in feature]\n",
    "LABEL_COLUMN          = 'flux'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5f408c4f-60b3-1e75-281b-ace7ae729e43"
   },
   "source": [
    "## Converting Data into Tensors\n",
    "\n",
    "> When building a TF.Learn model, the input data is specified by means of an Input Builder function. This builder function will not be called until it is later passed to TF.Learn methods such as fit and evaluate. The purpose of this function is to construct the input data, which is represented in the form of Tensors or SparseTensors.\n",
    "\n",
    "> Note that input_fn will be called while constructing the TensorFlow graph, not while running the graph. What it is returning is a representation of the input data as the fundamental unit of TensorFlow computations, a Tensor (or SparseTensor).\n",
    "\n",
    "[More detail][2] on input_fn.\n",
    "\n",
    "[2]: https://www.tensorflow.org/versions/r0.11/tutorials/input_fn/index.html#building-input-functions-with-tf-contrib-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "04601f78-9447-20e9-20ce-3adcfd9a433c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting Data into Tensors\n",
    "def input_fn(df, training = True):\n",
    "    # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "    # the values of that column stored in a constant Tensor.\n",
    "    continuous_cols = {k: tf.constant(df[k].values)\n",
    "                       for k in continuous_features}\n",
    "\n",
    "    # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "    # to the values of that column stored in a tf.SparseTensor.\n",
    "    # categorical_cols = {k: tf.SparseTensor(\n",
    "    #     indices=[[i, 0] for i in range(df[k].size)],\n",
    "    #     values=df[k].values,\n",
    "    #     shape=[df[k].size, 1])\n",
    "    #     for k in categorical_features}\n",
    "\n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = continuous_cols\n",
    "    # feature_cols = dict(list(continuous_cols.items()) + list(categorical_cols.items()))\n",
    "    \n",
    "    if training:\n",
    "        # Converts the label column into a constant Tensor.\n",
    "        label = tf.constant(df[LABEL_COLUMN].values)\n",
    "\n",
    "        # Returns the feature columns and the label.\n",
    "        return feature_cols, label\n",
    "    \n",
    "    # Returns the feature columns    \n",
    "    return feature_cols\n",
    "\n",
    "def train_input_fn():\n",
    "    return input_fn(train_df, training=True)\n",
    "\n",
    "def eval_input_fn():\n",
    "    return input_fn(evaluate_df, training=True)\n",
    "\n",
    "# def test_input_fn():\n",
    "#     return input_fn(test_df.drop(LABEL_COLUMN,axis=1), training=False)\n",
    "\n",
    "def test_input_fn():\n",
    "    return input_fn(test_df, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f53238a-876b-bf8b-a5f9-77cdfafe67d9"
   },
   "source": [
    "## Selecting and Engineering Features for the Model\n",
    "\n",
    "We use tf.learn's concept of [FeatureColumn][FeatureColumn] which help in transforming raw data into suitable input features. \n",
    "\n",
    "These engineered features will be used when we construct our model.\n",
    "\n",
    "[FeatureColumn]: https://www.tensorflow.org/versions/r0.11/tutorials/linear/overview.html#feature-columns-and-transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "87b90027-c671-d7aa-88b2-4e2856a83f52",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engineered_features = []\n",
    "\n",
    "for continuous_feature in continuous_features:\n",
    "    engineered_features.append(\n",
    "        tf.contrib.layers.real_valued_column(continuous_feature))\n",
    "\n",
    "\n",
    "# for categorical_feature in categorical_features:\n",
    "#     sparse_column = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "#         categorical_feature, hash_bucket_size=1000)\n",
    "\n",
    "#     engineered_features.append(tf.contrib.layers.embedding_column(sparse_id_column=sparse_column, dimension=16,\n",
    "#                                                                   combiner=\"sum\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ca73de97-e020-4cf3-934f-44dde67523ae"
   },
   "source": [
    "## Defining The Regression Model\n",
    "\n",
    "Following is the simple DNNRegressor model. More detail about hidden_units, etc can be found [here][123].\n",
    "\n",
    "**model_dir** is used to save and restore our model. This is because once we have trained the model we don't want to train it again, if we only want to predict on new data-set.\n",
    "\n",
    "[123]: https://www.tensorflow.org/versions/r0.9/api_docs/python/contrib.learn.html#DNNRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "6c5b7254-0e26-88d2-a65b-17f81c223ab4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nHidden1  = 10\n",
    "nHidden2  = 5\n",
    "nHidden3  = 10\n",
    "\n",
    "regressor = tf.contrib.learn.DNNRegressor(activation_fn=tf.nn.elu, dropout=0.5, optimizer=tf.train.AdamOptimizer,\n",
    "    feature_columns=engineered_features, hidden_units=[nHidden1, nHidden2, nHidden3], model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "09e6b3b1-0c96-df80-85e7-31e958018309"
   },
   "source": [
    "## Training and Evaluating Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add progress bar through python `logging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "109a3d46-d5b6-93f6-d792-14765b531b8d"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b2cbf6df5eba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnFitSteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mwrap\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnFitSteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TF Regressor took {} seconds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m               instructions)\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    298\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\u001b[0m\n\u001b[1;32m    456\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_session_run_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStopAtStepHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m           \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_fn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m       \u001b[0msummary_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSummaryWriterCache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    516\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    860\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    970\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Our Model\n",
    "nFitSteps = 10000\n",
    "start = time()\n",
    "wrap  = regressor.fit(input_fn=train_input_fn, steps=nFitSteps)\n",
    "print('TF Regressor took {} seconds'.format(time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7655bb62-0183-1778-4c86-a69ec5d3a3cf"
   },
   "outputs": [],
   "source": [
    "# Evaluating Our Model\n",
    "print('Evaluating ...')\n",
    "results = regressor.evaluate(input_fn=eval_input_fn, steps=1)\n",
    "\n",
    "for key in sorted(results):\n",
    "    print(\"{}: {}\".format(key, results[key]))\n",
    "\n",
    "print(\"Val Acc: {:.3f}\".format((1-results['loss'])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Track Scalable Growth**\n",
    "\n",
    "Shrunk data set to 23559 Training samples and 7853 Val/Test samples\n",
    "\n",
    "| n_iters | time (s) | val acc | multicore | gpu |\n",
    "|------------------------------------------------|\n",
    "|  100    |   5.869  |  6.332 | yes | no |\n",
    "|  200    |   6.380  | 13.178 | yes | no |\n",
    "|  500    |   8.656  | 54.220 | yes | no |\n",
    "|  1000   |  12.170  | 66.596 | yes | no |\n",
    "|  2000   |  19.891  | 62.996 | yes | no |\n",
    "|  5000   |  43.589  | 76.586 | yes | no |\n",
    "|  10000  |  80.581  | 66.872 | yes | no |\n",
    "|  20000  | 162.435  | 78.927 | yes | no |\n",
    "|  50000  | 535.584  | 75.493 | yes | no |\n",
    "|  100000 | 1062.656 | 73.162 | yes | no |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nItersList = [100,200,500,1000,2000,5000,10000,20000,50000,100000]\n",
    "rtimesList = [5.869, 6.380, 8.656, 12.170, 19.891, 43.589, 80.581, 162.435, 535.584, 1062.656]\n",
    "valAccList = [6.332, 13.178, 54.220, 66.596, 62.996, 76.586, 66.872, 78.927, 75.493, 73.162]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(nItersList, rtimesList,'o-');\n",
    "plt.twinx()\n",
    "plt.semilogx(nItersList, valAccList,'o-', color='orange');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c418c90-cbda-b772-f3be-bc9107bca2f8"
   },
   "source": [
    "## Predicting output for test data\n",
    "\n",
    "Most of the time prediction script would be separate from training script (we need not to train on same data again) but I am providing both in same script here; as I am not sure if we can create multiple notebook and somehow share data between them in Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def de_median(x):\n",
    "    return x - np.median(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_output = list(regressor.predict(input_fn=test_input_fn))\n",
    "# x = list(predicted_output)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "c4e45f26-f03e-9079-65a6-b13871b24a46",
    "scrolled": false
   },
   "source": [
    "# print([predicted_output() for _ in range(10)])\n",
    "plt.plot((predicted_output - np.median(predicted_output)) / np.std(predicted_output),'.',alpha=0.1);\n",
    "plt.plot((test_df['flux'].values - np.median(test_df['flux'].values)) / np.std(test_df['flux'].values),'.',alpha=0.1);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(de_median(x - test_df['flux'].values)/x,'.',alpha=0.1);\n",
    "plt.ylim(-1.,1.);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r2_score(test_df['flux'].values,predicted_output)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Full notebook took {} seconds'.format(time()-start0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "saveDir = 'tfSaveModels'\n",
    "regressor.export_savedmodel(saveDir, regressor)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "saveDir = 'tfSaveModels'\n",
    "reg_args = {'feature_columns': fc, 'hidden_units': hu_array, ...}\n",
    "regressor = tf.contrib.learn.DNNRegressor(**reg_args)\n",
    "pickle.dump(reg_args, open('reg_args.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "reg_args = pickle.load(open('reg_args.pkl', 'rb'))\n",
    "# On another machine and so my model dir path changed:\n",
    "reg_args['model_dir'] = NEW_MODEL_DIR\n",
    "regressor = tf.contrib.learn.DNNRegressor(**reg_args)"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 2,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
